{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: CNNs & RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'Yang Bai'\n",
    "email = 'baiyang94@ufl.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 7 -- name: Yang Bai, email: baiyang94@ufl.edu\n",
      "\n",
      "### Python version: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "### NumPy version: 1.19.5\n",
      "### Scikit-learn version: 0.24.1\n",
      "### Tensorflow version: 2.4.1\n",
      "### TF Keras version: 2.4.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 7 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    \n",
    "    # we'll use tensorflow and keras for neural networks\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras as keras\n",
    "    \n",
    "    # import layers we may use\n",
    "    from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "    # import callbacks we may use\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    #%load_ext tensorboard\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('### Tensorflow version: ' + tf.__version__)\n",
    "    print('### TF Keras version: ' + keras.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (20 points) Loading and Processing CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1a] (20 points) Complete the implementation of load_preprocess_cifar10(). Make sure you correctly implement all of the cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data\n",
    "# and to https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def load_preprocess_cifar10(onehot=True, minmax_normalize=True):\n",
    "    \n",
    "    labels = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "    \n",
    "    ### Load and preprocess the cifar10 data, then split it into train, test, validation\n",
    "    ### The shapes of train_x, test_x, val_x should be: (50000, 32, 32, 3), (5000, 32, 32, 3), (5000, 32, 32, 3)\n",
    "    ### If onehot=True you need to one hot encode the labels (y vector)\n",
    "    ### If minmax_normalize=True you need to minmax normalize the pixel values to be in the range [0,1]\n",
    "    ###* put your code here (~10-20 lines) *###\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    \n",
    "    # The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. \n",
    "    # The dataset is divided into 50,000 training images and 10,000 testing images. \n",
    "    # The classes are mutually exclusive and there is no overlap between them.\n",
    "#     print('Loaded CIFAR10 data; shape: {} [y: {}], test shape: {} [y: {}]'.format(x_train.shape, y_train.shape,\n",
    "#                                                                                       x_test.shape, y_test.shape))\n",
    "\n",
    "    if onehot:\n",
    "        # Put the labels in \"one-hot\" encoding using keras' to_categorical()\n",
    "        num_classes = 10\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    # let's aggregate all the data then split\n",
    "    all_x = np.r_[x_train, x_test]\n",
    "    all_y = np.r_[y_train, y_test]\n",
    "    \n",
    "    if minmax_normalize:\n",
    "        # Normalize pixel values to be between 0 and 1\n",
    "        all_x = all_x / 255.0\n",
    "    \n",
    "    # split the data into train, test, val\n",
    "    prop_vec = [10, 1, 1]\n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=False, minmax_normalize=False)\n",
    "assert train_x.shape[0] == train_y.shape[0] and test_x.shape[0] == test_y.shape[0] and val_x.shape[0] == val_y.shape[0]\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],) or train_y.shape == (train_y.shape[0],1)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=True, minmax_normalize=False)\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],10) and train_y.shape[1] == test_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10()\n",
    "assert np.amax(train_x) <= 1 and np.amax(test_x) <= 1 and np.amax(val_x) <= 1\n",
    "assert np.amax(train_x) >= 0 and np.amax(test_x) >= 0 and np.amax(val_x) >= 0\n",
    "\n",
    "assert labels.shape[0] == 10 and labels.shape[0] == train_y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (30 points) Training a CNN for Cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will use the following architecture\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Dropout with rate 25%\n",
    "- Flatten\n",
    "- FC with 128 units\n",
    "- Dropout with rate 25%\n",
    "- FC with 64 units\n",
    "- Dropout with rate 25%\n",
    "- (Output layer) FC with 10 units\n",
    "\n",
    "#### For all layers (if applicable) except the output layer you should use:\n",
    "- ReLU as activation function\n",
    "- He uniform weight initialization strategy\n",
    "- L2 regularization with regularization constant set to 0.001\n",
    "\n",
    "#### For the output layer you should select a suitable activation function that is consistent with the task and loss function you use. Use Adam for the optimizer with learning rate 0.002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (20 points) Implement create_compile_cnn() according to the architecture specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_cnn(input_shape=[32, 32, 3], num_outputs=10, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='CIFAR-10--CNN')\n",
    "    \n",
    "    initializer = tf.keras.initializers.HeUniform(seed=seed)\n",
    "    regularizer = tf.keras.regularizers.l2(0.001)\n",
    "    activation_func = 'relu'\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~20 lines) *###\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=1, input_shape=input_shape,\n",
    "                     padding='same', activation=activation_func, kernel_regularizer=regularizer, \n",
    "                     kernel_initializer=initializer, name='conv1'))\n",
    "    model.add(Conv2D(32, kernel_size=(3,3), strides=1,\n",
    "                     padding='same', activation=activation_func, kernel_regularizer=regularizer, \n",
    "                     kernel_initializer=initializer, name='conv2'))\n",
    "    model.add(MaxPooling2D(2, name='maxpool1'))\n",
    "    model.add(Dropout(0.25, name='dropout1'))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=1,\n",
    "                     padding='same', activation=activation_func, kernel_regularizer=regularizer, \n",
    "                     kernel_initializer=initializer, name='conv3'))\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), strides=1,\n",
    "                     padding='same', activation=activation_func, kernel_regularizer=regularizer, \n",
    "                     kernel_initializer=initializer, name='conv4'))\n",
    "    model.add(MaxPooling2D(2, name='maxpool2'))\n",
    "    model.add(Dropout(0.25, name='dropout2'))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides=1,\n",
    "                     padding='same', activation=activation_func, kernel_regularizer=regularizer, \n",
    "                     kernel_initializer=initializer, name='conv5'))\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), strides=1,\n",
    "                     padding='same', activation=activation_func, kernel_regularizer=regularizer, \n",
    "                     kernel_initializer=initializer, name='conv6'))\n",
    "    model.add(MaxPooling2D(2, name='maxpool3'))\n",
    "    model.add(Dropout(0.25, name='dropout3'))\n",
    "    \n",
    "    model.add(Flatten(name='flatten'))\n",
    "    \n",
    "    model.add(Dense(128, activation=activation_func, kernel_regularizer=regularizer, \n",
    "                    kernel_initializer=initializer, name='fc1'))\n",
    "    model.add(Dropout(0.25, name='dropout4'))\n",
    "    model.add(Dense(64, activation=activation_func, kernel_regularizer=regularizer, \n",
    "                    kernel_initializer=initializer, name='fc2'))\n",
    "    model.add(Dropout(0.25, name='dropout5'))\n",
    "    \n",
    "    model.add(Dense(num_outputs, activation=\"softmax\", name='output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.002)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CIFAR-10--CNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "maxpool1 (MaxPooling2D)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout1 (Dropout)           (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "maxpool2 (MaxPooling2D)      (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout2 (Dropout)           (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "maxpool3 (MaxPooling2D)      (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout3 (Dropout)           (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout4 (Dropout)           (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout5 (Dropout)           (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 558,186\n",
      "Trainable params: 558,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_ = create_compile_cnn(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (10 points) Train the model. Fill in the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./cifar10-cnn.h5\n",
      "Training based on the previous trained model...\n",
      "Epoch 1/15\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 1.1999 - accuracy: 0.6751 - val_loss: 1.0904 - val_accuracy: 0.7104\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.71040, saving model to ./mymodel-bestweights.h5\n",
      "Epoch 2/15\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 1.2112 - accuracy: 0.6719 - val_loss: 1.0747 - val_accuracy: 0.7208\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.71040 to 0.72080, saving model to ./mymodel-bestweights.h5\n",
      "Epoch 3/15\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 1.2254 - accuracy: 0.6682 - val_loss: 1.1111 - val_accuracy: 0.6952\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.72080\n",
      "Epoch 4/15\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 1.2156 - accuracy: 0.6700 - val_loss: 1.0578 - val_accuracy: 0.7176\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.72080\n",
      "Epoch 5/15\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 1.2256 - accuracy: 0.6647 - val_loss: 1.0584 - val_accuracy: 0.7124\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.72080\n"
     ]
    }
   ],
   "source": [
    "cnn_model_fp = './cifar10-cnn.h5'\n",
    "fp = \"./mymodel-bestweights.h5\"\n",
    "\n",
    "early_stop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "# set up a model checkpointing callback\n",
    "checkpoint_cb = ModelCheckpoint(fp, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "max_epochs = 15\n",
    "batch_size = 64\n",
    "\n",
    "if len(train_x.shape) < 4:\n",
    "    train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "    val_x = val_x.reshape(-1, 28, 28, 1)\n",
    "    test_x = test_x.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# If the model file exists, load it. Otherwise train it and save the model.\n",
    "# Note: if you need to retrain the model, simply delete the h5 file.\n",
    "if os.path.exists(cnn_model_fp):\n",
    "    print(\"Loading model from %s\" % cnn_model_fp)\n",
    "    model = keras.models.load_model(cnn_model_fp)\n",
    "    print(\"Training based on the previous trained model...\")\n",
    "    history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                         shuffle=True, callbacks=[early_stop_cb, checkpoint_cb])\n",
    "    # save the model\n",
    "    model.save(cnn_model_fp)\n",
    "else:\n",
    "    print(\"Training from scratch...\")\n",
    "    model = create_compile_cnn(verbose=False)\n",
    "    # train the model using model.fit() for at least 3 epochs and your chosen batch_size\n",
    "    # you can set any callback you want on it, including checkpoint, early stopping, etc.\n",
    "    ###* put your code here (~3-5 lines) *###\n",
    "    history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                         shuffle=True, callbacks=[early_stop_cb, checkpoint_cb])\n",
    "    # save the model\n",
    "    model.save(cnn_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 72.88%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (15 points) Processing Sequence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (15 points) Fill in the implementation of load_preprocess_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is NOT AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# the size of the vocabulary we'll use\n",
    "vocab_size = 12000\n",
    "maxlen = 150\n",
    "\n",
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb\n",
    "def load_preprocess_imdb(num_words=vocab_size, prop_vec=prop_vec, maxlen=maxlen, vectorize=False):\n",
    "    \n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    \n",
    "    # IMDB is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). \n",
    "    # Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). \n",
    "    # For convenience, words are indexed by overall frequency in the dataset, \n",
    "    # so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. \n",
    "    # This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, \n",
    "    # but eliminate the top 20 most common words\".\n",
    "    # As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "    train, testval = imdb.load_data(num_words=num_words, maxlen=maxlen, oov_char=0)\n",
    "    \n",
    "    np.warnings.filterwarnings('default', category=np.VisibleDeprecationWarning)    \n",
    "    \n",
    "    ### Process the data \n",
    "    ### Merge train and testval, but then split again into train, test, val sets (according to prop_vec). You can use utils.train_test_val_split().)\n",
    "    ### - If vectorize=True, then you must encode the features of each example into vectors of vocab_size entries \n",
    "    ### such that entry i contains the number of time word i appeared in the sequence\n",
    "    ### - If vectorize=False, then you must encode the features of each examples as a sequence of size maxlen (represented as a np.array()). \n",
    "    ### Make sure to pad sequences with 0 as appropriate.\n",
    "    ###* put your code here (~10-15 lines) *###\n",
    "    (x_train, y_train), (x_test, y_test) = train, testval\n",
    "    all_x = np.r_[x_train, x_test]\n",
    "    all_y = np.r_[y_train, y_test]\n",
    "    if vectorize:\n",
    "        vectorized_all_x = np.zeros((all_x.shape[0], num_words))\n",
    "        for i, sequence in enumerate(all_x):\n",
    "            for word in sequence:\n",
    "                vectorized_all_x[i, word] +=  1   \n",
    "        all_x = vectorized_all_x\n",
    "    else:        \n",
    "        padded_all_x = np.zeros((all_x.shape[0], maxlen))\n",
    "        for i, sequence in enumerate(all_x):\n",
    "            padded_all_x[i, :len(sequence)] =  sequence   \n",
    "        all_x = padded_all_x\n",
    "    \n",
    "    train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, \n",
    "                                                                                prop_vec, shuffle=True, \n",
    "                                                                                seed=seed)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=False)\n",
    "assert train_x.shape == (16281, maxlen) and train_y.shape == (train_x.shape[0],)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=True)\n",
    "assert train_x.shape == (16281, vocab_size) and train_y.shape == (train_x.shape[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "# sorted_word_index_dict = dict(sorted(word_index.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (35 points) RNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (35 points) Complete the code below to define an RNN architecture for sentiment analysis. The goal is to predict the sentiment of IMDB reviews. You can use any architecture you want, but a good place to start would be to use an embedding layer followed by some recurrent layers (e.g., LSTM, GRU, etc.). Keep the number of parameters of the model below 2m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, SimpleRNN, GRU, Embedding\n",
    "dropout_rate = 0.2\n",
    "\n",
    "def create_compile_rnn(input_shape=[None], vocab_size=vocab_size, embedding_size=128, num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-RNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ### Use binary_crossentropy as loss function.    \n",
    "    ###* put your code here (~15-20 lines) *###\n",
    "    model.add(keras.Input(shape=input_shape, sparse=False, name='input'))\n",
    "    \n",
    "    model.add(Embedding(vocab_size, embedding_size, name='embedding'))\n",
    "    \n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(GRU(192, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.0, name='gru1'))\n",
    "    model.add(GRU(128, recurrent_dropout=0.0, name='gru2'))\n",
    "    \n",
    "    # output\n",
    "    model.add(Dense(num_outputs, activation='sigmoid', name='output'))\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "        \n",
    "    opt = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb-RNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1536000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "gru1 (GRU)                   (None, None, 192)         185472    \n",
      "_________________________________________________________________\n",
      "gru2 (GRU)                   (None, 128)               123648    \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,845,249\n",
      "Trainable params: 1,845,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_compile_rnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing from scratch...\n",
      "Epoch 1/3\n",
      "509/509 [==============================] - 124s 239ms/step - loss: 0.6938 - accuracy: 0.5047 - val_loss: 0.6942 - val_accuracy: 0.5177\n",
      "Epoch 2/3\n",
      "509/509 [==============================] - 121s 237ms/step - loss: 0.6868 - accuracy: 0.5239 - val_loss: 0.4647 - val_accuracy: 0.7898\n",
      "Epoch 3/3\n",
      "509/509 [==============================] - 121s 237ms/step - loss: 0.3570 - accuracy: 0.8496 - val_loss: 0.2647 - val_accuracy: 0.8916\n"
     ]
    }
   ],
   "source": [
    "rnn_model_fp = './imdb-rnn.h5'\n",
    "\n",
    "# let's load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=False)\n",
    "\n",
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "if os.path.exists(rnn_model_fp) and False:\n",
    "    print(\"Loading model from %s\" % rnn_model_fp)\n",
    "    model = keras.models.load_model(rnn_model_fp)\n",
    "    print(\"Training based on the previous trained model...\")\n",
    "    hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), \n",
    "                     callbacks=[early_stop_cb])\n",
    "    model.save(rnn_model_fp)\n",
    "else:\n",
    "    print(\"Traing from scratch...\")\n",
    "    hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), \n",
    "                     callbacks=[early_stop_cb])\n",
    "    model.save(rnn_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 90.71%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): DNN for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the previous task, we use an RNN for sentiment analysis. In this task you will use a neural network without any recurrent layers for the same task as a comparison.\n",
    "\n",
    "### We'll use the data in vectorized form for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (20 points) Complete the code below to define an architecture of your choice *without* any recurrent layers. The goal is to get the best model with the fewest number of parameters. Keep the number of parameters of the model below 2m and ideally similar to the model of Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_dnn(input_shape=[vocab_size], num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-DNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    ###* put your code here (~10 lines) *###\n",
    "    model.add(Input(shape=input_shape, sparse=False, name='input'))\n",
    "    \n",
    "    hidden_widths=[150, 50]\n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                             kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(1/hw)),\n",
    "                             bias_initializer=keras.initializers.Zeros()))\n",
    "    \n",
    "    # output\n",
    "    model.add(Dense(num_outputs, activation='sigmoid', name='output'))\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "        \n",
    "    opt = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"imdb-DNN\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden_0 (Dense)             (None, 150)               1800150   \n",
      "_________________________________________________________________\n",
      "hidden_1 (Dense)             (None, 50)                7550      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,807,751\n",
      "Trainable params: 1,807,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_compile_dnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from scratch...\n",
      "Epoch 1/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.5691 - accuracy: 0.7049 - val_loss: 0.2943 - val_accuracy: 0.8776\n",
      "Epoch 2/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.1528 - accuracy: 0.9495 - val_loss: 0.3013 - val_accuracy: 0.8717\n",
      "Epoch 3/50\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 0.0575 - accuracy: 0.9873 - val_loss: 0.3592 - val_accuracy: 0.8776\n",
      "Epoch 4/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 0.0151 - accuracy: 0.9982 - val_loss: 0.4233 - val_accuracy: 0.8791\n",
      "Epoch 5/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.4798 - val_accuracy: 0.8754\n",
      "Epoch 6/50\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5186 - val_accuracy: 0.8776\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00006: early stopping\n"
     ]
    }
   ],
   "source": [
    "dnn_model_fp = './imdb-dnn.h5'\n",
    "\n",
    "# Let's load the data in vectorized form\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(vectorize=True)\n",
    "\n",
    "\n",
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 50\n",
    "batch_size = 100\n",
    "\n",
    "if os.path.exists(dnn_model_fp) and False:\n",
    "    print(\"Loading model from %s\" % dnn_model_fp)\n",
    "    model = keras.models.load_model(dnn_model_fp)\n",
    "    print(\"Training based on the previous trained model...\")\n",
    "    hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size,validation_data=(val_x, val_y), \n",
    "                     callbacks=[early_stop_cb])\n",
    "    model.save(dnn_model_fp)\n",
    "    \n",
    "else:\n",
    "    print(\"Training from scratch...\")\n",
    "    hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size,validation_data=(val_x, val_y), \n",
    "                     callbacks=[early_stop_cb])\n",
    "    model.save(dnn_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Test accuracy: 87.91%\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (5 points) Compare this model to the model of Task 4. What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# RNN got a test accuracy of 90.71% while DNN got a test accuracy of 87.91 at a similar number of parameters.\n",
    "# From this I can conclude that at a similar scale of parameters, RNN performs a little bit better than than DNN.\n",
    "# It could be counted as an evidence that RNN is better at capture features of sequence data than DNN.\n",
    "# However, training speed of DNN is much faster than RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
