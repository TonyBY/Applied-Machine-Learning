{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: ML Engineering 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'enter your name'\n",
    "email = 'enter your email'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 2 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "##rng = np.random.default_rng(seed)  # best practice but not fully implemented in scikit-learn\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [16, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this case, we'll directly load the Adult dataset pre-processed in a similar way as for assignment 1\n",
    "### and we'll immediately split it into train, test, validation.\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, features, labels = utils.load_preproc_adult(prop_vec=prop_vec, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have what we expect\n",
    "print('Training: {}, {}'.format(train_x.shape, train_y.shape))\n",
    "print('Test: {}, {}'.format(test_x.shape, test_y.shape))\n",
    "print('Validation: {}, {}'.format(val_x.shape, val_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print features and labels\n",
    "print('Features: {}'.format(features))\n",
    "print('Labels: {}'.format(labels))\n",
    "\n",
    "# as you can see this the one-hot encoded version of the data with proper names for the columns/features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at some rows of our training data just so we know what it looks like\n",
    "print(train_x[2,:])\n",
    "print(train_y[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (30 points) Let's tune hyperparameters! We will use scikit-learn in two ways to optimize hyperparameters of SVM: (1) grid search, (2) randomized search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1a] (10 points) Use GridSearchCV from scikit-learn to do a grid search for SVM hyperparameters. Note that this way will use cross-validation to find the best hyperparameters values and that we purposefully disable some warnings to avoid verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'C':[0.1, 1, 5], 'penalty': ['l1', 'l2']}\n",
    "model = LinearSVC(max_iter=1000, dual=False)\n",
    "\n",
    "# We'll use this to avoid some annoying convergence warnings \n",
    "# Note: don't just do that without thinking in your own projects, warnings are there for a reason folks!\n",
    "## (In this case, it's okay because it's for illustration, but obviously if the model doesn't converge in \n",
    "## some cases we may not find the true best hyperparams)\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def do_grid_search(model, param_grid):\n",
    "    \n",
    "    # use the GridSearchCV class of scikit-learn to do a grid search on the provided grid (use 'model')\n",
    "    # set accuracy as scoring function and return the output of fit() on the training data\n",
    "    \n",
    "    ###* put your code here (~2 lines) *###\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "gs_res = do_grid_search(model, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1b] (3 points) How many combination of hyperparameters were tested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1c] (2 points) What is the best combination of hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best combination of parameters found through the search \n",
    "# Hint: look at the documentation of GridSearchCV\n",
    "###* put your code here (~2 lines) *###\n",
    "# store this combination in 'gs_best_hyperparams' and print it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1d] (10 points) Use RandomizedSearchCV to do a search! We'll use a halfnormal distribution from Scipy to find values for C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import halfnorm\n",
    "\n",
    "param_dist = dict(C=halfnorm(loc=0, scale=4.0), penalty=['l1', 'l2'])\n",
    "model = LinearSVC(max_iter=1000, dual=False)\n",
    "\n",
    "# We'll use this to avoid some annoying convergence warnings \n",
    "# Note: don't just do that in your own projects, warnings are there for a reason!\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def do_randomized_search(model, param_dist):\n",
    "    # use the RandomizedSearchCV class of scikit-learn to do a randomized search (use 'model')\n",
    "    # use accuracy as scoring function and return the result of fit() on the training data \n",
    "    # Also for reproducibility: set the random_state\n",
    "    \n",
    "    ###* put your code here (~2 lines) *###\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "rs_res = do_randomized_search(model, param_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1e] (1 points) What is the best combination of hyperparameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the best combination of parameters found through the randomized search \n",
    "###* put your code here (~1 line) *###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1f] (4 points) What are the pros and cons of a randomized search? Explain your answer. (A few sentences is okay.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do you think are the pros and cons of a randomized search?\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (10 points) Let's train the model and evaluate it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete the function to calculate accuracy (value in [0,1] range)\n",
    "def model_accuracy(model, x, true_y):\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    \n",
    "\n",
    "def evaluate_model(name, model, train_x, train_y, val_x, val_y):\n",
    "    train_acc = model_accuracy(model, train_x, train_y)\n",
    "    val_acc = model_accuracy(model, val_x, val_y)\n",
    "    print('{}: \\n\\t--- Training accuracy: {:.2f}%, Validation accuracy: {:.2f}%'.format(name, train_acc*100, val_acc*100))\n",
    "    return\n",
    "\n",
    "###* put your code here (~1 line) *### \n",
    "### Train a LinearSVC using the best hyperparameters found during the grid search in Task 1\n",
    "### In addition you should also use: max_iter=10000, dual=False\n",
    "### Use the training data (train_x, train_y)\n",
    "### Hint: there is a way to pass the best hyperparameters object from Task 1b directly to the model object (i.e., without passing it one hyperparameter at a time)\n",
    "\n",
    "\n",
    "\n",
    "evaluate_model('LinearSVC(w/ best grid search hyperparams {})'.format(gs_best_hyperparams), \n",
    "               svm, train_x, train_y, val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (30 points) Manual Hyperparameter Optimization (i.e., without using scikit-learn's to do the search for us)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (10 points) Complete the code below to do a grid search manually. In this case you cannot use GridSearchCV, you must train and evaluate the model on each combination of hyperparameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we are allowed to use the following from scikit-learn\n",
    "from sklearn.model_selection import ParameterGrid \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "hyperparams_vals = {'weights': ['uniform', 'distance'],\n",
    "                    'metric': ['euclidean', 'chebyshev'],\n",
    "                    'n_neighbors': [1, 3, 5, 7, 11, 51]}\n",
    "\n",
    "grid = ParameterGrid(hyperparams_vals)\n",
    "\n",
    "trsub_size = 5000\n",
    "trsub_x = train_x[:trsub_size,:]\n",
    "trsub_y = train_y[:trsub_size]\n",
    "\n",
    "## iterate over the entire grid. In each case, train a KNN classifier with the given hyperparameters (on the training subset 'trsub')\n",
    "# and measure accuracy on both the training subset data and the validation data!\n",
    "# note: we use a subset of the training data to speed up the process a bit\n",
    "for i, hyperparams in enumerate(list(grid)):\n",
    "    \n",
    "    ###* put your code here  (~ 5 lines) *###\n",
    "    ### In each iteration of the loop you should train a KNeighborsClassifier using 'hyperparams' as hyperparameters\n",
    "    ### You should train the model on 'trsub_x' and 'trsub_y'!\n",
    "    ### Once your model is trained, compute the accuracy on trsub (training accuracy) and on val (validation accuracy) \n",
    "    ### store the results in 'train_acc' and 'val_acc' respectively\n",
    "\n",
    "    \n",
    "    \n",
    "    ## This will print information about the grid search as it progresses\n",
    "    print('Iter {}, hyperparams: {}\\n \\t-> train accuracy: {:.2f}%, val accuracy: {:.2f}%'.\n",
    "              format(i, hyperparams, 100*train_acc, 100*val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3b] (5 points) Answer the following questions. (A few sentences is fine.)\n",
    "### What combination of hyperparameters would you use and why? Is the training accuracy useful when doing hyperparameter tuning? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What combination of hyperparameters would you use and why?\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "#          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the training accuracy useful when doing hyperparameter tuning? Why or why not?\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3d] (10 points) Answer the following questions. (A few sentences is fine.)\n",
    "### Observe what happens when weights changes from 'uniform' to 'distance'? Provide a plausible explanation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you observe in terms of the training and validation accuracies when the weights change from 'uniform' to \n",
    "# 'distance'?\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "\n",
    "\n",
    "## provide a plausible explanation for this phenomenon.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "\n",
    "## If you were to train a KNN model would you set weights to 'uniform' or 'distance'? Why?\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3e] (5 points) Which hyperparameters (if any) have a significant impact on overfitting and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### According to the grid search you just performed, which hyperparameters seem to have significant impact on overfitting and why?\n",
    "## Explain your reasoning and justify your answer!\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (30 points) Exploring Bias & Variance, Underfitting & Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this task, you *must* only use models (kNN or SVM) and hyperparameters that we have seen/used somewhere in this assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (5 points) What is the irreducible error for this prediction task (income >=50k or <50k on Adult data)? Explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the rest of Task 4, we will assume that irreducible error (in the scale 1 - accuracy) is about 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4b] (5 points) Train a high bias, low variance model. Show *and* explain why the model has high bias and low variance! (Use evaluate_model() to calculate and display training accuracy and validation accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* Put your code here (~2 lines) *###\n",
    "\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4c] (5 points) Train a low bias, high variance model. Show *and* explain why the model has low bias and high variance! (Use evaluate_model() to calculate and display training accuracy and validation accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* Put your code here (~2 lines) *###\n",
    "\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4d] (5 points) Train a low bias, low variance model. Show *and* explain why the model has low bias and low variance! (Use evaluate_model() to calculate and display training accuracy and validation accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* Put your code here (~2 lines) *###\n",
    "\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4e] (5 points) Can you train a high bias, high variance model. If so how? If not why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* Put your code here (if applicable) *###\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4f] (5 points) Train a badly *underfitted* model. The accuracy should be below 55%! (Use evaluate_model() to calculate and display training accuracy and validation accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* Put your code here (~2 lines) *###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): Variance, Overfitting, Agreement Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we have two models and want to compare them and instead of comparing them in terms of how good the models are, we care about whether the models have learned a similar relationship between features and label? One way we can measure this is using agreement rate: we use both models to make predictions on a separate dataset and then measure what proportion of those predictions are identical.\n",
    "\n",
    "### Variance is the tendency to learn non-existing/wrong relationships between features and labels based on the idiosyncracies of the training data. So intuitively, if two models are trained on disjoint but randomly selected subsets of the training data, then if the variance is high the agreement rate between the two models should be low. So, we can try to measure variance by measuring agreement rate. But does this work? This is what you will explore experimentally in this task. (Note that overfitting and variance are related but are not the same thing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (5 points) Implement an overfitting measure and the agreement rate metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We'll define two metrics, one for overfitting and the other for agreement rate\n",
    "\n",
    "## for this assignment we define overfitting measure as the max between 0 and \n",
    "## the difference between training accuracy and validation accuracy\n",
    "def overfitting_measure(train_acc, val_acc):\n",
    "    ###* put your code here (~1 line) *###\n",
    "    ### your code should return the maxmimum between: 0 and training accuracy - validation accuracy\n",
    "\n",
    "\n",
    "\n",
    "## the agreement rate is the proportion of identical prediction of both models on a separate dataset\n",
    "## note: we don't care if the predictions are correct, we only care how often they are the same!\n",
    "def agreement_rate(m1_preds, m2_preds):\n",
    "    assert m1_preds.shape == m2_preds.shape\n",
    "    \n",
    "    ###* put your code here (~1 line) *###\n",
    "    ### your code should return the proportion of identical predictions in m1_preds and m2_preds\n",
    "    ### note: the agreement rate is a value in [0, 1] so make sure your code returns values in the same range!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def eval_accuracy(model, train_x, train_y, val_x, val_y):\n",
    "    train_acc = model_accuracy(model, train_x, train_y)\n",
    "    val_acc = model_accuracy(model, val_x, val_y)\n",
    "    \n",
    "    return train_acc, val_acc\n",
    "\n",
    "def measure_overfitting_agreement(model, train_x, train_y, trsz, val_x, val_y):\n",
    "    m1 = clone(model)\n",
    "    m2 = clone(model)\n",
    "    \n",
    "    n = train_x.shape[0]\n",
    "    assert n/2 >= trsz and trsz > 0\n",
    "    \n",
    "    rng = np.random.default_rng()\n",
    "    pi = rng.permutation(n)\n",
    "    pi1 = pi[0:trsz]\n",
    "    pi2 = pi[trsz:2*trsz]\n",
    "    \n",
    "    m1.fit(train_x[pi1], train_y[pi1])\n",
    "    m2.fit(train_x[pi2], train_y[pi2])\n",
    "    \n",
    "    train_acc, val_acc = eval_accuracy(m1, train_x[pi1], train_y[pi1], val_x, val_y)\n",
    "    m1_overfit =  np.maximum(0, train_acc - val_acc)\n",
    "    \n",
    "    train_acc, val_acc = eval_accuracy(m2, train_x[pi2], train_y[pi2], val_x, val_y)\n",
    "    m2_overfit =  np.maximum(0, train_acc - val_acc)\n",
    "    \n",
    "    m1_val_pred = m1.predict(val_x)\n",
    "    m2_val_pred = m2.predict(val_x)\n",
    "    \n",
    "    agr = agreement_rate(m1_val_pred, m2_val_pred)\n",
    "    \n",
    "    # for this task, we'll define our our overfitting measure as \n",
    "    # the average of the overfitting measure of the two models\n",
    "    overfit = (m1_overfit + m2_overfit)/2.0 \n",
    "    \n",
    "    return overfit, agr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (10 points) Train SVM models on random subsets of the data with different C (regularization constant) and two different size for the training data. In each case, use measure_overfitting_agreement() defined above to train the models and compute overfitting and agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-2,3,200)\n",
    "training_sizes = [100, 200]\n",
    "\n",
    "###* put your code here *###\n",
    "### your code should use 'SVC(C=<value from Cs>, kernel='linear')' as model instances\n",
    "### You should invoke measure_overfitting_agreement() to get the agreement rate and overfitting measure \n",
    "### (do this for all values in 'Cs' and both training sizes)\n",
    "### and store the results in some array(s) of your choice so that you can plot this for Task 5c\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5c] (5 points) Plot overfitting vs disagreement rate as a scatter plot for both training sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use matplotlib and supplement the provided code so it produces a single scatter plot of the two training set sizes\n",
    "### with overfitting on the x-axis and *disagreement* (i.e., 1 - agreement rate) on the y-axis.\n",
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "\n",
    "###* put your code here (~2 lines) *###\n",
    "### Invoke plt.scatter() first for Training Set Size: 100; use blue circle marker.\n",
    "### Invoke plt.scatter() again this time for Training Set Size: 200; use red square marker.\n",
    "\n",
    "\n",
    "\n",
    "## sets the axis labels, limits, etc.\n",
    "plt.xlabel('Overfitting Measure')\n",
    "plt.ylabel('Disagreement Rate (1 - Agreement Rate)')\n",
    "plt.xlim([-0.02, 0.3])\n",
    "plt.ylim([-0.02, 0.45])\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5d] (5 points) What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What do you conclude? Is agreement rate a possible way to measure overfitting? Can it be used to measure variance?\n",
    "# Describe your general observation of the correlation between these measures.\n",
    "# Hint: there is no single 'right' answer here, but there are many wrong answers!\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "#\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
