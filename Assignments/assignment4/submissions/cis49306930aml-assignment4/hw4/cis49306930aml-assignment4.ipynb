{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'enter your name'\n",
    "email = 'enter your email'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 3 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "##rng = np.random.default_rng(seed)  # best practice but not fully implemented in scikit-learn\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [14, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment we'll load the Bike Sharing dataset (hourly)\n",
    "### This dataset contains features of users bike sharing/rental on an hourly basis.\n",
    "### The task is to predict how many users are sharing/renting a bike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note: this dataset has missing values (artificially introduced), which you'll need to fill in before you can train a model\n",
    "df = pd.read_csv('../data/bikesharehour.csv.gz', compression='gzip', header=0, na_values='?')\n",
    "\n",
    "# Check that we loaded the data as expected\n",
    "df_expected_shape = (17379, 15)\n",
    "\n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what does the data look like?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some NaNs which we'll have to impute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the data as a numpy matrix\n",
    "all_xy = df.to_numpy()\n",
    "\n",
    "col_names = [c for c in df.columns]\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features: {} --- target: {}'.format(features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NaNs in each column?\n",
    "np.sum(np.isnan(all_xy), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe: no NaNs in the target/value column\n",
    "### About 1000+ NaNs in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y\n",
    "all_x_nan = all_xy[:,:-1]\n",
    "all_y = all_xy[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's impute the missing values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "###* put your code here (~2-3 lines) *###\n",
    "mf_imputer = SimpleImputer(missing_values=np.nan, strategy='median', copy=True)\n",
    "\n",
    "all_x_mf = mf_imputer.fit_transform(all_x_nan)\n",
    "all_x = all_x_mf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the shape is correct\n",
    "assert all_x.shape == (17379, 14)\n",
    "\n",
    "# check that there are no more NaNs\n",
    "assert np.sum(np.sum(np.isnan(all_x), axis=0)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we start, we'll min-max normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "scaler.fit(all_x) \n",
    "\n",
    "scaled_all_x = scaler.transform(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (35 points) Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a linear regression model that we can use as a point of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lrmodel = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement batch gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(X, y, gradient_fn, lr_schedule_fn, num_iter=1000, stop_fn = None, verbose=False):\n",
    "    (n, m) = X.shape\n",
    "    theta = np.random.uniform(low=-1.0, high=1.0, size=(X.shape[1],1)) # initialize uniformly at random in [-1, 1]\n",
    "    \n",
    "    for i in range(0, num_iter):\n",
    "        eta = lr_schedule_fn(i) # learning rate\n",
    "        \n",
    "        gradient = gradient_fn(X, y, theta) # calculate gradient vector\n",
    "        assert gradient.shape == theta.shape\n",
    "        \n",
    "        prev_theta = theta \n",
    "        \n",
    "        # update theta (actual gradient descent step)\n",
    "        theta = theta - eta * gradient\n",
    "        \n",
    "        # compute diff \n",
    "        diff = theta - prev_theta\n",
    "        l2ndiff = np.linalg.norm(diff)\n",
    "        \n",
    "        if verbose and i % (num_iter/20) == 0:\n",
    "            print('Iter {}, learning rate: {:.6f}, diff in theta (L2-norm): {:.6f}.'.format(i, eta, l2ndiff))\n",
    "            \n",
    "        if stop_fn is not None and stop_fn(diff):\n",
    "            if verbose:\n",
    "                print('Stop condition reached (iter {}).'.format(i))\n",
    "            break\n",
    "    \n",
    "    return theta.reshape(-1,), i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1a] (5 points) Define a constant learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a constant schedule, it should always return the learning rate eta (regardless of iteration)\n",
    "def constant_lr_schedule(eta, iteration):\n",
    "    ###* put your code here (~1 line) *###\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1b] (20 points) Implement gradient_mse() which calculates the gradient vector of MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For this you'll want to go back to the course slides (e.g., lecture 10 slide 7) or spend some time figuring out the gradient of MSE \n",
    "### (the loss) with respect to the parameters (i.e., theta which includes the weights vector w and bias b)\n",
    "### Note: asserts are there to help you ensure that things have the right shape. \n",
    "### If you get shape errors when running your code, you should think about what shape each component of the gradient should have.\n",
    "def gradient_mse(X, y, theta):\n",
    "    (n, m) = X.shape\n",
    "    \n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "    \n",
    "    ### Recall that the gradient of MSE is: - 2/n X^T (θ X - y)   (note: θ = theta)\n",
    "    ###* put your code here (~1-4 lines) *###\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's train the model (theta) using batch_gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a constant feature of 1 to each row to account for the bias term\n",
    "X_with_b = np.c_[np.ones((train_x.shape[0],1)), train_x]\n",
    "\n",
    "grad_fn = gradient_mse\n",
    "\n",
    "# use a lambda to define the constant schedule with the learning rate baked in\n",
    "learning_rate = 0.05\n",
    "lr_sched_fn = lambda i: constant_lr_schedule(learning_rate, i)\n",
    "\n",
    "# actually run the gradient descent and store the result in theta\n",
    "theta, _ = batch_gradient_descent(X_with_b, train_y, grad_fn, lr_sched_fn, num_iter=50000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1c] (5 points) Extract the parameters (w, b) from theta, then compare them to the parameters of the linear regression model (lrmodel). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights and bias for both models\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "print('Linear Regression model -- w: {}, b: {:.3f}'.format(lrmodel.coef_, lrmodel.intercept_))\n",
    "\n",
    "### extract (w,b) from theta and print them\n",
    "###* put your code here (~2 lines) *###\n",
    "\n",
    "\n",
    "print('Batch Gradient Descent model -- w: {}, b: {:.3f}'.format(w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1d] (5 points) What do you notice? Is it expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (35 points) Implementing Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (10 points) Fill in the implementation of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, gradient_fn, lr_schedule_fn, num_epochs=1000, stop_fn=None, verbose=False):\n",
    "    (n, m) = X.shape\n",
    "    theta = np.random.uniform(low=-1.0, high=1.0, size=(X.shape[1],1)) # initialize uniformly at random in [-1, 1]\n",
    "    \n",
    "    for i in range(0, num_epochs):\n",
    "        prev_theta = theta\n",
    "\n",
    "        # in each epoch go over the entire data\n",
    "        for j in range(0, n):\n",
    "            t = i*n+j\n",
    "            eta = lr_schedule_fn(t)\n",
    "            \n",
    "            ### Pick a *single* example out of the training data (X, y) uniformly at random, that is: \n",
    "            ### the feature vector and corresponding target/label\n",
    "            ### Call the feature vector 'xc' and the target/label 'yc'\n",
    "            ###* put your code here (~2-3 lines) *###\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            assert xc.shape == (m,) and yc.shape == ()\n",
    "            \n",
    "            # calculate gradients\n",
    "            gradient = gradient_fn(xc.reshape(1,-1), yc.reshape(-1,1), theta) \n",
    "            assert gradient.shape == theta.shape   \n",
    "            \n",
    "            # update theta (actual gradient descent step)\n",
    "            theta = theta - eta * gradient\n",
    "\n",
    "        # compute diff \n",
    "        diff = theta - prev_theta\n",
    "        l2ndiff = np.linalg.norm(diff)\n",
    "\n",
    "        if verbose and i % (num_epochs/20) == 0:\n",
    "            print('Epoch {}, learning rate: {:.9f}, diff in theta (L2-norm): {:.6f}.'.format(i, eta, l2ndiff))\n",
    "\n",
    "        if stop_fn is not None and stop_fn(diff):\n",
    "            if verbose:\n",
    "                print('Stop condition reached (iter {}).'.format(i))\n",
    "            break\n",
    "    \n",
    "    return theta.reshape(-1,), i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train the model for 250 epochs with a constant learning schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a lambda to define the constant schedule with the learning rate baked in\n",
    "learning_rate = 0.05\n",
    "lr_sched_fn = lambda i: constant_lr_schedule(learning_rate, i)\n",
    "\n",
    "theta, _ = stochastic_gradient_descent(X_with_b, train_y, gradient_mse, lr_sched_fn, num_epochs=250, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = theta[0]\n",
    "w = theta[1:]\n",
    "\n",
    "# Print the weights and bias for both models\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "print('Linear Regression model -- w: {}, b: {:.3f}'.format(lrmodel.coef_, lrmodel.intercept_))\n",
    "print('Stochastic Gradient Descent model -- w: {}, b: {:.3f}'.format(w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (10 points) Provide an explanation as to what is happening? Is the process converging? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an explanation as to what is happening? Is the process converging? Explain why.\n",
    "###* put your answer here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2c] (5 points) Train the model with a simple learning schedule that decreases the learning rate over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a lambda to define a simple schedule that decreases the learning rate over time\n",
    "learning_rate = 0.05\n",
    "lr_sched_fn = lambda t: learning_rate / (1 + np.sqrt(t))\n",
    "\n",
    "# actually run the stochastic_gradient_descent for 250 epochs and store the result in 'theta'\n",
    "# make sure to use the simple learning schedule defined above (lr_sched_fn). Also set verbose=True\n",
    "###* put your code here (~1 line) *###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = theta[0]\n",
    "w = theta[1:]\n",
    "\n",
    "# Print the weights and bias for both models\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "print('Linear Regression model -- w: {}, b: {:.3f}'.format(lrmodel.coef_, lrmodel.intercept_))\n",
    "print('Stochastic Gradient Descent model -- w: {}, b: {:.3f}'.format(w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2d] (10 points) Show that the model has similar performance as the linear regression model (lrmodel). For this, show the coefficient of determination, the RMSE, and the MedAE for both on the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given model parameters 'theta' and a feature matrix 'x', this will return predictions\n",
    "def predict_theta(theta, x):\n",
    "    b = theta[0]\n",
    "    w = theta[1:]\n",
    "    \n",
    "    assert w.shape[0] == x.shape[1]\n",
    "    \n",
    "    return np.dot(w, x.T) + b\n",
    "    \n",
    "from sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\n",
    "\n",
    "### You can implement this however you like. A simple way is to define a function to calculate and print \n",
    "### the scores and then call it for each model and dataset (train or val). For example, this function could have \n",
    "### the following signature: 'def print_scores(desc, true_y, pred_y):'\n",
    "### Hint: use predict_theta() for getting predictions from the model trained with SGD.\n",
    "###* put your code here (~7-10 lines) *###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (20 points) Mini-Batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (5 points) Complete the implementation of mini_batch_sgd to include a callback after each epoch. This callback will be useful to obtain information during the optimization process. If the callback function is defined, your code should call it with the proper arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_sgd(X, y, gradient_fn, lr_schedule_fn, num_epochs=1000, batch_size=100, callback_fn=None, stop_fn=None, verbose=False):\n",
    "    \n",
    "    (n, m) = X.shape\n",
    "    theta = np.random.uniform(low=-1.0, high=1.0, size=(X.shape[1],1)) # initialize uniformly at random in [-1, 1]\n",
    "    \n",
    "    batch_size = np.minimum(n, batch_size)\n",
    "    \n",
    "    for epoch in range(0, num_epochs):\n",
    "        prev_theta = theta\n",
    "\n",
    "        # shuffle the data\n",
    "        pi = np.random.permutation(n)\n",
    "        Xshuf = X[pi,:]\n",
    "        yshuf = y[pi]\n",
    "        for batch_start in range(0, n, batch_size):\n",
    "            \n",
    "            batch_idx = batch_start/batch_size\n",
    "            bsidx = batch_start\n",
    "            beidx = np.minimum(n, bsidx + batch_size)\n",
    "            Xmb = Xshuf[bsidx:beidx,:]\n",
    "            ymb = yshuf[bsidx:beidx]\n",
    "            \n",
    "            eta = lr_schedule_fn(epoch*n + bsidx)\n",
    "            \n",
    "            # grab gradient vector\n",
    "            gradient = gradient_fn(Xmb, ymb, theta)\n",
    "            assert gradient.shape == theta.shape  \n",
    "            \n",
    "            # update theta (actual gradient descent step)\n",
    "            theta = theta - eta * gradient\n",
    "            \n",
    "        \n",
    "        ### If callback_fn is defined (not None), call it and pass it the current epoch and current set of parameters\n",
    "        ###* put your code here (~2 lines) *###\n",
    "        \n",
    "        \n",
    "\n",
    "        # compute diff \n",
    "        diff = theta - prev_theta\n",
    "        l2ndiff = np.linalg.norm(diff)\n",
    "\n",
    "        if verbose and epoch % (num_epochs/20) == 0:\n",
    "            print('Epoch {}, learning rate: {:.9f}, diff in theta (L2-norm): {:.6f}.'.format(epoch, eta, l2ndiff))\n",
    "\n",
    "        if stop_fn is not None and stop_fn(diff):\n",
    "            if verbose:\n",
    "                print('Stop condition reached (iter {}).'.format(i))\n",
    "            break\n",
    "            \n",
    "    return theta.reshape(-1,), epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3b] (5 points) Fill in the implementation of the callback function to save the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the following plotting function to explore our loss during gradient descent\n",
    "def plot_process_data(proc_data, ylabel, xlim=None, ylim=None):\n",
    "    plt.figure(figsize=(15,8))\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    plt.plot(np.arange(proc_data.shape[0]), proc_data[:,0], 'b--', linewidth=3, label='Training')\n",
    "    plt.plot(np.arange(proc_data.shape[0]), proc_data[:,1], 'r:', linewidth=3, label='Validation')\n",
    "\n",
    "    plt.legend()\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# This will be our callback function. \n",
    "# After each epoch, this function gets called so we can check on the progress of gradient descent\n",
    "# We'll save the RMSE for both train and val in the 'rmse' array so we can plot it later\n",
    "def rmse_per_epoch(epoch, theta, rmse, train_x, train_y, val_x, val_y):\n",
    "    theta = theta.reshape(-1,)\n",
    "    pred_y = predict_theta(theta, train_x)\n",
    "    \n",
    "    ### compute the RMSE on the training data, store the result in 'rmse[epoch, 0]'\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    \n",
    "    \n",
    "    pred_y = predict_theta(theta, val_x)\n",
    "    \n",
    "    ### compute the RMSE on the validation data, store the result in 'rmse[epoch, 1]'\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    \n",
    "\n",
    "\n",
    "# params for gradient descent\n",
    "num_epochs = 500\n",
    "bsz = 100\n",
    "\n",
    "# define our array to store the rmse_data. One row per epoch, column 0 will be train, column 1 will be val.\n",
    "rmse_data = np.zeros((num_epochs,2))\n",
    "\n",
    "# define the callback function\n",
    "ecbfn = lambda i, t : rmse_per_epoch(i, t, rmse_data, train_x, train_y, val_x, val_y)\n",
    "\n",
    "# use a lambda to define a simple schedule that decreases the learning rate slowly over time\n",
    "learning_rate = 0.05\n",
    "lr_sched_fn = lambda t: learning_rate / (1 + np.log(1 + t)) \n",
    "\n",
    "theta, _ = mini_batch_sgd(X_with_b, train_y, grad_fn, lr_sched_fn, \n",
    "                          num_epochs=num_epochs, batch_size=bsz, callback_fn=ecbfn, verbose=False)\n",
    "\n",
    "# do the actual plotting\n",
    "plot_process_data(rmse_data, 'RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3c] (5 points) Does the process converge? Should the number of epochs be increased? What about the learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3d] (10 points) What do you conclude? Is it worth training a linear regression model this way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (10 points) Use SGDRegressor to train a similar that you did in Task 3. Show that the performance of both models is comparable (show R^2, RMSE, MedAE for both train and val). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "### Tip: if you defined a print_scores() method for Task 2d, you can reuse it here!\n",
    "###* put your code here (~5 lines) *###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): Ridge Regression with Mini-batch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this task, you will implement ridge regression using mini-batch SGD as defined in Task 3. \n",
    "### The main task is to derive the gradient vector for Ridge Regression (L2 regularization with MSE as loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (15 points) Implement the gradient_mse_ridge() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_mse_ridge(X, y, theta, lmbda):\n",
    "    (n, m) = X.shape\n",
    "    \n",
    "    y = y.reshape(-1,1)\n",
    "    assert y.shape == (n,1)\n",
    "    assert theta.shape == (m,1)\n",
    "    \n",
    "    ### Figure out the gradient for ridge regression and implement this function \n",
    "    ### (You can refer to the course slides: lecture 5 on linear models, slide 12.)\n",
    "    ### Note 1: use 'lmbda' (lambda) -- the regularization hyperparameter.\n",
    "    ### Note 2: normally we do not regularize the bias term b, but if you cannot avoid it is acceptable to regularize it for this task.\n",
    "    ###* put your code here (~1-4 lines) *###\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's put some code to do the mini-batch sgd ridge regression training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mbsgd_ridge(lmbda):\n",
    "    num_epochs = 1000\n",
    "    bsz = 100\n",
    "\n",
    "    rmse_data = np.zeros((num_epochs,2))\n",
    "\n",
    "    # define the callback function\n",
    "    ecbfn = lambda i, t : rmse_per_epoch(i, t, rmse_data, train_x, train_y, val_x, val_y)\n",
    "\n",
    "    # use a lambda to define a simple schedule that decreases the learning rate over time\n",
    "    learning_rate = 0.05\n",
    "    lr_sched_fn = lambda t: learning_rate / (1 + np.log(1 + t))\n",
    "\n",
    "    # bake in lambda into the gradient fn\n",
    "    grad_fn = lambda X, y, thet : gradient_mse_ridge(X, y, thet, lmbda)\n",
    "\n",
    "    theta, _ = mini_batch_sgd(X_with_b, train_y, grad_fn, lr_sched_fn, \n",
    "                              num_epochs=num_epochs, batch_size=bsz, callback_fn=ecbfn, verbose=False)\n",
    "\n",
    "    # do the actual plotting\n",
    "    plot_process_data(rmse_data, 'RMSE')\n",
    "\n",
    "    ### if you have defined print_scores above, you can uncomment the following lines.\n",
    "    #print_scores('SGD (Manual) Train', train_y, predict_theta(theta, train_x))\n",
    "    #print_scores('SGD (Manual) Val', val_y, predict_theta(theta, val_x))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (5 points) In theory, what is the effect of L2 regularization on the weights vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5c] (5 points) Run mini-batch SGD for Ridge regression using whatever value of lambda you think is appropriate. Make sure that the process stills converges and that you end up with a model with a comparable performance to the ones you trained in previous tasks (but that still shows some effect from L2 regularization). Show the effect of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your code here (~1 line) to set the value of lambda ('lmbda') *###\n",
    "\n",
    "\n",
    "\n",
    "theta = do_mbsgd_ridge(lmbda)\n",
    "\n",
    "b = theta[0]\n",
    "w = theta[1:]\n",
    "\n",
    "print('Linear Regression model -- w: {}, b: {:.3f}'.format(lrmodel.coef_, lrmodel.intercept_))\n",
    "print('Mini-batch SGD Ridge -- w: {}, b: {:.3f}\\n'.format(w, b))\n",
    "\n",
    "\n",
    "###* put your code/answer here *###\n",
    "#\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
