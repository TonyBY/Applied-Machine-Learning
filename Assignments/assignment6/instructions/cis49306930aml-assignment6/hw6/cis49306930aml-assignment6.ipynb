{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Training Fully-connected Neural Networks and CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'enter your name'\n",
    "email = 'enter your email'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 6 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import sklearn\n",
    "    \n",
    "    # we'll use tensorflow and keras for neural networks\n",
    "    import tensorflow as tf\n",
    "    import tensorflow.keras as keras\n",
    "    \n",
    "    # Load the TensorBoard notebook extension\n",
    "    #%load_ext tensorboard\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('### Tensorflow version: ' + tf.__version__)\n",
    "    print('### TF Keras version: ' + keras.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment we'll use the MNIST dataset, which contains images of handwritten digits.\n",
    "### The task is to predict what digit a given image contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, all_x, all_y = utils.load_preprocess_mnist_data(onehot=True, prop_vec=prop_vec, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's setup some performance evaluation and plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a custom callback class\n",
    "class PerfEvalCustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, perf_data):\n",
    "        self.perf_data = perf_data\n",
    "    \n",
    "    # we define the on_epoch_end callback and save the loss and accuracy in perf_data\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.perf_data[epoch,0] = logs['loss']\n",
    "        self.perf_data[epoch,1] = logs['accuracy']\n",
    "        self.perf_data[epoch,2] = logs['val_loss']\n",
    "        self.perf_data[epoch,3] = logs['val_accuracy']\n",
    "\n",
    "    def get_perf_data():\n",
    "        return self.perf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the model's performance during training (across epochs)\n",
    "def plot_training_perf(train_loss, train_acc, val_loss, val_acc, fs=(8,5)):\n",
    "    plt.figure(figsize=fs)\n",
    "\n",
    "\n",
    "    assert train_loss.shape == val_loss.shape and train_loss.shape == val_acc.shape and val_acc.shape == train_acc.shape\n",
    "    \n",
    "    # assume we have one measurement per epoch\n",
    "    num_epochs = train_loss.shape[0]\n",
    "    epochs = np.arange(0, num_epochs)\n",
    "    \n",
    "    # Can you figure out why this makes sense? Why remove -0.5?\n",
    "    plt.plot(epochs-0.5, train_loss, 'm', linewidth=2,  label='Loss (Training)')\n",
    "    plt.plot(epochs-0.5, train_acc, 'r--', linewidth=2, label='Accuracy (Training)')\n",
    "    \n",
    "    plt.plot(epochs, val_loss, 'g', linewidth=2, label='Loss (Validation)')\n",
    "    plt.plot(epochs, val_acc, 'b:', linewidth=2, label='Accuracy (Validation)')\n",
    "    \n",
    "    \n",
    "    plt.xlim([0, num_epochs])\n",
    "    plt.ylim([0, 1.05])\n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (10 points) Complete (& Customize) Your Model Training and Evaluation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1a] (10 points) Complete the implementation of evaluate_model(). You can customize it to add whatever evaluation functionality you like (e.g., classification report, error analysis, etc.). Make sure you like the way it shows you the information: you will use it throughout this assignment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize this function as you like but makes sure it is implemented correctly.    \n",
    "# Note: If you need to change the method definition to add more arguments, make sure to make \n",
    "# the new arguments are optional (& have a sensible default value)\n",
    "def evaluate_model(name, model, eval_data, \n",
    "                   plot_training=True, evaluate_on_test_set=True):\n",
    "    \n",
    "    # unpack the stuff\n",
    "    perf_data, dataset = eval_data\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = dataset\n",
    "    \n",
    "    # get predictions from the model\n",
    "    train_preds = model.predict(train_x)\n",
    "    val_preds = model.predict(val_x)\n",
    "    \n",
    "    # measure the accuracy (as categorical accuracy since we have a softmax layer)\n",
    "    catacc_metric = keras.metrics.CategoricalAccuracy()\n",
    "    catacc_metric.update_state(train_y, train_preds)\n",
    "    train_acc = catacc_metric.result()\n",
    "    \n",
    "    catacc_metric = keras.metrics.CategoricalAccuracy()\n",
    "    catacc_metric.update_state(val_y, val_preds)\n",
    "    val_acc = catacc_metric.result()\n",
    "    print('[{}] Training Accuracy: {:.3f}%, Validation Accuracy: {:.3f}%'.format(name, 100*train_acc, 100*val_acc))\n",
    "    \n",
    "    if plot_training:\n",
    "        plot_training_perf(perf_data[:,0], perf_data[:,1], perf_data[:,2], perf_data[:,3])\n",
    "        \n",
    "    if evaluate_on_test_set:\n",
    "        ### Evaluate the model on the test data  and put the results in 'test_loss', 'test_acc' (set verbose=0)\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        \n",
    "        print('[{}] Test loss: {:.5f}, test accuracy: {:.3f}%'.format(name, test_loss, 100*test_acc))\n",
    "        \n",
    "    # You can add stuff here\n",
    "    ###* put your code here (0+ lines) *###\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "# this is what we call to do the training\n",
    "def train_model(model, max_epochs=25, batch_size=100, verbose=0, \n",
    "                   dataset=(train_x, train_y, val_x, val_y, test_x, test_y)):\n",
    "\n",
    "    # unpack dataset\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = dataset\n",
    "    \n",
    "    # this is the callback we'll use for early stopping\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=4)\n",
    "    \n",
    "    # setup the performance data callback\n",
    "    perf_data = np.zeros((max_epochs, 4))\n",
    "    perf_eval_cb = PerfEvalCustomCallback(perf_data)\n",
    "    \n",
    "    hobj = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                     shuffle=True, callbacks=[perf_eval_cb, early_stop_cb], verbose=verbose)\n",
    "    \n",
    "    eff_epochs = len(hobj.history['loss'])\n",
    "    eval_data = (perf_data[0:eff_epochs,:], dataset) # tuple of evaluation data\n",
    "    \n",
    "    return eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the following Tasks (Tasks 2 & 3) you are given code to define the architecture of a model and compile it. But there is an issue for each model (it's broken in some way), which you need to identify and fix. All the models (if fixed) should achieve 95%+ val/test accuracy\n",
    "\n",
    "## To diagnose the issue you need to observe the broken model's training process. Then you need to fix it by making minimal changes. You will add 'if fixed:' branches in the code to implement the fix without modify the behavior in anyway whenever 'fixed=False'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (30 points) Diagnosing Simple Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (10 points) Consider the following model. It has one obvious problem which prevents the model from learning: the output layer's activation function and the loss are inconsistent. Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!\n",
    "\n",
    "### Hint: since the output layer's activation function is softmax (which makes sense since we have one-hot encoded class labels), the loss should be the cross entropy loss (see tf.keras losses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_model0(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    name = 'Model0--Fixed' if fixed else 'Model0--Broken'\n",
    "    hidden_widths=[300, 100]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(1/hw)),\n",
    "                                     bias_initializer=keras.initializers.Zeros()))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros()))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.0025)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        # comment/remove this line once you implement the fix\n",
    "        raise NotImplementedError \n",
    "        \n",
    "    else:\n",
    "        model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model0(False) \n",
    "\n",
    "# train\n",
    "eval_data = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's check if you fixed the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create and compile the model for fixed=True, train it, then evaluate it\n",
    "name, model = create_compile_model0(True, verbose=False) \n",
    "\n",
    "# train\n",
    "eval_data = train_model(model)\n",
    "\n",
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the model works now; it achieves about 96% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (5 points) Just like task 2a, consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!\n",
    "\n",
    "### Note: the model is different in some ways to model0 but most of these ways are unrelated to the problem you are asked to diagnose and fix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_model1(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    ###* Move the if fixed branch to where you want to use it, and create a corresponding else to preserve \n",
    "    ###* the broken functionality when fixed=False (if necessary)\n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        # comment/remove this line once you implement the fix\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    name = 'Model1--Fixed' if fixed else 'Model1--Broken'\n",
    "    hidden_widths=[256, 128, 48]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(1/hw)),\n",
    "                                     bias_initializer=keras.initializers.Zeros(), use_bias=False))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros(), use_bias=True))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.025)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model1(False) \n",
    "\n",
    "# train the model (hint you can set verbose to 1 to get more information during the training process)\n",
    "eval_data = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2c] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, model = create_compile_model1(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2d] (5 points) Just like task 2a and task 2b, consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_model2(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    ###* Move the if fixed branch to where you want to use it, and create a corresponding else to preserve \n",
    "    ###* the broken functionality when fixed=False (if necessary)\n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        # comment/remove this line once you implement the fix\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    name = 'Model2--Fixed' if fixed else 'Model2--Broken'\n",
    "    hidden_widths=[400, 96]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     use_bias=True))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output', use_bias=False))\n",
    "    \n",
    "    opt = keras.optimizers.RMSprop(lr=1e-7)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model2(False) \n",
    "\n",
    "# train the model (hint you can set verbose to 1 to get more information during the training process)\n",
    "eval_data = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2e] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, model = create_compile_model2(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (30 points) Diagnosing Other Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (5 points) Just like in task 2, consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_model3(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    ###* Move the if fixed branch to where you want to use it, and create a corresponding else to preserve \n",
    "    ###* the broken functionality when fixed=False (if necessary)\n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        # comment/remove this line once you implement the fix\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    name = 'Model3--Fixed' if fixed else 'Model3--Broken'\n",
    "    hidden_widths=[512, 128, 32, 24]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(stddev=0.001), use_bias=True))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='tanh', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros(), use_bias=True))\n",
    "\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.995, epsilon=1e-07, amsgrad=False)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model3(False) \n",
    "\n",
    "# train the model (hint you can set verbose get more or less information during the training process)\n",
    "eval_data = train_model(model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3b] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name, model = create_compile_model3(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3c] (5 points) Consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_model4(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    ###* Move the if fixed branch to where you want to use it, and create a corresponding else to preserve \n",
    "    ###* the broken functionality when fixed=False (if necessary)\n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        # comment/remove this line once you implement the fix\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    name = 'Model4--Fixed' if fixed else 'Model4--Broken'\n",
    "    hidden_widths=[512, 128, 3, 24]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i)))\n",
    "        \n",
    "    \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros(), use_bias=True))\n",
    "    \n",
    "    opt = keras.optimizers.Nadam(lr=0.001)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model4(False) \n",
    "\n",
    "# train the model (hint you can set verbose get more or less information during the training process)\n",
    "eval_data = train_model(model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3d] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name, model = create_compile_model4(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3e] (5 points) Consider the following model. It has several problems which prevent it from learning: can you figure out what those problems are? Fix them by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation! You can have more than one \"if fixed:\" branch if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_model5(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    ###* Move the if fixed branch to where you want to use it, and create a corresponding else to preserve \n",
    "    ###* the broken functionality when fixed=False (if necessary)\n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "        # comment/remove this line once you implement the fix\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    name = 'Model5--Fixed' if fixed else 'Model5--Broken'\n",
    "    hidden_widths=[256, 128, 24]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,))) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='sigmoid', name='hidden_{}'.format(i)))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='linear', name='output',\n",
    "                                bias_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                kernel_initializer=keras.initializers.Zeros(), use_bias=False))\n",
    "                  \n",
    "    \n",
    "    opt = keras.optimizers.Nadam(lr=0.01, beta_2=0.999, epsilon=1e-07, clipvalue=1.0)\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model5(False) \n",
    "\n",
    "# train the model (hint you can set verbose get more or less information during the training process)\n",
    "eval_data = train_model(model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3f] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name, model = create_compile_model5(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (30 points) Training CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this task, you will train a convolutional neural network with an architecture you define. The goal is (of course) to train the best possible model, but the constraint is that the number of parameters must not exceed 500k (500,000). We will aim to achieve test/val accuracy above 98.5%.\n",
    "\n",
    "### If you do this on a machine with a GPU, it will be very fast. Otherwise it may take 5 or 10 minutes to train the model for a few epochs, so we will use a subset of the training data for developing and testing the model.\n",
    "\n",
    "#### Note: it is possible to achieve reasonably high accuracy (above 98.5%) with only about 50k parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use a convolutional architecture we need to reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_subset = True\n",
    "tr_sz = 5000\n",
    "\n",
    "# uncomment this line to use the full data\n",
    "# use_subset = False \n",
    "\n",
    "# reshape for use with CNN\n",
    "train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "val_x = val_x.reshape(-1, 28, 28, 1)\n",
    "test_x = test_x.reshape(-1, 28, 28, 1)\n",
    "\n",
    "cnn_dataset = (train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "if use_subset:\n",
    "    cnn_dataset=(train_x[:tr_sz], train_y[:tr_sz], val_x, val_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (15 points) Fill in the implementation of create_compile_cnn() below with your chosen architecture. Ensure the total number of parameters chosen does not exceed 500k. You can look for insipiration for architectures on the web (or in books) if you like, but in that case you must provide a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_cnn(input_shape=(28, 28, 1), num_outputs=10, verbose=False):\n",
    "    name = 'CNN'\n",
    "    assert train_x.shape[1:] == input_shape # sanity check\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    ### You can use any architecture but the output layer (with softmax activation is fixed). \n",
    "    ### You can also use whatever optimizer 'opt' you want (with your choice hyperparameters values)\n",
    "    ### But the call to model.summary() must show that the number of parameters is within the constraint\n",
    "    ### Note: you can import layers from keras.layers to make the code more compact.\n",
    "    ###* put your code here (~10-20 lines) *###\n",
    "\n",
    "    \n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, cnn_model = create_compile_cnn(verbose=True)\n",
    "\n",
    "# note: you can change the number of epochs to train as long as it's reasonable\n",
    "epochs = 30 if use_subset else 15\n",
    "    \n",
    "# note: you can change how you call train_model as necessary (e.g., if you want to change the batch_size)\n",
    "eval_data = train_model(cnn_model, max_epochs=epochs, dataset=cnn_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate your CNN model. Does it achieve 98.5+% val/test accuracy?\n",
    "#### *note: once you are confident in your model architecture, you MUST switch back to the full dataset*\n",
    "#### to train the final version of the model (depending on your chosen architecture this could take 10-15 minutes if you are not on machine with GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(name, cnn_model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4b] (5 points) How long (in seconds/minutes) did your model take to train on the full dataset? Did you use a GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4c] (10 points) Explain how you decided on this architecture. (2-3 sentences is okay.) If you took inspiration from resources such as books/webpages, it's okay but you should include a reference in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): CNN vs. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this task we want to understand the impact of data augmentation on model quality. We pretend that we only have access to 2k examples from the MNIST training set and then multiply its size (by 5X) using data augmentation.\n",
    "\n",
    "### The idea is to compare the performance of three models (all of which using your CNN architecture from Task 4): (1) the CNN trained on tr_x, ty_y (2k examples), (2) the CNN trained on the augmented data (10k examples), and (3) the CNN trained on 10k examples from the MNIST training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (15 points) Complete the code below to use data augmentation to produce a dataset of size 'data_aug_sz' from the tr_x and tr_y. You can use tf.keras' ImageDataGenerator with parameters chosen by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# dataset 1 -- 2k examples from mnist\n",
    "tr_sz = 2000\n",
    "cnn_dataset1 = (train_x[:tr_sz], train_y[:tr_sz], val_x, val_y, test_x, test_y)\n",
    "\n",
    "# dataset 2 -- data augmentation (10k) starting from 2k examples from mnist\n",
    "tr_x = train_x[:tr_sz]\n",
    "tr_y = train_y[:tr_sz]\n",
    "\n",
    "data_aug_sz = 10000\n",
    "\n",
    "### Note: you will want to look very closely at the documentation of ImageDataGenerator (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n",
    "### in order to make sure you do not do strange things (e.g., it might not be a good idea to have vertical_flip=True, etc.)...\n",
    "### Store the augmented data into aug_tr_x and aug_tr_y\n",
    "###* put your code here (~5-10 lines) *###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# store the result\n",
    "cnn_dataset2 = (aug_tr_x, aug_tr_y, val_x, val_y, test_x, test_y)\n",
    "assert aug_tr_x.shape[0] == data_aug_sz and aug_tr_y.shape[0] == data_aug_sz\n",
    "\n",
    "# dataset 3 -- 10k examples from mnist\n",
    "cnn_dataset3 = (train_x[:data_aug_sz], train_y[:data_aug_sz], val_x, val_y, test_x, test_y)\n",
    "assert aug_tr_x.shape == cnn_dataset3[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's evaluate all three models\n",
    "\n",
    "datasets = (cnn_dataset1, cnn_dataset2, cnn_dataset3)\n",
    "names = ['Model 1 (train size: {})'.format(tr_sz), 'Model 2 Data Augmented (train size: {})'.format(data_aug_sz),\n",
    "         'Model 3 (train size: {})'.format(data_aug_sz)]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    name, cnn_model = create_compile_cnn(verbose=False)\n",
    "\n",
    "    # note: you can change how you call train_model as necessary (e.g., if you want to change the batch_size)\n",
    "    eval_data = train_model(cnn_model, max_epochs=25, dataset=dataset, verbose=0)\n",
    "\n",
    "    print('\\n---------- {} -----------'.format(names[i]))\n",
    "    evaluate_model(name, cnn_model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (10 points) What do you conclude about data augmentation? Does it help if you don't have enough data? Is it as good as having more real data? (A few sentences is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer here *###\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
