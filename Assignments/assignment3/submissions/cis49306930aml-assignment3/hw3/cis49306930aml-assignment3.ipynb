{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Regression & Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'enter your name'\n",
    "email = 'enter your email'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 3 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "##rng = np.random.default_rng(seed)  # best practice but not fully implemented in scikit-learn\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [14, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment we'll load the Bike Sharing dataset (hourly)\n",
    "### This dataset contains features of users bike sharing/rental on an hourly basis.\n",
    "### The task is to predict how many users are sharing/renting a bike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Note: this dataset has missing values (artificially introduced), which you'll need to fill in before you can train a model\n",
    "df = pd.read_csv('../data/bikesharehour.csv.gz', compression='gzip', header=0, na_values='?')\n",
    "\n",
    "# Check that we loaded the data as expected\n",
    "df_expected_shape = (17379, 15)\n",
    "\n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## what does the data look like?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some NaNs which we'll have to impute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the data as a numpy matrix\n",
    "all_xy = df.to_numpy()\n",
    "\n",
    "col_names = [c for c in df.columns]\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('features: {} --- target: {}'.format(features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many NaNs in each column?\n",
    "np.sum(np.isnan(all_xy), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe: no NaNs in the target/value column\n",
    "### About 1000+ NaNs in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y\n",
    "all_x_nan = all_xy[:,:-1]\n",
    "all_y = all_xy[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (10 points) Let's impute the missing values! Use Scikit-learn's SimpleImputer to replace all NaNs in 'all_x_nan' with the *most frequent* value in each column. Use copy=True and store the results in 'all_x' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "###* put your code here (~2-3 lines) *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the shape is correct\n",
    "assert all_x.shape == (17379, 14)\n",
    "\n",
    "# check that there are no more NaNs\n",
    "assert np.sum(np.sum(np.isnan(all_x), axis=0)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll min-max normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "scaler.fit(all_x) \n",
    "\n",
    "scaled_all_x = scaler.transform(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (30 points) Let's train linear models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (2 points) Train a Linear Regression model using the default hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### Train a linear regression model on the training data (train_x, train_y)\n",
    "### Call the resulting trained model 'lrmodel'\n",
    "###* put your code here (~1 line) *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def r2_mse_mae_eval(model, pref=''):\n",
    "    # R^2 the coefficient of determination\n",
    "    r2_train = model.score(train_x, train_y)\n",
    "    r2_val = model.score(val_x, val_y)\n",
    "\n",
    "    print('{}Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(pref, r2_train, r2_val))\n",
    "\n",
    "    train_pred = model.predict(train_x)\n",
    "    val_pred = model.predict(val_x)\n",
    "\n",
    "    # measure the error (MSE) wrt true target\n",
    "    train_error = mean_squared_error(train_pred, train_y)\n",
    "    val_error = mean_squared_error(val_pred, val_y)\n",
    "\n",
    "    print('{}Train MSE: {:.3f}, Val MSE: {:.3f}'.format(pref, train_error, val_error))\n",
    "    \n",
    "    train_error = mean_absolute_error(train_pred, train_y)\n",
    "    val_error = mean_absolute_error(val_pred, val_y)\n",
    "\n",
    "    print('{}Train MAE: {:.3f}, Val MAE: {:.3f}'.format(pref, train_error, val_error))\n",
    "    \n",
    "r2_mse_mae_eval(lrmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (3 points) How good is that model? (A few sentences is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's setup some functions so we can tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some code to do a grid search and automatically train & evaluate the model with the best hyperparams.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def do_grid_search(model, param_grid, x, y):\n",
    "    gs = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error')\n",
    "    gs_res = gs.fit(x, y)\n",
    "    return  gs_res.best_params_\n",
    "\n",
    "\n",
    "def search_train_eval(model, param_grid, tr_x=train_x, tr_y=train_y, v_x=val_x, v_y=val_y):\n",
    "    \n",
    "    # since we do CV for the grid search, let's concatenate the train and val sets for it\n",
    "    search_x = np.r_[tr_x, v_x]\n",
    "    search_y = np.r_[tr_y, v_y]\n",
    "    \n",
    "    hyperparams = do_grid_search(model, param_grid, search_x, search_y)\n",
    "    \n",
    "    class_obj = type(model)\n",
    "    m = class_obj(**hyperparams).fit(tr_x, tr_y)\n",
    "    \n",
    "    cn = str(class_obj).split(\"'\")[1]\n",
    "    cn = cn.split('.')[-1]\n",
    "    print('{}({})'.format(cn, hyperparams))\n",
    "\n",
    "    r2_mse_mae_eval(m, pref='\\t')\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2c] (5 points) Do a grid search to tune hyperparameters and train an ElasticNet model. You can choose the values of hyperparameters your search over, but you must search over 'alpha' and 'l1_ratio'. Ensure that during the search, the training of models converges in all cases (you may need to increase 'max_iter' based on your chosen values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hint: you should define a parameter grid dictionary and call search_train_eval() to do the actual search\n",
    "### Note: you only need to pass a model instance (model) and a parameter grid (param_grid)\n",
    "### Call the output of search_train_eval(): 'enmodel'\n",
    "###* put your code here (~3 lines) *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2d] (2 points) Do a grid search to tune hyperparameters and train a Ridge Regression model. You can choose the values of hyperparameters your search over, but you must search over 'alpha'. Ensure that during the search, the training of models converges in all cases (you may need to increase 'max_iter' based on your chosen values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call the output of search_train_eval(): 'rmodel'\n",
    "###* put your code here (~3 lines) *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2e] (3 points) Print the parameter values (w and b) of the ElasticNet and Ridge Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights and bias for both models\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "### Make sure you print the weights and bias for both models and that it is clear which is which.\n",
    "###* put your code here (~2 lines) *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2f] (2 points) How similar are the parameter values of the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2g] (8 points) For each of the two models, display the three most important features alongside with their coefficients. Are these the same across both models?\n",
    "### What are the coefficients? Which feature is the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hint: don't forget that coefficients can be positive as well as negative.\n",
    "###* put your code here *###\n",
    "\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "# \n",
    "# \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2h] (5 points) Take a look at the code of search_train_eval() and do_grid_search(). Answer the following questions: \n",
    "### 1. Why is the scoring function for the grid search 'neg_mean_squared_error' (as opposed to 'mean_squared_error')? \n",
    "### 2. Why is it okay to do the search over search_x and search_y which are the concatenation of the training and validation sets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hint: take a look at the documentation of scikit-learn for GridSearchCV and related classes.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# 1.\n",
    "#\n",
    "#\n",
    "# 2. \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (30 points) Let's train polynomial regression models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (5 points) Use PolynomialFeatures to create a version of the data with all features of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "### Use PolynomialFeatures to create a version of the data with all features of degree 2. Make sure to allow interactions (interaction_only=False) and set include_bias=False.\n",
    "### Store the result in 'all_x_polyf'. Ensure that you make a copy of the original data and you use the scaled features ('scaled_all_x')!\n",
    "###* put your code here (~2 lines) *###\n",
    "\n",
    "\n",
    "\n",
    "assert all_x_polyf.shape == (17379, 119)\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_polyf, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a LinearRegression model and a Ridge model on our polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model\n",
    "pf_lrmodel = LinearRegression().fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_lrmodel)\n",
    "\n",
    "print()\n",
    "\n",
    "# Train a Ridge regression model\n",
    "pf_ridgemodel = Ridge(alpha=0.5).fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_ridgemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3b] (5 points) What is the difference between LinearRegression and Ridge? (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3c] (5 points) Look at (e.g., print) the parameters of both the LinearRegression model ('pf_lrmodel') and the Ridge model ('pf_ridgemodel'). What do you notice? Explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your code here *###\n",
    "\n",
    "\n",
    "###  What do you notice? Explain what is going on.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3d] (5 points) Focus on the Ridge model. What are the three most important features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print the three most important features alongside with their weights.\n",
    "### Remember: weights can be positive as well as negative.\n",
    "### Hint: you can use the get_feature_names() method of PolynomialFeatures to relate polynomial features to the original features.\n",
    "###* put your code here *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3e] (5 points) Let's use only these three most important features. Extract the three features from the polynomial features to create a new feature matrix with three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the three features from 'all_x_polyf' and store the results in 'all_x_3most'\n",
    "###* put your code here *###\n",
    "\n",
    "\n",
    "\n",
    "assert all_x_3most.shape == (17379, 3)\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_3most, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3f] (2 points) Now train a LinearRegression model (default hyperparams) on the training data from 'all_x_3most'. What do you observe about the performance of this model? What is your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call the model 'threemost_model' and evaluate it using r2_mse_mae_eval()\n",
    "###* put your code here *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3g] (3 points) How good is that model? What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (30 points) Trees, More Trees, lots of Trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to reset the data to the original form (before polynomial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some cleanup\n",
    "del train_x, train_y, test_x, test_y, val_x, val_y\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape\n",
    "assert train_x.shape == (12166, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "dtmodel = DecisionTreeRegressor(random_state=seed).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the code in the cell below if you have some time to wait around and want to visualize the tree, otherwise skip it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will take a long time (10-20 minutes); skip if you are in a hurry\n",
    "# let's plot what the tree looks like\n",
    "#plt.figure(figsize=(16,12))\n",
    "#\n",
    "#plot_tree(dtmodel, feature_names=features, filled=True, label='all', rounded=True)\n",
    "#\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (10 points) Answer some questions about the structure of our tree (dtmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Can the tree be visualized easily?\n",
    "#### 2. How deep is the tree?\n",
    "#### 3. How many nodes it contain?\n",
    "#### 4. How many total splits are there?\n",
    "#### 5. What is the impurity of the last 2 nodes?\n",
    "#### Hint: lookup the scikit-learn documentation to know how to manipulate the 'tree_' attribute of Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_mse_mae_eval(dtmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4b] (5 points) Is it a good model? Is it overfitted? Is it better than the models trained in Tasks 2 and 3? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4c] (5 points) Train another decision tree but this time regularize it. Can you obtain a model with similar performance to 'dtmodel' but not (or at least less) overfitted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call your new model 'dtregmodel'\n",
    "###* put your code here *###\n",
    "\n",
    "\n",
    "\n",
    "r2_mse_mae_eval(dtregmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4d] (5 points) Now let's train a random forest and see if we can train an even better model. Use search_train_eval() to do a grid search over hyperparameters. You are free to pick whatever hyperparameters & values you want, but you should try to avoid badly overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### Call your random forest model 'rfmodel'\n",
    "### Make sure to set random_state=seed for reproducibility!\n",
    "###* put your code here *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4e] (5 points) Is your RF model better than the decision tree you trained for Task 4c? Justify your answer. What can you conclude about ensembles/random forests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): Stacking Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this task we'll use stacking to create a meta model or blender model to predict the target using predictions from 6 other models from Tasks 1 - 4 as features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (10 points) Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "# these are the models we'll use from previous tasks\n",
    "# (this is why it's important that you named the models as instructed in Tasks 2-4)\n",
    "regressors = [('lr', lrmodel), ('elasticnet', enmodel), ('ridge', rmodel), \n",
    "             ('dt', dtmodel), ('dtreg', dtregmodel), ('rf', rfmodel)]\n",
    "\n",
    "# this will return predictions for all of our regressors on matrix x\n",
    "def regressors_preds(x):\n",
    "    num_regs = len(regressors)\n",
    "    \n",
    "    ### Create an array to contain the predictions from the regressors over all examples in 'x'\n",
    "    ### Each regressor will correspond to one feature (i.e., one column)\n",
    "    ### The numpy array you return should have shape (x.shape[0], num_regs)\n",
    "    ###* put your code here (~4-6 lines) *###\n",
    "\n",
    "    \n",
    "\n",
    "def stacking_train_eval(model_name, model, standardize=False):\n",
    "    ### Create a new training dataset 'meta_train_x' and 'meta_train_y'\n",
    "    ### For this use the validation data (val_x, val_y) alongside with regressors_preds()\n",
    "    ###* put your code here (~2 lines) *###\n",
    "\n",
    "    \n",
    "    assert meta_train_x.shape == (2606, 6) and meta_train_x.shape[0] == meta_train_y.shape[0]\n",
    "\n",
    "\n",
    "    ### Create our new test dataset 'meta_test_x' and 'meta_test_y'\n",
    "    ### For this we use the test data (test_x, test_y) alongside with regressors_preds()\n",
    "    ###* put your code here (~2 lines) *###\n",
    "\n",
    "    \n",
    "    \n",
    "    assert meta_test_x.shape == (2607, 6) and meta_test_x.shape[0] == meta_test_y.shape[0]\n",
    "    \n",
    "    # zscore normalize the features if standardize = True\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        meta_train_x = scaler.fit_transform(meta_train_x)\n",
    "        meta_test_x = scaler.transform(meta_test_x)\n",
    "\n",
    "    \n",
    "    # train the meta model\n",
    "    model.fit(meta_train_x, meta_train_y)\n",
    "\n",
    "    # make predictions & eval\n",
    "    train_pred = model.predict(meta_train_x)\n",
    "    test_pred = model.predict(meta_test_x)\n",
    "\n",
    "    train_error = mean_squared_error(train_pred, meta_train_y)\n",
    "    val_error = mean_squared_error(test_pred, test_y)\n",
    "\n",
    "    print('Stacking (Meta model: {})'.format(model_name))\n",
    "    print('\\tTrain MSE: {:.3f}, Test MSE: {:.3f}'.format(train_error, val_error))\n",
    "\n",
    "    train_error = mean_absolute_error(train_pred, meta_train_y)\n",
    "    val_error = mean_absolute_error(test_pred, test_y)\n",
    "\n",
    "    print('\\tTrain MAE: {:.3f}, Test MAE: {:.3f}'.format(train_error, val_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (3 points) Train a SVM regression model with a linear kernel and C=100. Use stacking_train_eval(). You can set standardize=True to zscore normalize features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a SVM regressor with a linear kernel and C=100\n",
    "### Note: the training will take a few minutes\n",
    "###* put your code here (~2-3 lines) *###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5c] (2 points) How good is this model? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5d] (5 points) Now train a SVM regression model with any other kernel (i.e., not linear) and combination of hyperparameters of your choice. Can you train a better stacking model than for Task 5b?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train a SVM regressor with any non-linear kernel and hyperparameters you want \n",
    "### You can do a hyperparameter search if you want\n",
    "###* put your code here (~2-3 lines) *###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5e] (5 points) What do you conclude? Provide a plausible explanation why non-linear kernels do not seem to improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# \n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
