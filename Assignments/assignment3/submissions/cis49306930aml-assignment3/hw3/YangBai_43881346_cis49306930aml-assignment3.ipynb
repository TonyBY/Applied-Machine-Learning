{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Regression & Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your name and UFL email address\n",
    "name = 'Yang Bai'\n",
    "email = 'baiyang94@ufl.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assignment 3 -- name: Yang Bai, email: baiyang94@ufl.edu\n",
      "\n",
      "### Python version: 3.8.3 (default, Jul  2 2020, 11:26:31) \n",
      "[Clang 10.0.0 ]\n",
      "### NumPy version: 1.19.5\n",
      "### Scikit-learn version: 0.23.1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "if name == 'enter your name' or email == 'enter your email':\n",
    "    assert False, 'Enter your name & email first!'\n",
    "else:\n",
    "    print('Assignment 3 -- name: {}, email: {}\\n'.format(name, email))\n",
    "    \n",
    "    # Load packages we need\n",
    "    import sys\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "    # Let's check our software versions\n",
    "    print('### Python version: ' + __import__('sys').version)\n",
    "    print('### NumPy version: ' + np.__version__)\n",
    "    print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "    print('------------')\n",
    "\n",
    "\n",
    "    # load our packages / code\n",
    "    sys.path.insert(1, '../common/')\n",
    "    import utils\n",
    "    import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "# deterministic seed for reproducibility\n",
    "##rng = np.random.default_rng(seed)  # best practice but not fully implemented in scikit-learn\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [14, 3, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment we'll load the Bike Sharing dataset (hourly)\n",
    "### This dataset contains features of users bike sharing/rental on an hourly basis.\n",
    "### The task is to predict how many users are sharing/renting a bike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   season      16320 non-null  float64\n",
      " 1   year        16231 non-null  float64\n",
      " 2   month       16304 non-null  float64\n",
      " 3   hour        16254 non-null  float64\n",
      " 4   holiday     16277 non-null  float64\n",
      " 5   weekday     16282 non-null  float64\n",
      " 6   workingday  16297 non-null  float64\n",
      " 7   weathersit  16324 non-null  float64\n",
      " 8   temp        16242 non-null  float64\n",
      " 9   atemp       16271 non-null  float64\n",
      " 10  hum         16252 non-null  float64\n",
      " 11  windspeed   16281 non-null  float64\n",
      " 12  registered  16244 non-null  float64\n",
      " 13  nsqrtc      16263 non-null  float64\n",
      " 14  count       17379 non-null  int64  \n",
      "dtypes: float64(14), int64(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "### Note: this dataset has missing values (artificially introduced), which you'll need to fill in before you can train a model\n",
    "df = pd.read_csv('../data/bikesharehour.csv.gz', compression='gzip', header=0, na_values='?')\n",
    "\n",
    "# Check that we loaded the data as expected\n",
    "df_expected_shape = (17379, 15)\n",
    "\n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>registered</th>\n",
       "      <th>nsqrtc</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  year  month  hour  holiday  weekday  workingday  weathersit  temp  \\\n",
       "0     1.0   0.0    NaN   0.0      0.0      6.0         0.0         1.0   NaN   \n",
       "1     1.0   0.0    NaN   1.0      0.0      6.0         0.0         1.0   NaN   \n",
       "2     1.0   0.0    1.0   2.0      0.0      6.0         0.0         1.0   0.0   \n",
       "3     1.0   0.0    1.0   3.0      0.0      6.0         0.0         1.0   0.0   \n",
       "4     1.0   0.0    1.0   4.0      0.0      6.0         0.0         1.0   0.0   \n",
       "\n",
       "   atemp  hum  windspeed  registered  nsqrtc  count  \n",
       "0    0.0  0.0        0.0        13.0    -5.0     16  \n",
       "1    0.0  0.0        0.0        32.0    -8.0     40  \n",
       "2    0.0  0.0        0.0        27.0    -7.0     32  \n",
       "3    0.0  0.0        0.0        10.0    -5.0     13  \n",
       "4    0.0  0.0        0.0         1.0     0.0      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what does the data look like?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are some NaNs which we'll have to impute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the data as a numpy matrix\n",
    "all_xy = df.to_numpy()\n",
    "\n",
    "col_names = [c for c in df.columns]\n",
    "features = col_names[:-1]\n",
    "target = col_names[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: ['season', 'year', 'month', 'hour', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'registered', 'nsqrtc'] --- target: count\n"
     ]
    }
   ],
   "source": [
    "print('features: {} --- target: {}'.format(features, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1059, 1148, 1075, 1125, 1102, 1097, 1082, 1055, 1137, 1108, 1127,\n",
       "       1098, 1135, 1116,    0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many NaNs in each column?\n",
    "np.sum(np.isnan(all_xy), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe: no NaNs in the target/value column\n",
    "### About 1000+ NaNs in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y\n",
    "all_x_nan = all_xy[:,:-1]\n",
    "all_y = all_xy[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   0.  nan ...   0.  13.  -5.]\n",
      " [  1.   0.  nan ...   0.  32.  -8.]\n",
      " [  1.   0.   1. ...   0.  27.  -7.]\n",
      " ...\n",
      " [  1.   1.  12. ...   3.  83.  -8.]\n",
      " [ nan   1.  12. ...   2.  48. -11.]\n",
      " [  1.   1.  12. ...   2.  37.  nan]]\n"
     ]
    }
   ],
   "source": [
    "print(all_x_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 1] (10 points) Let's impute the missing values! Use Scikit-learn's SimpleImputer to replace all NaNs in 'all_x_nan' with the *most frequent* value in each column. Use copy=True and store the results in 'all_x' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.   0.   7. ...   0.  13.  -5.]\n",
      " [  1.   0.   7. ...   0.  32.  -8.]\n",
      " [  1.   0.   1. ...   0.  27.  -7.]\n",
      " ...\n",
      " [  1.   1.  12. ...   3.  83.  -8.]\n",
      " [  3.   1.  12. ...   2.  48. -11.]\n",
      " [  1.   1.  12. ...   2.  37.   0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "###* put your code here (~2-3 lines) *###\n",
    "imp_mf = SimpleImputer(missing_values=np.nan, strategy='most_frequent', copy=True)\n",
    "imp_mf.fit(all_x_nan)\n",
    "all_x = imp_mf.transform(all_x_nan)\n",
    "print(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the shape is correct\n",
    "assert all_x.shape == (17379, 14)\n",
    "\n",
    "# check that there are no more NaNs\n",
    "assert np.sum(np.sum(np.isnan(all_x), axis=0)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescale the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll min-max normalize the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(copy=True)\n",
    "scaler.fit(all_x) \n",
    "\n",
    "scaled_all_x = scaler.transform(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12166, 14), (12166,), (2607, 14), (2607,), (2606, 14), (2606,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 2] (30 points) Let's train linear models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2a] (2 points) Train a Linear Regression model using the default hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "### Train a linear regression model on the training data (train_x, train_y)\n",
    "### Call the resulting trained model 'lrmodel'\n",
    "###* put your code here (~1 line) *###\n",
    "\n",
    "lrmodel = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.847, Val  R^2: 0.833\n",
      "Train MSE: 5033.098, Val MSE: 5532.778\n",
      "Train MAE: 39.039, Val MAE: 39.742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def r2_mse_mae_eval(model, pref=''):\n",
    "    # R^2 the coefficient of determination\n",
    "    r2_train = model.score(train_x, train_y)\n",
    "    r2_val = model.score(val_x, val_y)\n",
    "\n",
    "    print('{}Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(pref, r2_train, r2_val))\n",
    "\n",
    "    train_pred = model.predict(train_x)\n",
    "    val_pred = model.predict(val_x)\n",
    "\n",
    "    # measure the error (MSE) wrt true target\n",
    "    train_error = mean_squared_error(train_pred, train_y)\n",
    "    val_error = mean_squared_error(val_pred, val_y)\n",
    "\n",
    "    print('{}Train MSE: {:.3f}, Val MSE: {:.3f}'.format(pref, train_error, val_error))\n",
    "    \n",
    "    train_error = mean_absolute_error(train_pred, train_y)\n",
    "    val_error = mean_absolute_error(val_pred, val_y)\n",
    "\n",
    "    print('{}Train MAE: {:.3f}, Val MAE: {:.3f}'.format(pref, train_error, val_error))\n",
    "    \n",
    "r2_mse_mae_eval(lrmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2b] (3 points) How good is that model? (A few sentences is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline prediction mean: 189.30100279467368\n",
      "Baseline error (MSE): 32846.529\n",
      "Baseline error (MAE): 142.710\n"
     ]
    }
   ],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "\n",
    "meanv = np.mean(train_y)\n",
    "train_pred = lrmodel.predict(train_x)\n",
    "baseline_pred = np.ones_like(train_pred) * meanv\n",
    "print('Baseline prediction mean: {}'.format(meanv))\n",
    "\n",
    "baseline_error = mean_squared_error(baseline_pred, train_y)\n",
    "print('Baseline error (MSE): {:.3f}'.format(baseline_error))\n",
    "\n",
    "baseline_error = mean_absolute_error(baseline_pred, train_y)\n",
    "print('Baseline error (MAE): {:.3f}'.format(baseline_error))\n",
    "\n",
    "# Answer:\n",
    "# By comparing the results over the training dataset and the validation dataset, we can see this model \n",
    "# generalizes well.\n",
    "# But to evaluate whether the model works well or not, we need a baseline to compare with.\n",
    "# Say, we compare it with a simple baseline model which always predict the mean.\n",
    "# By comparing the MAE between these two models, we can say the linear regression model works pretty well.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's setup some functions so we can tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some code to do a grid search and automatically train & evaluate the model with the best hyperparams.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def do_grid_search(model, param_grid, x, y):\n",
    "    gs = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error')\n",
    "    gs_res = gs.fit(x, y)\n",
    "    return  gs_res.best_params_\n",
    "\n",
    "\n",
    "def search_train_eval(model, param_grid, tr_x=train_x, tr_y=train_y, v_x=val_x, v_y=val_y):\n",
    "    \n",
    "    # since we do CV for the grid search, let's concatenate the train and val sets for it\n",
    "    search_x = np.r_[tr_x, v_x]\n",
    "    search_y = np.r_[tr_y, v_y]\n",
    "    \n",
    "    hyperparams = do_grid_search(model, param_grid, search_x, search_y)\n",
    "    \n",
    "    class_obj = type(model)\n",
    "    m = class_obj(**hyperparams).fit(tr_x, tr_y)\n",
    "    \n",
    "    cn = str(class_obj).split(\"'\")[1]\n",
    "    cn = cn.split('.')[-1]\n",
    "    print('{}({})'.format(cn, hyperparams))\n",
    "\n",
    "    r2_mse_mae_eval(m, pref='\\t')\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2c] (5 points) Do a grid search to tune hyperparameters and train an ElasticNet model. You can choose the values of hyperparameters your search over, but you must search over 'alpha' and 'l1_ratio'. Ensure that during the search, the training of models converges in all cases (you may need to increase 'max_iter' based on your chosen values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet({'alpha': 0.001, 'l1_ratio': 0.99})\n",
      "\tTrain R^2: 0.847, Val  R^2: 0.833\n",
      "\tTrain MSE: 5033.116, Val MSE: 5532.434\n",
      "\tTrain MAE: 39.048, Val MAE: 39.749\n"
     ]
    }
   ],
   "source": [
    "### Hint: you should define a parameter grid dictionary and call search_train_eval() to do the actual search\n",
    "### Note: you only need to pass a model instance (model) and a parameter grid (param_grid)\n",
    "### Call the output of search_train_eval(): 'enmodel'\n",
    "###* put your code here (~3 lines) *###\n",
    "\n",
    "param_grid = {'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0], 'l1_ratio': np.arange(0.01, 1, 0.01)}\n",
    "model = sklearn.linear_model.ElasticNet()\n",
    "enmodel = search_train_eval(model, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2d] (2 points) Do a grid search to tune hyperparameters and train a Ridge Regression model. You can choose the values of hyperparameters your search over, but you must search over 'alpha'. Ensure that during the search, the training of models converges in all cases (you may need to increase 'max_iter' based on your chosen values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge({'alpha': 0.1})\n",
      "\tTrain R^2: 0.847, Val  R^2: 0.833\n",
      "\tTrain MSE: 5033.101, Val MSE: 5532.577\n",
      "\tTrain MAE: 39.046, Val MAE: 39.749\n"
     ]
    }
   ],
   "source": [
    "### Call the output of search_train_eval(): 'rmodel'\n",
    "###* put your code here (~3 lines) *###\n",
    "\n",
    "param_grid = {'alpha':[1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0]}\n",
    "model = sklearn.linear_model.Ridge()\n",
    "rmodel = search_train_eval(model, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2e] (3 points) Print the parameter values (w and b) of the ElasticNet and Ridge Regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet: \n",
      "---w: [ 26.397  14.278 -10.928  52.874 -8.286  6.622 -30.461 -30.814  0.000\n",
      " -5.692 -13.158  28.819  926.691 -14.575] \n",
      "--- b: 20.647297762732535\n",
      "\n",
      "Ridge Regression: \n",
      "---w: [ 26.431  14.271 -10.972  52.865 -8.324  6.631 -30.472 -30.828  1.476\n",
      " -16.757 -13.215  28.873  926.780 -14.613] \n",
      "--- b: 20.661251354260088\n"
     ]
    }
   ],
   "source": [
    "# Print the weights and bias for both models\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "### Make sure you print the weights and bias for both models and that it is clear which is which.\n",
    "print('ElasticNet: \\n---w: {} \\n--- b: {}'.format(enmodel.coef_, enmodel.intercept_))\n",
    "print('\\nRidge Regression: \\n---w: {} \\n--- b: {}'.format(rmodel.coef_, rmodel.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2f] (2 points) How similar are the parameter values of the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE between the two set of parameters: 0.8637771652792111\n"
     ]
    }
   ],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "parameter_error = mean_absolute_error(np.append(enmodel.coef_, enmodel.intercept_), \n",
    "                                     np.append(rmodel.coef_, rmodel.intercept_))\n",
    "print(\"MAE between the two set of parameters: {}\".format(parameter_error))\n",
    "# \n",
    "# Answer:\n",
    "# To me, these two set of parameters are very similar in the sense of MAE.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2g] (8 points) For each of the two models, display the three most important features alongside with their coefficients. Are these the same across both models?\n",
    "### What are the coefficients? Which feature is the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three Most important features of the ElasticNet model: ['registered', 'hour', 'weathersit'] \n",
      "(weight: [ 926.691  52.874 -30.814])\n",
      "The three Most important features of the Ridge Regression model: ['registered', 'hour', 'weathersit'] \n",
      "(weight: [ 926.780  52.865 -30.828])\n"
     ]
    }
   ],
   "source": [
    "### Hint: don't forget that coefficients can be positive as well as negative.\n",
    "###* put your code here *###\n",
    "mwfidx_Elastic = (-np.abs(enmodel.coef_)).argsort()[:3]\n",
    "most_important_features_Elastic = [col_names[x] for x in mwfidx_Elastic]\n",
    "print('The three Most important features of the ElasticNet model: {} \\n(weight: {})'.format(most_important_features_Elastic, \n",
    "                                                                                              enmodel.coef_[mwfidx_Elastic]))\n",
    "\n",
    "mwfidx_Ridge = (-np.abs(rmodel.coef_)).argsort()[:3]\n",
    "most_important_features_Ridge = [col_names[x] for x in mwfidx_Ridge]\n",
    "print('The three Most important features of the Ridge Regression model: {} \\n(weight: {})'.format(most_important_features_Ridge, \n",
    "                                                                                              rmodel.coef_[mwfidx_Ridge]))\n",
    "\n",
    "\n",
    "###* put your answer as comment here *###\n",
    "# \n",
    "# Answer: \n",
    "# The three most important features are the same across both models.\n",
    "# The coefficients are printed below.\n",
    "# The feature 'registered' is the most important one.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2h] (5 points) Take a look at the code of search_train_eval() and do_grid_search(). Answer the following questions: \n",
    "### 1. Why is the scoring function for the grid search 'neg_mean_squared_error' (as opposed to 'mean_squared_error')? \n",
    "### 2. Why is it okay to do the search over search_x and search_y which are the concatenation of the training and validation sets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hint: take a look at the documentation of scikit-learn for GridSearchCV and related classes.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# 1. Because, all the scorer objects in the sklearn follow the convention that higher return values \n",
    "#    are better than lower return values. Hence, it has to be 'neg_mean_squared_error' \n",
    "#    instead of 'mean_squared_error'.\n",
    "#\n",
    "# 2. Because GridSearchCV does a cross-validated grid-search over a parameter grid, \n",
    "#    hence, the validation set is no longer needed.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 3] (30 points) Let's train polynomial regression models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3a] (5 points) Use PolynomialFeatures to create a version of the data with all features of degree 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "### Use PolynomialFeatures to create a version of the data with all features of degree 2. Make sure to allow interactions (interaction_only=False) and set include_bias=False.\n",
    "### Store the result in 'all_x_polyf'. Ensure that you make a copy of the original data and you use the scaled features ('scaled_all_x')!\n",
    "###* put your code here (~2 lines) *###\n",
    "polyf = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "all_x_polyf = polyf.fit_transform(scaled_all_x.copy())\n",
    "\n",
    "assert all_x_polyf.shape == (17379, 119)\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_polyf, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a LinearRegression model and a Ridge model on our polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.927, Val  R^2: 0.923\n",
      "Train MSE: 2410.398, Val MSE: 2553.171\n",
      "Train MAE: 24.883, Val MAE: 24.807\n",
      "\n",
      "Train R^2: 0.926, Val  R^2: 0.921\n",
      "Train MSE: 2444.731, Val MSE: 2634.107\n",
      "Train MAE: 24.644, Val MAE: 24.661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Train a linear regression model\n",
    "pf_lrmodel = LinearRegression().fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_lrmodel)\n",
    "\n",
    "print()\n",
    "\n",
    "# Train a Ridge regression model\n",
    "pf_ridgemodel = Ridge(alpha=0.5).fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_ridgemodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3b] (5 points) What is the difference between LinearRegression and Ridge? (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Ridge is a regularized version of LinearRegression.\n",
    "# Ridge penalizes the model for the sum of squared value of the weights.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3c] (5 points) Look at (e.g., print) the parameters of both the LinearRegression model ('pf_lrmodel') and the Ridge model ('pf_ridgemodel'). What do you notice? Explain what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression: \n",
      "---w: [ 5.326  30521076005237.777  45.602  130.619  26943418922304.711  30.151\n",
      " -9980426199437.012  32.620  27106435058197.676 -59664970501572.828\n",
      "  13937610703289.674 -15.169  714.621 -1777.331 -15.881  14.484  24.562\n",
      "  36.437  6.538 -3.561  1.582 -7.538  3430595177194.502\n",
      "  12908014073732.291 -7.685  12.806 -81.200 -18.620 -30521076005228.848\n",
      " -13.176  41.264 -4.891  1.611  5.390 -10.917 -7571619403674.456\n",
      "  16594786022.967 -4.121  16.353 -113.504 -14.633 -41.108 -0.335 -6.680\n",
      "  3.458 -2.974  2.181 -3960308083550.017  7834099707978.211  2.426 -11.298\n",
      "  4.105 -0.968 -114.819 -1.390  8.415  27.144 -47.840 -5199916544932.583\n",
      "  7453194668229.455 -15.876  45.100 -213.870 -14.104 -26943418922321.574\n",
      " -9.536 -13.706  5.559 -1616411.592 -1004507.483 -401834.677  42.514\n",
      " -0.721  18.139 -26.896  3.822  2.090 -7480638434452.571\n",
      "  10444543480497.895 -9.198 -28.013 -25.604  6.392  9980426199429.299\n",
      " -10.121  0.000  13589086047201.746 -3.090  10.087 -45.943 -1.166 -28.786\n",
      "  0.000  0.000  5.336 -43.031  106.078  3.193 -7480638423548.181  0.000\n",
      "  0.000 -1237000731149.585 -1760675879357.564 -1999699417766.011\n",
      "  13522715663485.896  0.000  3848788271871.425  1113193455103.246\n",
      "  8328681603918.935 -13937610703282.418  7.537  64.375 -8.404 -23.500\n",
      " -90.232  60.818  506.044  72.217  1765.671] \n",
      "--- b: 423.5835349503576\n",
      "\n",
      "Ridge Regression: \n",
      "---w: [ 16.347  7.337  66.439  176.621 -6.577  24.568 -2.569  31.044 -5.120\n",
      " -6.905  2.513  2.124  717.231 -1469.418 -22.315  13.513  23.884  35.374\n",
      "  5.887 -3.275  0.453 -8.459 -3.414 -4.604 -7.073  14.000 -67.229 -26.480\n",
      "  7.337 -13.644  38.798 -5.140  2.087  6.122 -10.807 -5.120  0.000 -2.165\n",
      "  16.034 -102.178 -25.542 -54.080 -1.729 -4.261  3.096 -0.667  2.656\n",
      " -2.793 -3.767  2.529 -8.909  1.434 -17.343 -136.208  0.321  8.279  26.105\n",
      " -48.626 -3.562 -4.203 -14.097  40.617 -169.384 -56.432 -6.577 -9.999\n",
      " -4.715  7.348  0.000  0.000  0.000  25.655 -12.335  17.473 -17.424  3.010\n",
      "  2.255 -5.120 -5.755 -8.206 -28.720 -11.958 -3.935 -2.569 -7.197  0.000\n",
      " -6.905 -1.123  6.509 -87.675 -2.441 -30.328  0.000  0.000  6.964 -44.029\n",
      "  97.383  2.161 -5.120  0.000  0.000 -0.904 -1.110 -1.237 -6.905  0.000\n",
      " -2.031 -0.600 -4.316  2.513  5.425  40.523 -9.988 -21.331 -73.229  29.575\n",
      "  485.084  92.357  1513.202] \n",
      "--- b: 328.73221150905476\n"
     ]
    }
   ],
   "source": [
    "###* put your code here *###\n",
    "print('LinearRegression: \\n---w: {} \\n--- b: {}'.format(pf_lrmodel.coef_, pf_lrmodel.intercept_))\n",
    "print('\\nRidge Regression: \\n---w: {} \\n--- b: {}'.format(pf_ridgemodel.coef_, pf_ridgemodel.intercept_))\n",
    "\n",
    "###  What do you notice? Explain what is going on.\n",
    "###* put your answer as comment here *###\n",
    "#\n",
    "# I notice that the parameters of the Ridge model is much smaller than the ones in the LinearRegression model.\n",
    "# Ridge penalizes the model for the sum of squared value of the weights.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3d] (5 points) Focus on the Ridge model. What are the three most important features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices of the three Most important features of the Ridge Regression model: [118  13  12] \n",
      "(weight: [ 1513.202 -1469.418  717.231])\n"
     ]
    }
   ],
   "source": [
    "### Print the three most important features alongside with their weights.\n",
    "### Remember: weights can be positive as well as negative.\n",
    "### Hint: you can use the get_feature_names() method of PolynomialFeatures to relate polynomial features to the original features.\n",
    "###* put your code here *###\n",
    "mwfidx_Ridge = (-np.abs(pf_ridgemodel.coef_)).argsort()[:3]\n",
    "# most_important_features_Ridge = [col_names[x] for x in mwfidx_Ridge]\n",
    "print('The indices of the three Most important features of the Ridge Regression model: {} \\n(weight: {})'.format(mwfidx_Ridge, \n",
    "                                                                                              pf_ridgemodel.coef_[mwfidx_Ridge]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3e] (5 points) Let's use only these three most important features. Extract the three features from the polynomial features to create a new feature matrix with three columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the three features from 'all_x_polyf' and store the results in 'all_x_3most'\n",
    "###* put your code here *###\n",
    "all_x_3most = all_x_polyf[:, mwfidx_Ridge]\n",
    "\n",
    "assert all_x_3most.shape == (17379, 3)\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_3most, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3f] (2 points) Now train a LinearRegression model (default hyperparams) on the training data from 'all_x_3most'. What do you observe about the performance of this model? What is your conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.912, Val  R^2: 0.904\n",
      "Train MSE: 2903.069, Val MSE: 3181.395\n",
      "Train MAE: 23.434, Val MAE: 23.624\n"
     ]
    }
   ],
   "source": [
    "### Call the model 'threemost_model' and evaluate it using r2_mse_mae_eval()\n",
    "###* put your code here *###\n",
    "\n",
    "pf_lrmodel_3most = LinearRegression().fit(train_x, train_y)\n",
    "r2_mse_mae_eval(pf_lrmodel_3most)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 3g] (3 points) How good is that model? What do you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Answer:\n",
    "# The model trained with the three most important features performs not as good as the models trained over all\n",
    "# polynomial features. But the performancs are very close.\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Task 4] (30 points) Trees, More Trees, lots of Trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to reset the data to the original form (before polynomial features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do some cleanup\n",
    "del train_x, train_y, test_x, test_y, val_x, val_y\n",
    "\n",
    "# split the data into train, test, val\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)\n",
    "\n",
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape\n",
    "assert train_x.shape == (12166, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "dtmodel = DecisionTreeRegressor(random_state=seed).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment the code in the cell below if you have some time to wait around and want to visualize the tree, otherwise skip it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This will take a long time (10-20 minutes); skip if you are in a hurry\n",
    "# let's plot what the tree looks like\n",
    "# plt.figure(figsize=(16,12))\n",
    "\n",
    "# plot_tree(dtmodel, feature_names=features, filled=True, label='all', rounded=True)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4a] (10 points) Answer some questions about the structure of our tree (dtmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Can the tree be visualized easily?\n",
    "#### 2. How deep is the tree?\n",
    "#### 3. How many nodes it contain?\n",
    "#### 4. How many total splits are there?\n",
    "#### 5. What is the impurity of the last 2 nodes?\n",
    "#### Hint: lookup the scikit-learn documentation to know how to manipulate the 'tree_' attribute of Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth of the tree:  35\n",
      "Number of nodes in the tree:  18157\n",
      "Number of leaves:  9079\n",
      "Number of total splits = Number of nodes - Number of leaves =  9078\n",
      "The impurity of the last 2 nodes:  [ 0.000  0.000]\n"
     ]
    }
   ],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# 1. No. Because, first, it takes long time to run; second, when the size(depth) of the graph is large, it is hard to visualize the details of each node.\n",
    "print(\"Depth of the tree: \", dtmodel.get_depth())\n",
    "# 2. The depth of the tree is 35.\n",
    "print(\"Number of nodes in the tree: \", dtmodel.tree_.node_count)\n",
    "# 3. There are 18157 nodes in the tree.\n",
    "print(\"Number of leaves: \", dtmodel.tree_.n_leaves)\n",
    "print(\"Number of total splits = Number of nodes - Number of leaves = \", dtmodel.tree_.node_count - dtmodel.tree_.n_leaves)\n",
    "# 4. Number of total splits is 9078.\n",
    "print(\"The impurity of the last 2 nodes: \", dtmodel.tree_.impurity[-2:])\n",
    "# 5. The impurity of the last two nodes are both 0.000\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Tree in module sklearn.tree._tree:\n",
      "\n",
      "class Tree(builtins.object)\n",
      " |  Array-based representation of a binary decision tree.\n",
      " |  \n",
      " |  The binary tree is represented as a number of parallel arrays. The i-th\n",
      " |  element of each array holds information about the node `i`. Node 0 is the\n",
      " |  tree's root. You can find a detailed description of all arrays in\n",
      " |  `_tree.pxd`. NOTE: Some of the arrays only apply to either leaves or split\n",
      " |  nodes, resp. In this case the values of nodes of the other type are\n",
      " |  arbitrary!\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  node_count : int\n",
      " |      The number of nodes (internal nodes + leaves) in the tree.\n",
      " |  \n",
      " |  capacity : int\n",
      " |      The current capacity (i.e., size) of the arrays, which is at least as\n",
      " |      great as `node_count`.\n",
      " |  \n",
      " |  max_depth : int\n",
      " |      The depth of the tree, i.e. the maximum depth of its leaves.\n",
      " |  \n",
      " |  children_left : array of int, shape [node_count]\n",
      " |      children_left[i] holds the node id of the left child of node i.\n",
      " |      For leaves, children_left[i] == TREE_LEAF. Otherwise,\n",
      " |      children_left[i] > i. This child handles the case where\n",
      " |      X[:, feature[i]] <= threshold[i].\n",
      " |  \n",
      " |  children_right : array of int, shape [node_count]\n",
      " |      children_right[i] holds the node id of the right child of node i.\n",
      " |      For leaves, children_right[i] == TREE_LEAF. Otherwise,\n",
      " |      children_right[i] > i. This child handles the case where\n",
      " |      X[:, feature[i]] > threshold[i].\n",
      " |  \n",
      " |  feature : array of int, shape [node_count]\n",
      " |      feature[i] holds the feature to split on, for the internal node i.\n",
      " |  \n",
      " |  threshold : array of double, shape [node_count]\n",
      " |      threshold[i] holds the threshold for the internal node i.\n",
      " |  \n",
      " |  value : array of double, shape [node_count, n_outputs, max_n_classes]\n",
      " |      Contains the constant prediction value of each node.\n",
      " |  \n",
      " |  impurity : array of double, shape [node_count]\n",
      " |      impurity[i] holds the impurity (i.e., the value of the splitting\n",
      " |      criterion) at node i.\n",
      " |  \n",
      " |  n_node_samples : array of int, shape [node_count]\n",
      " |      n_node_samples[i] holds the number of training samples reaching node i.\n",
      " |  \n",
      " |  weighted_n_node_samples : array of int, shape [node_count]\n",
      " |      weighted_n_node_samples[i] holds the weighted number of training samples\n",
      " |      reaching node i.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getstate__(...)\n",
      " |      Getstate re-implementation, for pickling.\n",
      " |  \n",
      " |  __reduce__(...)\n",
      " |      Reduce re-implementation, for pickling.\n",
      " |  \n",
      " |  __setstate__(...)\n",
      " |      Setstate re-implementation, for unpickling.\n",
      " |  \n",
      " |  apply(...)\n",
      " |      Finds the terminal region (=leaf node) for each sample in X.\n",
      " |  \n",
      " |  compute_feature_importances(...)\n",
      " |      Computes the importance of each feature (aka variable).\n",
      " |  \n",
      " |  compute_partial_dependence(...)\n",
      " |      Partial dependence of the response on the ``target_feature`` set.\n",
      " |      \n",
      " |      For each sample in ``X`` a tree traversal is performed.\n",
      " |      Each traversal starts from the root with weight 1.0.\n",
      " |      \n",
      " |      At each non-leaf node that splits on a target feature, either\n",
      " |      the left child or the right child is visited based on the feature\n",
      " |      value of the current sample, and the weight is not modified.\n",
      " |      At each non-leaf node that splits on a complementary feature,\n",
      " |      both children are visited and the weight is multiplied by the fraction\n",
      " |      of training samples which went to each child.\n",
      " |      \n",
      " |      At each leaf, the value of the node is multiplied by the current\n",
      " |      weight (weights sum to 1 for all visited terminal nodes).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : view on 2d ndarray, shape (n_samples, n_target_features)\n",
      " |          The grid points on which the partial dependence should be\n",
      " |          evaluated.\n",
      " |      target_features : view on 1d ndarray, shape (n_target_features)\n",
      " |          The set of target features for which the partial dependence\n",
      " |          should be evaluated.\n",
      " |      out : view on 1d ndarray, shape (n_samples)\n",
      " |          The value of the partial dependence function on each grid\n",
      " |          point.\n",
      " |  \n",
      " |  decision_path(...)\n",
      " |      Finds the decision path (=node) for each sample in X.\n",
      " |  \n",
      " |  predict(...)\n",
      " |      Predict target for X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  capacity\n",
      " |  \n",
      " |  children_left\n",
      " |  \n",
      " |  children_right\n",
      " |  \n",
      " |  feature\n",
      " |  \n",
      " |  impurity\n",
      " |  \n",
      " |  max_depth\n",
      " |  \n",
      " |  max_n_classes\n",
      " |  \n",
      " |  n_classes\n",
      " |  \n",
      " |  n_features\n",
      " |  \n",
      " |  n_leaves\n",
      " |  \n",
      " |  n_node_samples\n",
      " |  \n",
      " |  n_outputs\n",
      " |  \n",
      " |  node_count\n",
      " |  \n",
      " |  threshold\n",
      " |  \n",
      " |  value\n",
      " |  \n",
      " |  weighted_n_node_samples\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sklearn.tree._tree.Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 1.000, Val  R^2: 0.968\n",
      "Train MSE: 0.000, Val MSE: 1076.791\n",
      "Train MAE: 0.000, Val MAE: 10.691\n"
     ]
    }
   ],
   "source": [
    "r2_mse_mae_eval(dtmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4b] (5 points) Is it a good model? Is it overfitted? Is it better than the models trained in Tasks 2 and 3? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Answer:\n",
    "# 1. Yes, this is a good model with low a bias and a low variance.\n",
    "# 2. Yes, this model is a little bit overfitted, since the performance over the validation set is \n",
    "#    little bit worse than the training set.\n",
    "# 3. Yes, this model is better than the models trained in Tasks 2 and 3, because its performance over the \n",
    "#    validation dataset is better than the others.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4c] (5 points) Train another decision tree but this time regularize it. Can you obtain a model with similar performance to 'dtmodel' but not (or at least less) overfitted? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R^2: 0.916, Val  R^2: 0.917\n",
      "Train MSE: 2760.888, Val MSE: 2761.336\n",
      "Train MAE: 32.212, Val MAE: 31.264\n"
     ]
    }
   ],
   "source": [
    "### Call your new model 'dtregmodel'\n",
    "###* put your code here *###\n",
    "\n",
    "dtregmodel = DecisionTreeRegressor(ccp_alpha=110,random_state=seed).fit(train_x, train_y)\n",
    "\n",
    "r2_mse_mae_eval(dtregmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4d] (5 points) Now let's train a random forest and see if we can train an even better model. Use search_train_eval() to do a grid search over hyperparameters. You are free to pick whatever hyperparameters & values you want, but you should try to avoid badly overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor({'max_depth': 30, 'n_estimators': 81})\n",
      "\tTrain R^2: 0.997, Val  R^2: 0.983\n",
      "\tTrain MSE: 86.011, Val MSE: 547.458\n",
      "\tTrain MAE: 2.859, Val MAE: 7.583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "### Call your random forest model 'rfmodel'\n",
    "### Make sure to set random_state=seed for reproducibility!\n",
    "###* put your code here *###\n",
    "\n",
    "param_grid = {'max_depth':np.arange(5, 40, 5), 'n_estimators': np.arange(1, 100, 20)}\n",
    "model = RandomForestRegressor(random_state=seed)\n",
    "rfmodel = search_train_eval(model, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 4e] (5 points) Is your RF model better than the decision tree you trained for Task 4c? Justify your answer. What can you conclude about ensembles/random forests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Answer:\n",
    "# 1. Yes, my RF model is better than the decision tree I trained for Task 4c, because its performance on the \n",
    "# validation dataset is better than the decision tree in 4c in the sence of all three metrics: R^2, MSE, and MAE.\n",
    "# 2. I can conclude that, compare to a normal decison tree, the ensembles/random forests has lower variance \n",
    "# and similar bias, thanks to more diversity of trees. Overall enseles/random forests is better.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [CIS6930 Additional Task -- Task 5] (25 points): Stacking Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this task we'll use stacking to create a meta model or blender model to predict the target using predictions from 6 other models from Tasks 1 - 4 as features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5a] (10 points) Fill in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "# these are the models we'll use from previous tasks\n",
    "# (this is why it's important that you named the models as instructed in Tasks 2-4)\n",
    "regressors = [('lr', lrmodel), ('elasticnet', enmodel), ('ridge', rmodel), \n",
    "             ('dt', dtmodel), ('dtreg', dtregmodel), ('rf', rfmodel)]\n",
    "\n",
    "# this will return predictions for all of our regressors on matrix x\n",
    "def regressors_preds(x):\n",
    "    num_regs = len(regressors)\n",
    "    \n",
    "    ### Create an array to contain the predictions from the regressors over all examples in 'x'\n",
    "    ### Each regressor will correspond to one feature (i.e., one column)\n",
    "    ### The numpy array you return should have shape (x.shape[0], num_regs)\n",
    "    ###* put your code here (~4-6 lines) *###\n",
    "    predictions = np.zeros(x.shape[0])\n",
    "#     print(\"predictions shape: \", predictions.shape)\n",
    "    for regressor in regressors:\n",
    "#         print('---------------------------')\n",
    "#         print(\"predictions shape: \", predictions.shape)\n",
    "#         print(\"model_name: \", regressor[0])\n",
    "#         print(\"shape of data: \", x.shape)\n",
    "        x_pred = regressor[1].predict(x)\n",
    "        predictions = np.c_[predictions, x_pred]\n",
    "    predictions = predictions[:, 1:]\n",
    "#     print(\"################################\")\n",
    "#     print(\"predictions shape: \", predictions.shape)\n",
    "    assert predictions.shape == (x.shape[0], len(regressors))\n",
    "    return predictions   \n",
    "\n",
    "def stacking_train_eval(model_name, model, standardize=False):\n",
    "    ### Create a new training dataset 'meta_train_x' and 'meta_train_y'\n",
    "    ### For this use the validation data (val_x, val_y) alongside with regressors_preds()\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    meta_train_x = regressors_preds(val_x)\n",
    "    meta_train_y = val_y\n",
    "    \n",
    "    assert meta_train_x.shape == (2606, 6) and meta_train_x.shape[0] == meta_train_y.shape[0]\n",
    "\n",
    "\n",
    "    ### Create our new test dataset 'meta_test_x' and 'meta_test_y'\n",
    "    ### For this we use the test data (test_x, test_y) alongside with regressors_preds()\n",
    "    ###* put your code here (~2 lines) *###\n",
    "    meta_test_x = regressors_preds(test_x)\n",
    "    meta_test_y = test_y\n",
    "    \n",
    "    assert meta_test_x.shape == (2607, 6) and meta_test_x.shape[0] == meta_test_y.shape[0]\n",
    "    \n",
    "    # zscore normalize the features if standardize = True\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        meta_train_x = scaler.fit_transform(meta_train_x)\n",
    "        meta_test_x = scaler.transform(meta_test_x)\n",
    "\n",
    "    \n",
    "    # train the meta model\n",
    "    model.fit(meta_train_x, meta_train_y)\n",
    "\n",
    "    # make predictions & eval\n",
    "    train_pred = model.predict(meta_train_x)\n",
    "    test_pred = model.predict(meta_test_x)\n",
    "\n",
    "    train_error = mean_squared_error(train_pred, meta_train_y)\n",
    "    val_error = mean_squared_error(test_pred, meta_test_y)\n",
    "    \n",
    "    print('Stacking (Meta model: {})'.format(model_name))\n",
    "    r2_train = model.score(meta_train_x, meta_train_y)\n",
    "    r2_val = model.score(meta_test_x, meta_test_y)\n",
    "    print('\\tTrain R^2: {:.3f}, Val  R^2: {:.3f}'.format(r2_train, r2_val))\n",
    "    \n",
    "    print('\\tTrain MSE: {:.3f}, Test MSE: {:.3f}'.format(train_error, val_error))\n",
    "\n",
    "    train_error = mean_absolute_error(train_pred, meta_train_y)\n",
    "    val_error = mean_absolute_error(test_pred, test_y)\n",
    "\n",
    "    print('\\tTrain MAE: {:.3f}, Test MAE: {:.3f}'.format(train_error, val_error))\n",
    "    \n",
    "    return r2_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5b] (3 points) Train a SVM regression model with a linear kernel and C=100. Use stacking_train_eval(). You can set standardize=True to zscore normalize features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.984, Val  R^2: 0.978\n",
      "\tTrain MSE: 540.267, Test MSE: 736.985\n",
      "\tTrain MAE: 7.516, Test MAE: 7.742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9775845275741305"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "### Train a SVM regressor with a linear kernel and C=100\n",
    "### Note: the training will take a few minutes\n",
    "###* put your code here (~2-3 lines) *###\n",
    "\n",
    "svm_regressor = SVR(C = 100, kernel='linear')\n",
    "model_name = [model[0] for model in regressors]\n",
    "stacking_train_eval(model_name, svm_regressor, standardize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5c] (2 points) How good is this model? (A few sentences suffice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# This model is not as good as the rf model but better than other models.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5d] (5 points) Now train a SVM regression model with any other kernel (i.e., not linear) and combination of hyperparameters of your choice. Can you train a better stacking model than for Task 5b?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.325, Val  R^2: 0.328\n",
      "\tTrain MSE: 22380.015, Test MSE: 22092.261\n",
      "\tTrain MAE: 93.449, Test MAE: 92.096\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.107, Val  R^2: 0.116\n",
      "\tTrain MSE: 29613.160, Test MSE: 29050.514\n",
      "\tTrain MAE: 125.226, Test MAE: 123.313\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: -0.041, Val  R^2: -0.028\n",
      "\tTrain MSE: 34506.024, Test MSE: 33803.396\n",
      "\tTrain MAE: 136.537, Test MAE: 134.421\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.823, Val  R^2: 0.819\n",
      "\tTrain MSE: 5866.709, Test MSE: 5954.561\n",
      "\tTrain MAE: 29.864, Test MAE: 30.703\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.859, Val  R^2: 0.852\n",
      "\tTrain MSE: 4675.185, Test MSE: 4853.965\n",
      "\tTrain MAE: 34.729, Test MAE: 35.068\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.161, Val  R^2: 0.169\n",
      "\tTrain MSE: 27830.754, Test MSE: 27332.083\n",
      "\tTrain MAE: 122.146, Test MAE: 120.293\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.969, Val  R^2: 0.962\n",
      "\tTrain MSE: 1037.201, Test MSE: 1244.258\n",
      "\tTrain MAE: 10.698, Test MAE: 11.243\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.963, Val  R^2: 0.956\n",
      "\tTrain MSE: 1213.465, Test MSE: 1458.103\n",
      "\tTrain MAE: 14.830, Test MAE: 15.732\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.920, Val  R^2: 0.913\n",
      "\tTrain MSE: 2636.308, Test MSE: 2873.285\n",
      "\tTrain MAE: 26.251, Test MAE: 26.834\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.984, Val  R^2: 0.976\n",
      "\tTrain MSE: 539.425, Test MSE: 774.433\n",
      "\tTrain MAE: 7.557, Test MAE: 8.091\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.982, Val  R^2: 0.975\n",
      "\tTrain MSE: 610.040, Test MSE: 818.197\n",
      "\tTrain MAE: 8.158, Test MAE: 8.550\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.976, Val  R^2: 0.968\n",
      "\tTrain MSE: 806.859, Test MSE: 1046.491\n",
      "\tTrain MAE: 11.423, Test MAE: 12.122\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.987, Val  R^2: 0.977\n",
      "\tTrain MSE: 444.091, Test MSE: 766.287\n",
      "\tTrain MAE: 6.695, Test MAE: 7.602\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.983, Val  R^2: 0.977\n",
      "\tTrain MSE: 548.257, Test MSE: 741.264\n",
      "\tTrain MAE: 7.519, Test MAE: 7.817\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.982, Val  R^2: 0.976\n",
      "\tTrain MSE: 586.258, Test MSE: 801.649\n",
      "\tTrain MAE: 7.972, Test MAE: 8.358\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.987, Val  R^2: 0.977\n",
      "\tTrain MSE: 438.274, Test MSE: 767.187\n",
      "\tTrain MAE: 6.606, Test MAE: 7.561\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.984, Val  R^2: 0.978\n",
      "\tTrain MSE: 520.912, Test MSE: 723.423\n",
      "\tTrain MAE: 7.387, Test MAE: 7.675\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.983, Val  R^2: 0.977\n",
      "\tTrain MSE: 552.085, Test MSE: 755.382\n",
      "\tTrain MAE: 7.634, Test MAE: 7.976\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.987, Val  R^2: 0.977\n",
      "\tTrain MSE: 437.076, Test MSE: 766.967\n",
      "\tTrain MAE: 6.583, Test MAE: 7.566\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.985, Val  R^2: 0.978\n",
      "\tTrain MSE: 509.785, Test MSE: 718.423\n",
      "\tTrain MAE: 7.342, Test MAE: 7.628\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.983, Val  R^2: 0.977\n",
      "\tTrain MSE: 547.561, Test MSE: 749.313\n",
      "\tTrain MAE: 7.587, Test MAE: 7.909\n",
      "\n",
      "#######################################\n",
      "best_hyperparams:  {'C': 4000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Stacking (Meta model: ['lr', 'elasticnet', 'ridge', 'dt', 'dtreg', 'rf'])\n",
      "\tTrain R^2: 0.985, Val  R^2: 0.978\n",
      "\tTrain MSE: 509.785, Test MSE: 718.423\n",
      "\tTrain MAE: 7.342, Test MAE: 7.628\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9781491059792333"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "### Train a SVM regressor with any non-linear kernel and hyperparameters you want \n",
    "### You can do a hyperparameter search if you want\n",
    "###* put your code here (~2-3 lines) *###\n",
    "\n",
    "hyperparams_vals = {'C':[1e-1, 1.0, 10.0, 100.0, 1000, 3000, 4000], \n",
    "                    \"gamma\": [0.1, 0.01, 0.001],\n",
    "                    \"kernel\": ['rbf']}\n",
    "grid = ParameterGrid(hyperparams_vals)\n",
    "\n",
    "best_r2_score = -1\n",
    "for i, hyperparams in enumerate(list(grid)):\n",
    "#     print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "#     print('hyperparams: ', hyperparams)\n",
    "    svm_regressor = SVR().set_params(**hyperparams)\n",
    "    r2_score = stacking_train_eval(model_name, svm_regressor, standardize=True)\n",
    "    if best_r2_score < r2_score:\n",
    "        best_r2_score = r2_score\n",
    "        best_hyperparams = hyperparams\n",
    "print('\\n#######################################')\n",
    "print(\"best_hyperparams: \", best_hyperparams)\n",
    "best_model = SVR().set_params(**best_hyperparams)\n",
    "stacking_train_eval(model_name, best_model, standardize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 5e] (5 points) What do you conclude? Provide a plausible explanation why non-linear kernels do not seem to improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "###* put your answer as comment here *###\n",
    "#\n",
    "# Answer:\n",
    "# 1. My conclusion is that: non-linear kernels can improve the performance (see above result).\n",
    "# 2. My results from 5d show that the premise of this question is incorrect.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
