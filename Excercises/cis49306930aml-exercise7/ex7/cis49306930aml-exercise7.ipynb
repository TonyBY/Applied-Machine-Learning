{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Discovering Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Python version: 3.8.5 (default, Sep  4 2020, 07:30:14) \n",
      "[GCC 7.3.0]\n",
      "### Numpy version: 1.18.5\n",
      "### Scikit-learn version: 0.24.1\n",
      "### Tensorflow version: 2.3.1\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we'll use keras for neural networks\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + sys.version)\n",
    "print('### Numpy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "\n",
    "seed = 42 # deterministic seed\n",
    "np.random.seed(seed) \n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to think of Tensorflow? Is it like scikit-learn but for neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really, think of Tensorflow as a kind of NumPy with additional features (i.e., ability to create computational graphs on tensors, automatically compute derivative, run operations on GPUs). (Tensorflow also has many high-level APIs.)\n",
    "\n",
    "### What are tensors? Well formally they are multilinear maps from vector spaces to reals; but that doesn't matter the point is that tensors can represent scalars, vectors, matrices, etc.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beware that Tensorflow 2.0 is different from Tensorflow 1.0! In this course we'll use Tensorflow 2.0.\n",
    "\n",
    "### Compared to TF 1.0:\n",
    "### - TF 2.0 incorporates Keras as a high-level API\n",
    "### - TF 2.0 does *eager* execution by default!\n",
    "#### In TF 1.0 you would first build the computational graph (construction phase) and then you would execute it in a session (execution phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we set the seed for Tensorflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get familiar with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "scalar = 7 # a scalar in Python\n",
    "\n",
    "scalar_tf = tf.constant(7) # a TF scalar\n",
    "\n",
    "print(scalar)\n",
    "print(scalar_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like numpy array, tensors have a shape and dtype property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 -5  9  1]\n",
      "tf.Tensor([ 3 -5  9  1], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vector_np = np.array([3, -5, 9, 1])\n",
    "print(vector_np)\n",
    "\n",
    "vector_tf = tf.constant([3, -5, 9, 1])\n",
    "print(vector_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can get the dtype, shape of tensor. We can also get at the underlying numpy array using numpy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (4,)\n",
      "numpy array: [ 3 -5  9  1], type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('dtype: ' + str(vector_tf.dtype))\n",
    "print('shape: ' + str(vector_tf.shape))\n",
    "\n",
    "numpy_arr = vector_tf.numpy()\n",
    "print('numpy array: {}, type: {}'.format(str(numpy_arr), type(numpy_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3 -7]\n",
      " [ 0  9]], shape=(2, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# we can also build a tensor out of a numpy array\n",
    "matrix_np = np.array([[3, -7], [0, 9]])\n",
    "matrix_tf = tf.constant(matrix_np)\n",
    "\n",
    "print(matrix_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[0.6645621  0.44100678 0.3528825  0.46448255]\n",
      " [0.03366041 0.68467236 0.74011743 0.8724445 ]], shape=(2, 4), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# We can construct tensors in similar ways to how we construct some numpy arrays. For example:\n",
    "\n",
    "tf_ones = tf.ones((3,3))\n",
    "print(tf_ones)\n",
    "print()\n",
    "\n",
    "# and\n",
    "\n",
    "tf_unifrand = tf.random.uniform((2, 4))\n",
    "print(tf_unifrand)\n",
    "print()\n",
    "\n",
    "tf_zeros_like_ones = tf.zeros_like(tf_ones)\n",
    "print(tf_zeros_like_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check if something is a Tensor. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.is_tensor(matrix_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.is_tensor(matrix_tf.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also place tensors onto devices. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    matrix_on_gpu0 = tf.identity(matrix_tf) # won't work if you don't have a GPU\n",
    "    \n",
    "print(matrix_on_gpu0.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs Available:  1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do operations as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 5], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3])\n",
    "y = tf.constant([-1, 2])\n",
    "\n",
    "add_x_y = tf.add(x, y)\n",
    "print(add_x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 5], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Can we do x + y?\n",
    "x_plus_y = x + y\n",
    "print(x_plus_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1 -3], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# multiplication by a scalar\n",
    "x_mult_mone = x * -1\n",
    "print(x_mult_mone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1  6], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# elementwise multiplication\n",
    "x_mult_y = x * y\n",
    "# or: x_mult_y = tf.multiply(x,y)\n",
    "print(x_mult_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what about matrix multiplication and similar ops?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3, 2)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([[1, 0, 3], [0, -2, 5]])\n",
    "B = tf.constant([2, -3])\n",
    "\n",
    "print(A.shape)\n",
    "print(B.shape)\n",
    "\n",
    "A_transposed = tf.transpose(A)\n",
    "print(A_transposed.shape)\n",
    "\n",
    "B_reshaped = tf.reshape(B, (-1, 1))\n",
    "\n",
    "print(B_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2]\n",
      " [ 6]\n",
      " [-9]], shape=(3, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "A_T_matrix_mult_B = tf.linalg.matmul(A_transposed, B_reshaped)\n",
    "# or A_transposed @ B_reshaped\n",
    "\n",
    "print(A_T_matrix_mult_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because tensors are immutable, we cannot change their values in place. This seems like it could be a problem because parameters of a model are variables whose values should change frequently.\n",
    "### For this we can use: tf.Variable\n",
    "\n",
    "#### We'll typically use those for model parameters and other variables that need to change often in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's declare a variable\n",
    "# variables in TF represent tensors and you change their values by running operations (ops) on them\n",
    "x = tf.Variable([7, 3], name=\"x\")   # we can name variables (we don't have to, but we can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=(2,) dtype=int32, numpy=array([7, 3], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) <dtype: 'int32'> x:0\n"
     ]
    }
   ],
   "source": [
    "# Variables also have shape and dtype, etc.\n",
    "print(x.shape, x.dtype, x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([49  9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# if you do ops on a variable the result is a tensor not a variable!\n",
    "xsquared = tf.square(x)\n",
    "print(xsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=(2,) dtype=int32, numpy=array([-1,  0], dtype=int32)>\n",
      "<tf.Variable 'x:0' shape=(2,) dtype=int32, numpy=array([2, 3], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# but variables unlike constant can have their values changed in-place (e.g., using one of the assign*() methods). \n",
    "# For example:\n",
    "x.assign(tf.constant([-1, 0]))\n",
    "print(x)\n",
    "\n",
    "x.assign_add(tf.constant([3, 3]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (2,) and (3,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7c467cd52e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# However, shapes must be compatible!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-3.8/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m    860\u001b[0m           self.handle, value_tensor, name=name)\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-3.8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \"\"\"\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (2,) and (3,) are incompatible"
     ]
    }
   ],
   "source": [
    "# However, shapes must be compatible!\n",
    "x.assign(tf.constant([5, 9, -17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool (and important) feature: automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2, name=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we want to compute the derivative of x ** 3. Clearly it's 3 x ** 2\n",
    "### We can do it using tf.GradientTape to keep track of the operations on tensor and then compute the gradient afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.0, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "# Note: to watch a tensor it must be floating point, so we'll cast x\n",
    "x = tf.cast(x, dtype=tf.float16)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x) # we tell the tape to watch variable 'x'\n",
    "    # now we can do operations like x ** 3\n",
    "    y = x ** 3\n",
    "    \n",
    "    \n",
    "## What is y?\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is the gradient of y wrt x?\n",
    "# we want the gradient of y (x**3) with respect to x\n",
    "grad_xcube = tape.gradient(target=y, sources=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12.0, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(grad_xcube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "print((3 * x**2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: once we get the gradients from the tape, the resources are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "GradientTape.gradient can only be called once on non-persistent tapes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7920cd642f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This will cause an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrad_xcube2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-3.8/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \"\"\"\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m       raise RuntimeError(\"GradientTape.gradient can only be called once on \"\n\u001b[0m\u001b[1;32m   1015\u001b[0m                          \"non-persistent tapes.\")\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: GradientTape.gradient can only be called once on non-persistent tapes."
     ]
    }
   ],
   "source": [
    "# This will cause an error\n",
    "grad_xcube2 = tape.gradient(target=y, sources=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we can create a persistent tape if we want. For example (a bit more complicated example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.         4.8520303  7.690286   9.704061  11.266066 ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([1, 2, 3, 4, 5])\n",
    "x = tf.Variable(x_np, name=\"x\", dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "    # watch_accessed_variables=True allows us to not have to set each variable we want to watch\n",
    "    \n",
    "    z = tf.constant(7, dtype=tf.float32)\n",
    "    #z = tf.Variable([7, 7, 7, 7, 7], dtype=tf.float32, name='z')\n",
    "    \n",
    "    y = z * tf.math.log(x)\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7.        3.5       2.3333335 1.75      1.4      ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "grad_y_wrt_x = tape.gradient(target=y, sources=x)\n",
    "print(grad_y_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y_wrt_x2 = tape.gradient(target=y, sources=x) # we can grab it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# we can even grab the gradient with respect to something else (e.g., z)\n",
    "grad_y_wrt_z = tape.gradient(target=y, sources=z)\n",
    "print(grad_y_wrt_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So this is nice but what can we do with it? Let's train linear regression model with Tensorflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this, we'll create some simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-1.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [-7.],\n",
       "       [ 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First make up a model\n",
    "true_theta = tf.constant([-1, 5, 2, -7, 3], dtype=tf.float32)[:, tf.newaxis]\n",
    "true_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1500\n",
    "ntr = 1000\n",
    "\n",
    "# make some random data\n",
    "x = tf.constant(tf.random.uniform((n, 5), minval=-1, maxval=+1), dtype=tf.float32)\n",
    "\n",
    "# now calculate the y based on the true parameters\n",
    "y = tf.constant(x @ true_theta, dtype=tf.float32)\n",
    "\n",
    "# split the data\n",
    "train_x = x[:ntr,:]\n",
    "train_y = y[:ntr]\n",
    "\n",
    "val_x = x[ntr:,:].numpy()\n",
    "val_y = y[ntr:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is batch gradient descent\n",
    "def train_lr_tf(x, y, eta=0.05, num_iter=250, verbose=False):\n",
    "    \n",
    "    n, m = x.shape\n",
    "    \n",
    "    # weights / parameters (randomly initialized)\n",
    "    theta = tf.Variable(tf.random.uniform((m, 1), minval=-1, maxval=1), dtype=tf.float32)\n",
    "        \n",
    "    for i in range(0, num_iter):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = tf.linalg.matmul(x, theta) # prediction\n",
    "            mse = tf.reduce_mean(tf.square(y - y_pred)) \n",
    "        \n",
    "        # extract the gradients \n",
    "        gradient_vec = tape.gradient(mse, theta)\n",
    "\n",
    "        # do a gradient descent step (we use assign_sub() to update theta in place)\n",
    "        theta.assign_sub(tf.constant([eta], dtype=tf.float32) * gradient_vec) \n",
    "\n",
    "\n",
    "        if verbose and i % int(num_iter/10) == 0:\n",
    "            print('Iteration {}: the (training) loss (MSE) is {:.5f}'.format(i, mse))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: the (training) loss (MSE) is 28.77184\n",
      "Iteration 25: the (training) loss (MSE) is 5.35863\n",
      "Iteration 50: the (training) loss (MSE) is 1.00340\n",
      "Iteration 75: the (training) loss (MSE) is 0.18878\n",
      "Iteration 100: the (training) loss (MSE) is 0.03567\n",
      "Iteration 125: the (training) loss (MSE) is 0.00676\n",
      "Iteration 150: the (training) loss (MSE) is 0.00129\n",
      "Iteration 175: the (training) loss (MSE) is 0.00025\n",
      "Iteration 200: the (training) loss (MSE) is 0.00005\n",
      "Iteration 225: the (training) loss (MSE) is 0.00001\n"
     ]
    }
   ],
   "source": [
    "# Let's do the training\n",
    "theta = train_lr_tf(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-0.9995907],\n",
      "       [ 4.998418 ],\n",
      "       [ 1.9997115],\n",
      "       [-6.998523 ],\n",
      "       [ 2.9993668]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF-GD Train] R^2: 1.00, RMSE: 0.00, MedAE: 0.00\n",
      "[TF-GD Val] R^2: 1.00, RMSE: 0.00, MedAE: 0.00\n"
     ]
    }
   ],
   "source": [
    "# given model parameters 'theta' and a feature matrix 'x', this will return predictions\n",
    "def predict_theta(theta, x):\n",
    "    return np.dot(x, theta) # note: there is no bias 'b' in this case\n",
    "    \n",
    "from sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\n",
    "\n",
    "def print_scores(desc, true_y, pred_y):\n",
    "    r2 = r2_score(true_y, pred_y)\n",
    "    rmse = mean_squared_error(true_y, pred_y, squared=False)\n",
    "    medae = median_absolute_error(true_y, pred_y)\n",
    "    \n",
    "    print('[{}] R^2: {:.2f}, RMSE: {:.2f}, MedAE: {:.2f}'.format(desc, r2, rmse, medae))\n",
    "        \n",
    "print_scores('TF-GD Train', train_y, predict_theta(theta.numpy(), train_x))\n",
    "print_scores('TF-GD Val', val_y, predict_theta(theta.numpy(), val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is nice but it seems tedious. Do we have to implement the gradient descent ourselves and do all the low-level stuff?\n",
    "### => No, we can use a higher-level API like Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to define the architecture\n",
    "def create_model(input_shape, num_outputs=1):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # declare input layer (keras needs to know the number of input features to expect)\n",
    "    model.add(keras.Input(shape=(input_shape[1],))) \n",
    "    \n",
    "    # next add our output layer (1 output, linear activation function)\n",
    "    model.add(keras.layers.Dense(num_outputs, activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# first we create the model (i.e., define the architecture)\n",
    "model = create_model(train_x.shape)\n",
    "\n",
    "# Tip: before you go on, use summary() to check that the architecture is what you intended\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we compile it to specify optimizer, loss, and metrics\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 26.3165 - mae: 4.1876 - val_loss: 25.0808 - val_mae: 4.1231\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 20.3852 - mae: 3.6921 - val_loss: 19.4303 - val_mae: 3.6274\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 15.7941 - mae: 3.2478 - val_loss: 15.0541 - val_mae: 3.1917\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 12.2403 - mae: 2.8572 - val_loss: 11.6641 - val_mae: 2.8084\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 9.4860 - mae: 2.5180 - val_loss: 9.0377 - val_mae: 2.4714\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.3543 - mae: 2.2166 - val_loss: 7.0027 - val_mae: 2.1747\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 5.6970 - mae: 1.9514 - val_loss: 5.4266 - val_mae: 1.9139\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 4.4162 - mae: 1.7169 - val_loss: 4.2052 - val_mae: 1.6845\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.4232 - mae: 1.5112 - val_loss: 3.2589 - val_mae: 1.4825\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.6530 - mae: 1.3312 - val_loss: 2.5257 - val_mae: 1.3048\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.0563 - mae: 1.1716 - val_loss: 1.9576 - val_mae: 1.1485\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.5944 - mae: 1.0312 - val_loss: 1.5172 - val_mae: 1.0109\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2359 - mae: 0.9096 - val_loss: 1.1760 - val_mae: 0.8899\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9581 - mae: 0.8003 - val_loss: 0.9116 - val_mae: 0.7834\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.7430 - mae: 0.7031 - val_loss: 0.7066 - val_mae: 0.6897\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.5760 - mae: 0.6191 - val_loss: 0.5477 - val_mae: 0.6072\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4466 - mae: 0.5457 - val_loss: 0.4246 - val_mae: 0.5346\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3463 - mae: 0.4811 - val_loss: 0.3292 - val_mae: 0.4707\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2684 - mae: 0.4227 - val_loss: 0.2552 - val_mae: 0.4144\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2083 - mae: 0.3722 - val_loss: 0.1978 - val_mae: 0.3649\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3279 - val_loss: 0.1534 - val_mae: 0.3212\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1252 - mae: 0.2884 - val_loss: 0.1189 - val_mae: 0.2829\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0971 - mae: 0.2543 - val_loss: 0.0922 - val_mae: 0.2491\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0753 - mae: 0.2237 - val_loss: 0.0715 - val_mae: 0.2193\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0584 - mae: 0.1975 - val_loss: 0.0554 - val_mae: 0.1931\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0453 - mae: 0.1734 - val_loss: 0.0430 - val_mae: 0.1701\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0351 - mae: 0.1529 - val_loss: 0.0333 - val_mae: 0.1498\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0272 - mae: 0.1346 - val_loss: 0.0259 - val_mae: 0.1319\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0211 - mae: 0.1185 - val_loss: 0.0201 - val_mae: 0.1161\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 986us/step - loss: 0.0164 - mae: 0.1044 - val_loss: 0.0156 - val_mae: 0.1023\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0127 - mae: 0.0919 - val_loss: 0.0121 - val_mae: 0.0901\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0099 - mae: 0.0809 - val_loss: 0.0094 - val_mae: 0.0793\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0076 - mae: 0.0713 - val_loss: 0.0073 - val_mae: 0.0699\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0059 - mae: 0.0627 - val_loss: 0.0056 - val_mae: 0.0615\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0046 - mae: 0.0553 - val_loss: 0.0044 - val_mae: 0.0542\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0487 - val_loss: 0.0034 - val_mae: 0.0477\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0428 - val_loss: 0.0026 - val_mae: 0.0420\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0378 - val_loss: 0.0020 - val_mae: 0.0370\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0332 - val_loss: 0.0016 - val_mae: 0.0326\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0293 - val_loss: 0.0012 - val_mae: 0.0287\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0258 - val_loss: 9.5095e-04 - val_mae: 0.0253\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.7889e-04 - mae: 0.0227 - val_loss: 7.3772e-04 - val_mae: 0.0223\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 6.0440e-04 - mae: 0.0200 - val_loss: 5.7226e-04 - val_mae: 0.0196\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.6884e-04 - mae: 0.0176 - val_loss: 4.4399e-04 - val_mae: 0.0173\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.6391e-04 - mae: 0.0155 - val_loss: 3.4448e-04 - val_mae: 0.0152\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.8237e-04 - mae: 0.0137 - val_loss: 2.6725e-04 - val_mae: 0.0134\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.1910e-04 - mae: 0.0120 - val_loss: 2.0733e-04 - val_mae: 0.0118\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7002e-04 - mae: 0.0106 - val_loss: 1.6087e-04 - val_mae: 0.0104\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 1.3192e-04 - mae: 0.0093 - val_loss: 1.2482e-04 - val_mae: 0.0092\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0237e-04 - mae: 0.0082 - val_loss: 9.6854e-05 - val_mae: 0.0081\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.9448e-05 - mae: 0.0072 - val_loss: 7.5152e-05 - val_mae: 0.0071\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.1691e-05 - mae: 0.0064 - val_loss: 5.8307e-05 - val_mae: 0.0063\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.7841e-05 - mae: 0.0056 - val_loss: 4.5241e-05 - val_mae: 0.0055\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 3.7136e-05 - mae: 0.0049 - val_loss: 3.5101e-05 - val_mae: 0.0049\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.8813e-05 - mae: 0.0044 - val_loss: 2.7239e-05 - val_mae: 0.0043\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 2.2365e-05 - mae: 0.0038 - val_loss: 2.1139e-05 - val_mae: 0.0038\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7359e-05 - mae: 0.0034 - val_loss: 1.6407e-05 - val_mae: 0.0033\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3475e-05 - mae: 0.0030 - val_loss: 1.2732e-05 - val_mae: 0.0029\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0461e-05 - mae: 0.0026 - val_loss: 9.8832e-06 - val_mae: 0.0026\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.1192e-06 - mae: 0.0023 - val_loss: 7.6726e-06 - val_mae: 0.0023\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3079e-06 - mae: 0.0020 - val_loss: 5.9574e-06 - val_mae: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.8957e-06 - mae: 0.0018 - val_loss: 4.6223e-06 - val_mae: 0.0018\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 3.8002e-06 - mae: 0.0016 - val_loss: 3.5877e-06 - val_mae: 0.0016\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.9493e-06 - mae: 0.0014 - val_loss: 2.7845e-06 - val_mae: 0.0014\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 2.2896e-06 - mae: 0.0012 - val_loss: 2.1602e-06 - val_mae: 0.0012\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7773e-06 - mae: 0.0011 - val_loss: 1.6769e-06 - val_mae: 0.0011\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3793e-06 - mae: 9.5352e-04 - val_loss: 1.3009e-06 - val_mae: 9.3623e-04\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0705e-06 - mae: 8.4032e-04 - val_loss: 1.0106e-06 - val_mae: 8.2515e-04\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.3127e-07 - mae: 7.3953e-04 - val_loss: 7.8426e-07 - val_mae: 7.2693e-04\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4588e-07 - mae: 6.5262e-04 - val_loss: 6.1002e-07 - val_mae: 6.4109e-04\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.0192e-07 - mae: 5.7539e-04 - val_loss: 4.7357e-07 - val_mae: 5.6484e-04\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.8990e-07 - mae: 5.0671e-04 - val_loss: 3.6797e-07 - val_mae: 4.9790e-04\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 3.0255e-07 - mae: 4.4668e-04 - val_loss: 2.8501e-07 - val_mae: 4.3819e-04\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 2.3446e-07 - mae: 3.9293e-04 - val_loss: 2.2083e-07 - val_mae: 3.8572e-04\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 1.8192e-07 - mae: 3.4633e-04 - val_loss: 1.7142e-07 - val_mae: 3.3984e-04\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4107e-07 - mae: 3.0460e-04 - val_loss: 1.3317e-07 - val_mae: 2.9952e-04\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 1.0973e-07 - mae: 2.6891e-04 - val_loss: 1.0338e-07 - val_mae: 2.6390e-04\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.5025e-08 - mae: 2.3656e-04 - val_loss: 8.0048e-08 - val_mae: 2.3222e-04\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5898e-08 - mae: 2.0825e-04 - val_loss: 6.2064e-08 - val_mae: 2.0448e-04\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.1298e-08 - mae: 1.8367e-04 - val_loss: 4.8475e-08 - val_mae: 1.8070e-04\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.9964e-08 - mae: 1.6222e-04 - val_loss: 3.7785e-08 - val_mae: 1.5953e-04\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.1149e-08 - mae: 1.4329e-04 - val_loss: 2.9360e-08 - val_mae: 1.4062e-04\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.4202e-08 - mae: 1.2615e-04 - val_loss: 2.2815e-08 - val_mae: 1.2397e-04\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.8762e-08 - mae: 1.1109e-04 - val_loss: 1.7678e-08 - val_mae: 1.0913e-04\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4583e-08 - mae: 9.7790e-05 - val_loss: 1.3703e-08 - val_mae: 9.6088e-05\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1265e-08 - mae: 8.6041e-05 - val_loss: 1.0565e-08 - val_mae: 8.4372e-05\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.7210e-09 - mae: 7.5834e-05 - val_loss: 8.2078e-09 - val_mae: 7.4370e-05\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.7871e-09 - mae: 6.6735e-05 - val_loss: 6.4666e-09 - val_mae: 6.6006e-05\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 5.3549e-09 - mae: 5.9285e-05 - val_loss: 5.0051e-09 - val_mae: 5.8069e-05\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.1057e-09 - mae: 5.1984e-05 - val_loss: 3.8483e-09 - val_mae: 5.0929e-05\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.1074e-09 - mae: 4.5226e-05 - val_loss: 2.8553e-09 - val_mae: 4.3875e-05\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.2979e-09 - mae: 3.8866e-05 - val_loss: 2.0886e-09 - val_mae: 3.7537e-05\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.6646e-09 - mae: 3.3091e-05 - val_loss: 1.5218e-09 - val_mae: 3.2051e-05\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2708e-09 - mae: 2.8987e-05 - val_loss: 1.1660e-09 - val_mae: 2.8055e-05\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 9.6860e-10 - mae: 2.5241e-05 - val_loss: 9.4909e-10 - val_mae: 2.5307e-05\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.2624e-10 - mae: 2.3334e-05 - val_loss: 8.4160e-10 - val_mae: 2.3803e-05\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.3582e-10 - mae: 2.1986e-05 - val_loss: 7.6816e-10 - val_mae: 2.2713e-05\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 6.8673e-10 - mae: 2.1252e-05 - val_loss: 6.9390e-10 - val_mae: 2.1565e-05\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.1685e-10 - mae: 2.0135e-05 - val_loss: 6.5119e-10 - val_mae: 2.0875e-05\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 5.8620e-10 - mae: 1.9623e-05 - val_loss: 6.1501e-10 - val_mae: 2.0274e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69f8308e20>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we train the model\n",
    "model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we extract the parameters?\n",
    "def extract_weights(model):\n",
    "    for layer in model.layers:\n",
    "        return layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the weights? Are they similar as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.9999956],\n",
      "       [ 4.999971 ],\n",
      "       [ 1.9999985],\n",
      "       [-6.999973 ],\n",
      "       [ 2.9999857]], dtype=float32), array([2.5642566e-07], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weights = extract_weights(model)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a more complex problem with a more complex neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use the Adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (38762, 88), (38762,)\n",
      "Test: (3231, 88), (3231,)\n",
      "Validation: (3229, 88), (3229,)\n"
     ]
    }
   ],
   "source": [
    "### In this case, we'll directly load the Adult dataset pre-processed in a similar way as for assignment 1\n",
    "### and we'll immediately split it into train, test, validation.\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, features, labels = utils.load_preproc_adult(prop_vec=prop_vec, seed=seed)\n",
    "\n",
    "# check that we have what we expect\n",
    "print('Training: {}, {}'.format(train_x.shape, train_y.shape))\n",
    "print('Test: {}, {}'.format(test_x.shape, test_y.shape))\n",
    "print('Validation: {}, {}'.format(val_x.shape, val_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In assignment 2 we had found the best model was a SVM classifier which achieved around 85% accuracy. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to define the architecture\n",
    "def create_model_adult(input_shape, hidden_widths=[96, 32], num_outputs=1):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # declare input layer (keras needs to know the number of input features to expect)\n",
    "    model.add(keras.Input(shape=(input_shape[1],))) \n",
    "    \n",
    "    # add two hidden layers with ReLU activation\n",
    "    model.add(keras.layers.Dense(hidden_widths[0], activation='relu'))\n",
    "    model.add(keras.layers.Dense(hidden_widths[1], activation='relu'))\n",
    "    \n",
    "    # next add our output layer (binary classification with 1 output, so sigmoid makes the most sense)\n",
    "    model.add(keras.layers.Dense(num_outputs, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 96)                8544      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                3104      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 11,681\n",
      "Trainable params: 11,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model (i.e., define the architecture)\n",
    "model = create_model_adult(train_x.shape)\n",
    "\n",
    "# Tip: before you go on, use summary() to check that the architecture is what you intended\n",
    "model.summary()\n",
    "\n",
    "# then we compile it to specify optimizer, loss, and metrics\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.4807 - accuracy: 0.7734 - val_loss: 0.3913 - val_accuracy: 0.8210\n",
      "Epoch 2/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3657 - accuracy: 0.8299 - val_loss: 0.3540 - val_accuracy: 0.8346\n",
      "Epoch 3/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3456 - accuracy: 0.8372 - val_loss: 0.3435 - val_accuracy: 0.8374\n",
      "Epoch 4/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3366 - accuracy: 0.8427 - val_loss: 0.3377 - val_accuracy: 0.8421\n",
      "Epoch 5/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3308 - accuracy: 0.8462 - val_loss: 0.3332 - val_accuracy: 0.8421\n",
      "Epoch 6/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3268 - accuracy: 0.8483 - val_loss: 0.3300 - val_accuracy: 0.8421\n",
      "Epoch 7/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3239 - accuracy: 0.8497 - val_loss: 0.3274 - val_accuracy: 0.8442\n",
      "Epoch 8/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3220 - accuracy: 0.8504 - val_loss: 0.3261 - val_accuracy: 0.8439\n",
      "Epoch 9/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3205 - accuracy: 0.8508 - val_loss: 0.3248 - val_accuracy: 0.8445\n",
      "Epoch 10/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3193 - accuracy: 0.8510 - val_loss: 0.3237 - val_accuracy: 0.8430\n",
      "Epoch 11/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3184 - accuracy: 0.8515 - val_loss: 0.3229 - val_accuracy: 0.8430\n",
      "Epoch 12/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3176 - accuracy: 0.8516 - val_loss: 0.3224 - val_accuracy: 0.8430\n",
      "Epoch 13/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3170 - accuracy: 0.8520 - val_loss: 0.3214 - val_accuracy: 0.8436\n",
      "Epoch 14/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3164 - accuracy: 0.8521 - val_loss: 0.3202 - val_accuracy: 0.8445\n",
      "Epoch 15/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3158 - accuracy: 0.8527 - val_loss: 0.3211 - val_accuracy: 0.8458\n",
      "Epoch 16/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3154 - accuracy: 0.8525 - val_loss: 0.3193 - val_accuracy: 0.8452\n",
      "Epoch 17/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3150 - accuracy: 0.8533 - val_loss: 0.3193 - val_accuracy: 0.8455\n",
      "Epoch 18/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3145 - accuracy: 0.8526 - val_loss: 0.3189 - val_accuracy: 0.8452\n",
      "Epoch 19/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3142 - accuracy: 0.8533 - val_loss: 0.3188 - val_accuracy: 0.8439\n",
      "Epoch 20/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.8535 - val_loss: 0.3184 - val_accuracy: 0.8461\n",
      "Epoch 21/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3136 - accuracy: 0.8532 - val_loss: 0.3176 - val_accuracy: 0.8470\n",
      "Epoch 22/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3134 - accuracy: 0.8535 - val_loss: 0.3178 - val_accuracy: 0.8464\n",
      "Epoch 23/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3131 - accuracy: 0.8539 - val_loss: 0.3180 - val_accuracy: 0.8470\n",
      "Epoch 24/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3128 - accuracy: 0.8540 - val_loss: 0.3183 - val_accuracy: 0.8467\n",
      "Epoch 25/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3126 - accuracy: 0.8541 - val_loss: 0.3171 - val_accuracy: 0.8476\n",
      "Epoch 26/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3124 - accuracy: 0.8544 - val_loss: 0.3168 - val_accuracy: 0.8473\n",
      "Epoch 27/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3122 - accuracy: 0.8546 - val_loss: 0.3170 - val_accuracy: 0.8467\n",
      "Epoch 28/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3120 - accuracy: 0.8542 - val_loss: 0.3166 - val_accuracy: 0.8483\n",
      "Epoch 29/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3117 - accuracy: 0.8547 - val_loss: 0.3167 - val_accuracy: 0.8479\n",
      "Epoch 30/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3116 - accuracy: 0.8550 - val_loss: 0.3161 - val_accuracy: 0.8479\n",
      "Epoch 31/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3115 - accuracy: 0.8548 - val_loss: 0.3162 - val_accuracy: 0.8464\n",
      "Epoch 32/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3113 - accuracy: 0.8553 - val_loss: 0.3166 - val_accuracy: 0.8473\n",
      "Epoch 33/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3109 - accuracy: 0.8554 - val_loss: 0.3161 - val_accuracy: 0.8476\n",
      "Epoch 34/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3109 - accuracy: 0.8551 - val_loss: 0.3162 - val_accuracy: 0.8486\n",
      "Epoch 35/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3107 - accuracy: 0.8554 - val_loss: 0.3164 - val_accuracy: 0.8467\n",
      "Epoch 36/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3105 - accuracy: 0.8553 - val_loss: 0.3166 - val_accuracy: 0.8458\n",
      "Epoch 37/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3104 - accuracy: 0.8552 - val_loss: 0.3165 - val_accuracy: 0.8455\n",
      "Epoch 38/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3104 - accuracy: 0.8552 - val_loss: 0.3158 - val_accuracy: 0.8489\n",
      "Epoch 39/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3101 - accuracy: 0.8555 - val_loss: 0.3160 - val_accuracy: 0.8483\n",
      "Epoch 40/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3100 - accuracy: 0.8551 - val_loss: 0.3158 - val_accuracy: 0.8489\n",
      "Epoch 41/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3098 - accuracy: 0.8559 - val_loss: 0.3163 - val_accuracy: 0.8470\n",
      "Epoch 42/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3098 - accuracy: 0.8554 - val_loss: 0.3155 - val_accuracy: 0.8489\n",
      "Epoch 43/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3096 - accuracy: 0.8556 - val_loss: 0.3160 - val_accuracy: 0.8476\n",
      "Epoch 44/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3094 - accuracy: 0.8557 - val_loss: 0.3156 - val_accuracy: 0.8501\n",
      "Epoch 45/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3094 - accuracy: 0.8551 - val_loss: 0.3161 - val_accuracy: 0.8473\n",
      "Epoch 46/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3092 - accuracy: 0.8565 - val_loss: 0.3155 - val_accuracy: 0.8476\n",
      "Epoch 47/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3091 - accuracy: 0.8563 - val_loss: 0.3155 - val_accuracy: 0.8489\n",
      "Epoch 48/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3091 - accuracy: 0.8554 - val_loss: 0.3155 - val_accuracy: 0.8495\n",
      "Epoch 49/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3088 - accuracy: 0.8560 - val_loss: 0.3160 - val_accuracy: 0.8498\n",
      "Epoch 50/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3088 - accuracy: 0.8564 - val_loss: 0.3159 - val_accuracy: 0.8501\n",
      "Epoch 51/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3085 - accuracy: 0.8571 - val_loss: 0.3155 - val_accuracy: 0.8498\n",
      "Epoch 52/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3085 - accuracy: 0.8561 - val_loss: 0.3161 - val_accuracy: 0.8464\n",
      "Epoch 53/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3083 - accuracy: 0.8563 - val_loss: 0.3155 - val_accuracy: 0.8492\n",
      "Epoch 54/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3083 - accuracy: 0.8565 - val_loss: 0.3153 - val_accuracy: 0.8489\n",
      "Epoch 55/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3081 - accuracy: 0.8568 - val_loss: 0.3154 - val_accuracy: 0.8479\n",
      "Epoch 56/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3081 - accuracy: 0.8562 - val_loss: 0.3162 - val_accuracy: 0.8473\n",
      "Epoch 57/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3079 - accuracy: 0.8564 - val_loss: 0.3156 - val_accuracy: 0.8489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8563 - val_loss: 0.3161 - val_accuracy: 0.8467\n",
      "Epoch 59/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8569 - val_loss: 0.3153 - val_accuracy: 0.8492\n",
      "Epoch 60/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8573 - val_loss: 0.3150 - val_accuracy: 0.8507\n",
      "Epoch 61/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3075 - accuracy: 0.8564 - val_loss: 0.3155 - val_accuracy: 0.8483\n",
      "Epoch 62/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3074 - accuracy: 0.8561 - val_loss: 0.3153 - val_accuracy: 0.8476\n",
      "Epoch 63/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3073 - accuracy: 0.8570 - val_loss: 0.3167 - val_accuracy: 0.8461\n",
      "Epoch 64/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3071 - accuracy: 0.8571 - val_loss: 0.3151 - val_accuracy: 0.8498\n",
      "Epoch 65/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3071 - accuracy: 0.8577 - val_loss: 0.3152 - val_accuracy: 0.8489\n",
      "Epoch 66/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3070 - accuracy: 0.8574 - val_loss: 0.3158 - val_accuracy: 0.8473\n",
      "Epoch 67/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3067 - accuracy: 0.8574 - val_loss: 0.3165 - val_accuracy: 0.8467\n",
      "Epoch 68/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3067 - accuracy: 0.8574 - val_loss: 0.3153 - val_accuracy: 0.8483\n",
      "Epoch 69/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3067 - accuracy: 0.8569 - val_loss: 0.3159 - val_accuracy: 0.8473\n",
      "Epoch 70/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8575 - val_loss: 0.3156 - val_accuracy: 0.8483\n",
      "Epoch 71/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3065 - accuracy: 0.8568 - val_loss: 0.3151 - val_accuracy: 0.8467\n",
      "Epoch 72/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3064 - accuracy: 0.8566 - val_loss: 0.3151 - val_accuracy: 0.8479\n",
      "Epoch 73/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3062 - accuracy: 0.8574 - val_loss: 0.3163 - val_accuracy: 0.8470\n",
      "Epoch 74/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3062 - accuracy: 0.8569 - val_loss: 0.3154 - val_accuracy: 0.8510\n",
      "Epoch 75/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3061 - accuracy: 0.8575 - val_loss: 0.3147 - val_accuracy: 0.8501\n",
      "Epoch 76/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3060 - accuracy: 0.8573 - val_loss: 0.3154 - val_accuracy: 0.8489\n",
      "Epoch 77/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3059 - accuracy: 0.8576 - val_loss: 0.3149 - val_accuracy: 0.8498\n",
      "Epoch 78/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3056 - accuracy: 0.8567 - val_loss: 0.3153 - val_accuracy: 0.8486\n",
      "Epoch 79/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3057 - accuracy: 0.8573 - val_loss: 0.3164 - val_accuracy: 0.8476\n",
      "Epoch 80/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3055 - accuracy: 0.8580 - val_loss: 0.3158 - val_accuracy: 0.8476\n",
      "Epoch 81/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3056 - accuracy: 0.8575 - val_loss: 0.3151 - val_accuracy: 0.8507\n",
      "Epoch 82/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3054 - accuracy: 0.8576 - val_loss: 0.3159 - val_accuracy: 0.8461\n",
      "Epoch 83/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3054 - accuracy: 0.8573 - val_loss: 0.3153 - val_accuracy: 0.8467\n",
      "Epoch 84/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3052 - accuracy: 0.8580 - val_loss: 0.3151 - val_accuracy: 0.8476\n",
      "Epoch 85/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3053 - accuracy: 0.8581 - val_loss: 0.3157 - val_accuracy: 0.8483\n",
      "Epoch 86/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3052 - accuracy: 0.8579 - val_loss: 0.3157 - val_accuracy: 0.8476\n",
      "Epoch 87/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3050 - accuracy: 0.8577 - val_loss: 0.3156 - val_accuracy: 0.8486\n",
      "Epoch 88/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3050 - accuracy: 0.8583 - val_loss: 0.3155 - val_accuracy: 0.8483\n",
      "Epoch 89/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3049 - accuracy: 0.8574 - val_loss: 0.3155 - val_accuracy: 0.8455\n",
      "Epoch 90/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3048 - accuracy: 0.8578 - val_loss: 0.3153 - val_accuracy: 0.8483\n",
      "Epoch 91/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3047 - accuracy: 0.8579 - val_loss: 0.3155 - val_accuracy: 0.8476\n",
      "Epoch 92/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3045 - accuracy: 0.8579 - val_loss: 0.3153 - val_accuracy: 0.8467\n",
      "Epoch 93/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3044 - accuracy: 0.8581 - val_loss: 0.3154 - val_accuracy: 0.8479\n",
      "Epoch 94/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3044 - accuracy: 0.8580 - val_loss: 0.3155 - val_accuracy: 0.8467\n",
      "Epoch 95/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3043 - accuracy: 0.8578 - val_loss: 0.3161 - val_accuracy: 0.8476\n",
      "Epoch 96/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3044 - accuracy: 0.8582 - val_loss: 0.3158 - val_accuracy: 0.8464\n",
      "Epoch 97/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3042 - accuracy: 0.8582 - val_loss: 0.3156 - val_accuracy: 0.8479\n",
      "Epoch 98/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3041 - accuracy: 0.8589 - val_loss: 0.3160 - val_accuracy: 0.8473\n",
      "Epoch 99/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3040 - accuracy: 0.8589 - val_loss: 0.3156 - val_accuracy: 0.8476\n",
      "Epoch 100/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3039 - accuracy: 0.8586 - val_loss: 0.3153 - val_accuracy: 0.8473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69f03b6a30>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we train the model\n",
    "model.fit(x=train_x, y=train_y, epochs=100, batch_size=100, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 85.52%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x=test_x, y=test_y, verbose=0)\n",
    "print('Test accuracy: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/388 [..............................] - ETA: 0s - loss: 0.6455 - accuracy: 0.7900WARNING:tensorflow:From /home/tony/anaconda3/envs/deep-learning-3.8/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0049s vs `on_train_batch_end` time: 0.0295s). Check your callbacks.\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.4544 - accuracy: 0.7929 - val_loss: 0.3814 - val_accuracy: 0.8309\n",
      "Epoch 2/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3589 - accuracy: 0.8346 - val_loss: 0.3541 - val_accuracy: 0.8340\n",
      "Epoch 3/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3432 - accuracy: 0.8400 - val_loss: 0.3450 - val_accuracy: 0.8368\n",
      "Epoch 4/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3356 - accuracy: 0.8429 - val_loss: 0.3403 - val_accuracy: 0.8417\n",
      "Epoch 5/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3306 - accuracy: 0.8460 - val_loss: 0.3358 - val_accuracy: 0.8408\n",
      "Epoch 6/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3271 - accuracy: 0.8478 - val_loss: 0.3326 - val_accuracy: 0.8408\n",
      "Epoch 7/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8489 - val_loss: 0.3298 - val_accuracy: 0.8433\n",
      "Epoch 8/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3226 - accuracy: 0.8493 - val_loss: 0.3284 - val_accuracy: 0.8436\n",
      "Epoch 9/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3210 - accuracy: 0.8503 - val_loss: 0.3271 - val_accuracy: 0.8442\n",
      "Epoch 10/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3198 - accuracy: 0.8504 - val_loss: 0.3257 - val_accuracy: 0.8436\n",
      "Epoch 11/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3189 - accuracy: 0.8506 - val_loss: 0.3248 - val_accuracy: 0.8448\n",
      "Epoch 12/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3182 - accuracy: 0.8509 - val_loss: 0.3244 - val_accuracy: 0.8473\n",
      "Epoch 13/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3175 - accuracy: 0.8516 - val_loss: 0.3233 - val_accuracy: 0.8442\n",
      "Epoch 14/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3170 - accuracy: 0.8510 - val_loss: 0.3219 - val_accuracy: 0.8461\n",
      "Epoch 15/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3164 - accuracy: 0.8516 - val_loss: 0.3229 - val_accuracy: 0.8479\n",
      "Epoch 16/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3161 - accuracy: 0.8521 - val_loss: 0.3209 - val_accuracy: 0.8461\n",
      "Epoch 17/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3157 - accuracy: 0.8517 - val_loss: 0.3210 - val_accuracy: 0.8458\n",
      "Epoch 18/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3153 - accuracy: 0.8513 - val_loss: 0.3207 - val_accuracy: 0.8464\n",
      "Epoch 19/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3151 - accuracy: 0.8521 - val_loss: 0.3204 - val_accuracy: 0.8470\n",
      "Epoch 20/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3147 - accuracy: 0.8523 - val_loss: 0.3200 - val_accuracy: 0.8464\n",
      "Epoch 21/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3144 - accuracy: 0.8527 - val_loss: 0.3193 - val_accuracy: 0.8473\n",
      "Epoch 22/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3143 - accuracy: 0.8528 - val_loss: 0.3193 - val_accuracy: 0.8476\n",
      "Epoch 23/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3139 - accuracy: 0.8532 - val_loss: 0.3195 - val_accuracy: 0.8473\n",
      "Epoch 24/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3137 - accuracy: 0.8527 - val_loss: 0.3198 - val_accuracy: 0.8476\n",
      "Epoch 25/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3136 - accuracy: 0.8529 - val_loss: 0.3186 - val_accuracy: 0.8492\n",
      "Epoch 26/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3134 - accuracy: 0.8529 - val_loss: 0.3185 - val_accuracy: 0.8498\n",
      "Epoch 27/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3132 - accuracy: 0.8528 - val_loss: 0.3187 - val_accuracy: 0.8473\n",
      "Epoch 28/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3130 - accuracy: 0.8528 - val_loss: 0.3182 - val_accuracy: 0.8486\n",
      "Epoch 29/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3128 - accuracy: 0.8537 - val_loss: 0.3181 - val_accuracy: 0.8470\n",
      "Epoch 30/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3126 - accuracy: 0.8537 - val_loss: 0.3176 - val_accuracy: 0.8495\n",
      "Epoch 31/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3125 - accuracy: 0.8538 - val_loss: 0.3177 - val_accuracy: 0.8501\n",
      "Epoch 32/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3123 - accuracy: 0.8540 - val_loss: 0.3180 - val_accuracy: 0.8455\n",
      "Epoch 33/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3120 - accuracy: 0.8541 - val_loss: 0.3175 - val_accuracy: 0.8498\n",
      "Epoch 34/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3120 - accuracy: 0.8549 - val_loss: 0.3177 - val_accuracy: 0.8492\n",
      "Epoch 35/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3118 - accuracy: 0.8541 - val_loss: 0.3178 - val_accuracy: 0.8464\n",
      "Epoch 36/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3117 - accuracy: 0.8543 - val_loss: 0.3179 - val_accuracy: 0.8464\n",
      "Epoch 37/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3115 - accuracy: 0.8540 - val_loss: 0.3177 - val_accuracy: 0.8464\n",
      "Epoch 38/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3115 - accuracy: 0.8542 - val_loss: 0.3172 - val_accuracy: 0.8483\n",
      "Epoch 39/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3112 - accuracy: 0.8544 - val_loss: 0.3173 - val_accuracy: 0.8470\n",
      "Epoch 40/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3112 - accuracy: 0.8542 - val_loss: 0.3167 - val_accuracy: 0.8483\n",
      "Epoch 41/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3110 - accuracy: 0.8544 - val_loss: 0.3171 - val_accuracy: 0.8461\n",
      "Epoch 42/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3110 - accuracy: 0.8542 - val_loss: 0.3166 - val_accuracy: 0.8492\n",
      "Epoch 43/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3108 - accuracy: 0.8543 - val_loss: 0.3172 - val_accuracy: 0.8479\n",
      "Epoch 44/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3106 - accuracy: 0.8549 - val_loss: 0.3165 - val_accuracy: 0.8483\n",
      "Epoch 45/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3106 - accuracy: 0.8541 - val_loss: 0.3169 - val_accuracy: 0.8473\n",
      "Epoch 46/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3104 - accuracy: 0.8548 - val_loss: 0.3164 - val_accuracy: 0.8479\n",
      "Epoch 47/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3104 - accuracy: 0.8544 - val_loss: 0.3165 - val_accuracy: 0.8495\n",
      "Epoch 48/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3103 - accuracy: 0.8543 - val_loss: 0.3163 - val_accuracy: 0.8483\n",
      "Epoch 49/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3101 - accuracy: 0.8547 - val_loss: 0.3167 - val_accuracy: 0.8510\n",
      "Epoch 50/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3101 - accuracy: 0.8553 - val_loss: 0.3166 - val_accuracy: 0.8492\n",
      "Epoch 51/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3099 - accuracy: 0.8549 - val_loss: 0.3162 - val_accuracy: 0.8479\n",
      "Epoch 52/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3097 - accuracy: 0.8543 - val_loss: 0.3168 - val_accuracy: 0.8476\n",
      "Epoch 53/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3096 - accuracy: 0.8553 - val_loss: 0.3162 - val_accuracy: 0.8492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3096 - accuracy: 0.8546 - val_loss: 0.3158 - val_accuracy: 0.8467\n",
      "Epoch 55/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3094 - accuracy: 0.8551 - val_loss: 0.3161 - val_accuracy: 0.8495\n",
      "Epoch 56/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3094 - accuracy: 0.8547 - val_loss: 0.3163 - val_accuracy: 0.8470\n",
      "Epoch 57/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3092 - accuracy: 0.8550 - val_loss: 0.3161 - val_accuracy: 0.8483\n",
      "Epoch 58/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3090 - accuracy: 0.8558 - val_loss: 0.3166 - val_accuracy: 0.8470\n",
      "Epoch 59/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3090 - accuracy: 0.8553 - val_loss: 0.3157 - val_accuracy: 0.8501\n",
      "Epoch 60/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3090 - accuracy: 0.8554 - val_loss: 0.3154 - val_accuracy: 0.8501\n",
      "Epoch 61/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3088 - accuracy: 0.8550 - val_loss: 0.3157 - val_accuracy: 0.8486\n",
      "Epoch 62/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3087 - accuracy: 0.8552 - val_loss: 0.3157 - val_accuracy: 0.8467\n",
      "Epoch 63/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3086 - accuracy: 0.8551 - val_loss: 0.3170 - val_accuracy: 0.8476\n",
      "Epoch 64/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3084 - accuracy: 0.8559 - val_loss: 0.3154 - val_accuracy: 0.8473\n",
      "Epoch 65/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3084 - accuracy: 0.8555 - val_loss: 0.3154 - val_accuracy: 0.8495\n",
      "Epoch 66/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3083 - accuracy: 0.8558 - val_loss: 0.3159 - val_accuracy: 0.8489\n",
      "Epoch 67/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3080 - accuracy: 0.8557 - val_loss: 0.3167 - val_accuracy: 0.8489\n",
      "Epoch 68/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3080 - accuracy: 0.8559 - val_loss: 0.3155 - val_accuracy: 0.8507\n",
      "Epoch 69/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3080 - accuracy: 0.8560 - val_loss: 0.3159 - val_accuracy: 0.8492\n",
      "Epoch 70/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3078 - accuracy: 0.8553 - val_loss: 0.3157 - val_accuracy: 0.8473\n",
      "Epoch 71/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3078 - accuracy: 0.8563 - val_loss: 0.3154 - val_accuracy: 0.8486\n",
      "Epoch 72/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3077 - accuracy: 0.8555 - val_loss: 0.3154 - val_accuracy: 0.8501\n",
      "Epoch 73/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3075 - accuracy: 0.8563 - val_loss: 0.3165 - val_accuracy: 0.8492\n",
      "Epoch 74/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3075 - accuracy: 0.8559 - val_loss: 0.3154 - val_accuracy: 0.8517\n",
      "Epoch 75/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3074 - accuracy: 0.8559 - val_loss: 0.3150 - val_accuracy: 0.8504\n",
      "Epoch 76/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3074 - accuracy: 0.8565 - val_loss: 0.3156 - val_accuracy: 0.8479\n",
      "Epoch 77/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3072 - accuracy: 0.8563 - val_loss: 0.3151 - val_accuracy: 0.8507\n",
      "Epoch 78/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3070 - accuracy: 0.8556 - val_loss: 0.3155 - val_accuracy: 0.8529\n",
      "Epoch 79/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3070 - accuracy: 0.8569 - val_loss: 0.3165 - val_accuracy: 0.8486\n",
      "Epoch 80/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.8571 - val_loss: 0.3158 - val_accuracy: 0.8498\n",
      "Epoch 81/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3069 - accuracy: 0.8564 - val_loss: 0.3152 - val_accuracy: 0.8513\n",
      "Epoch 82/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3067 - accuracy: 0.8564 - val_loss: 0.3161 - val_accuracy: 0.8483\n",
      "Epoch 83/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3068 - accuracy: 0.8565 - val_loss: 0.3155 - val_accuracy: 0.8495\n",
      "Epoch 84/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8571 - val_loss: 0.3153 - val_accuracy: 0.8504\n",
      "Epoch 85/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3066 - accuracy: 0.8564 - val_loss: 0.3160 - val_accuracy: 0.8492\n",
      "Epoch 86/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.8566 - val_loss: 0.3156 - val_accuracy: 0.8486\n",
      "Epoch 87/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3063 - accuracy: 0.8567 - val_loss: 0.3156 - val_accuracy: 0.8486\n",
      "Epoch 88/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3063 - accuracy: 0.8579 - val_loss: 0.3156 - val_accuracy: 0.8495\n",
      "Epoch 89/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3062 - accuracy: 0.8570 - val_loss: 0.3157 - val_accuracy: 0.8476\n",
      "Epoch 90/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3060 - accuracy: 0.8574 - val_loss: 0.3153 - val_accuracy: 0.8507\n",
      "Epoch 91/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3060 - accuracy: 0.8574 - val_loss: 0.3156 - val_accuracy: 0.8532\n",
      "Epoch 92/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3058 - accuracy: 0.8565 - val_loss: 0.3155 - val_accuracy: 0.8492\n",
      "Epoch 93/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3058 - accuracy: 0.8572 - val_loss: 0.3156 - val_accuracy: 0.8504\n",
      "Epoch 94/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3057 - accuracy: 0.8568 - val_loss: 0.3155 - val_accuracy: 0.8489\n",
      "Epoch 95/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3056 - accuracy: 0.8567 - val_loss: 0.3163 - val_accuracy: 0.8498\n",
      "Epoch 96/100\n",
      "388/388 [==============================] - 1s 1ms/step - loss: 0.3056 - accuracy: 0.8573 - val_loss: 0.3157 - val_accuracy: 0.8495\n",
      "Epoch 97/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3055 - accuracy: 0.8575 - val_loss: 0.3157 - val_accuracy: 0.8495\n",
      "Epoch 98/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3054 - accuracy: 0.8569 - val_loss: 0.3162 - val_accuracy: 0.8492\n",
      "Epoch 99/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3052 - accuracy: 0.8575 - val_loss: 0.3156 - val_accuracy: 0.8517\n",
      "Epoch 100/100\n",
      "388/388 [==============================] - 1s 2ms/step - loss: 0.3052 - accuracy: 0.8580 - val_loss: 0.3153 - val_accuracy: 0.8513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f69f02ef5b0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model_adult(train_x.shape)\n",
    "#model.summary()\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# set up tensorboard log directory and callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=100, batch_size=100, validation_data=(val_x, val_y), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14903), started 0:47:30 ago. (Use '!kill 14903' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2b44d49e057554fe\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2b44d49e057554fe\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start tensorboard (notebook experience)\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
