{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Discovering Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Python version: 3.8.3 (default, Jul  2 2020, 11:26:31) \n",
      "[Clang 10.0.0 ]\n",
      "### Numpy version: 1.19.5\n",
      "### Scikit-learn version: 0.23.1\n",
      "### Tensorflow version: 2.4.0\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we'll use keras for neural networks\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + sys.version)\n",
    "print('### Numpy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "\n",
    "seed = 42 # deterministic seed\n",
    "np.random.seed(seed) \n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to think of Tensorflow? Is it like scikit-learn but for neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really, think of Tensorflow as a kind of NumPy with additional features (i.e., ability to create computational graphs on tensors, automatically compute derivative, run operations on GPUs). (Tensorflow also has many high-level APIs.)\n",
    "\n",
    "### What are tensors? Well formally they are multilinear maps from vector spaces to reals; but that doesn't matter the point is that tensors can represent scalars, vectors, matrices, etc.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beware that Tensorflow 2.0 is different from Tensorflow 1.0! In this course we'll use Tensorflow 2.0.\n",
    "\n",
    "### Compared to TF 1.0:\n",
    "### - TF 2.0 incorporates Keras as a high-level API\n",
    "### - TF 2.0 does *eager* execution by default!\n",
    "#### In TF 1.0 you would first build the computational graph (construction phase) and then you would execute it in a session (execution phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we set the seed for Tensorflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get familiar with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "scalar = 7 # a scalar in Python\n",
    "\n",
    "scalar_tf = tf.constant(7) # a TF scalar\n",
    "\n",
    "print(scalar)\n",
    "print(scalar_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like numpy array, tensors have a shape and dtype property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 -5  9  1]\n",
      "tf.Tensor([ 3 -5  9  1], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vector_np = np.array([3, -5, 9, 1])\n",
    "print(vector_np)\n",
    "\n",
    "vector_tf = tf.constant([3, -5, 9, 1])\n",
    "print(vector_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can get the dtype, shape of tensor. We can also get at the underlying numpy array using numpy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (4,)\n",
      "numpy array: [ 3 -5  9  1], type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('dtype: ' + str(vector_tf.dtype))\n",
    "print('shape: ' + str(vector_tf.shape))\n",
    "\n",
    "numpy_arr = vector_tf.numpy()\n",
    "print('numpy array: {}, type: {}'.format(str(numpy_arr), type(numpy_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 3 -7]\n",
      " [ 0  9]], shape=(2, 2), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# we can also build a tensor out of a numpy array\n",
    "matrix_np = np.array([[3, -7], [0, 9]])\n",
    "matrix_tf = tf.constant(matrix_np)\n",
    "\n",
    "print(matrix_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[0.6645621  0.44100678 0.3528825  0.46448255]\n",
      " [0.03366041 0.68467236 0.74011743 0.8724445 ]], shape=(2, 4), dtype=float32)\n",
      "\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# We can construct tensors in similar ways to how we construct some numpy arrays. For example:\n",
    "\n",
    "tf_ones = tf.ones((3,3))\n",
    "print(tf_ones)\n",
    "print()\n",
    "\n",
    "# and\n",
    "\n",
    "tf_unifrand = tf.random.uniform((2, 4))\n",
    "print(tf_unifrand)\n",
    "print()\n",
    "\n",
    "tf_zeros_like_ones = tf.zeros_like(tf_ones)\n",
    "print(tf_zeros_like_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check if something is a Tensor. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.is_tensor(matrix_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.is_tensor(matrix_tf.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also place tensors onto devices. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/job:localhost/replica:0/task:0/device:CPU:0\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    matrix_on_gpu0 = tf.identity(matrix_tf) # won't work if you don't have a GPU\n",
    "    \n",
    "print(matrix_on_gpu0.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs Available:  1\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do operations as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 5], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3])\n",
    "y = tf.constant([-1, 2])\n",
    "\n",
    "add_x_y = tf.add(x, y)\n",
    "print(add_x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 5], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Can we do x + y?\n",
    "x_plus_y = x + y\n",
    "print(x_plus_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1 -3], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# multiplication by a scalar\n",
    "x_mult_mone = x * -1\n",
    "print(x_mult_mone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([-1  6], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# elementwise multiplication\n",
    "x_mult_y = x * y\n",
    "# or: x_mult_y = tf.multiply(x,y)\n",
    "print(x_mult_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what about matrix multiplication and similar ops?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2,)\n",
      "(3, 2)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "A = tf.constant([[1, 0, 3], [0, -2, 5]])\n",
    "B = tf.constant([2, -3])\n",
    "\n",
    "print(A.shape)\n",
    "print(B.shape)\n",
    "\n",
    "A_transposed = tf.transpose(A)\n",
    "print(A_transposed.shape)\n",
    "\n",
    "B_reshaped = tf.reshape(B, (-1, 1))\n",
    "\n",
    "print(B_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2]\n",
      " [ 6]\n",
      " [-9]], shape=(3, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "A_T_matrix_mult_B = tf.linalg.matmul(A_transposed, B_reshaped)\n",
    "# or A_transposed @ B_reshaped\n",
    "\n",
    "print(A_T_matrix_mult_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because tensors are immutable, we cannot change their values in place. This seems like it could be a problem because parameters of a model are variables whose values should change frequently.\n",
    "### For this we can use: tf.Variable\n",
    "\n",
    "#### We'll typically use those for model parameters and other variables that need to change often in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's declare a variable\n",
    "# variables in TF represent tensors and you change their values by running operations (ops) on them\n",
    "x = tf.Variable([7, 3], name=\"x\")   # we can name variables (we don't have to, but we can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=(2,) dtype=int32, numpy=array([7, 3], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,) <dtype: 'int32'> x:0\n"
     ]
    }
   ],
   "source": [
    "# Variables also have shape and dtype, etc.\n",
    "print(x.shape, x.dtype, x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([49  9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# if you do ops on a variable the result is a tensor not a variable!\n",
    "xsquared = tf.square(x)\n",
    "print(xsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'x:0' shape=(2,) dtype=int32, numpy=array([-1,  0], dtype=int32)>\n",
      "<tf.Variable 'x:0' shape=(2,) dtype=int32, numpy=array([2, 3], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# but variables unlike constant can have their values changed in-place (e.g., using one of the assign*() methods). \n",
    "# For example:\n",
    "x.assign(tf.constant([-1, 0]))\n",
    "print(x)\n",
    "\n",
    "x.assign_add(tf.constant([3, 3]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot assign to variable x:0 due to variable shape (2,) and value shape (3,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7c467cd52e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# However, shapes must be compatible!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m17\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0mtensor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    889\u001b[0m             (\"Cannot assign to variable%s due to variable shape %s and value \"\n\u001b[1;32m    890\u001b[0m              \"shape %s are incompatible\") %\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot assign to variable x:0 due to variable shape (2,) and value shape (3,) are incompatible"
     ]
    }
   ],
   "source": [
    "# However, shapes must be compatible!\n",
    "x.assign(tf.constant([5, 9, -17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool (and important) feature: automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2, name=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we want to compute the derivative of x ** 3. Clearly it's 3 x ** 2\n",
    "### We can do it using tf.GradientTape to keep track of the operations on tensor and then compute the gradient afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.0, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "# Note: to watch a tensor it must be floating point, so we'll cast x\n",
    "x = tf.cast(x, dtype=tf.float16)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x) # we tell the tape to watch variable 'x'\n",
    "    # now we can do operations like x ** 3\n",
    "    y = x ** 3\n",
    "    \n",
    "    \n",
    "## What is y?\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is the gradient of y wrt x?\n",
    "# we want the gradient of y (x**3) with respect to x\n",
    "grad_xcube = tape.gradient(target=y, sources=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(12.0, shape=(), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "print(grad_xcube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "print((3 * x**2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: once we get the gradients from the tape, the resources are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7920cd642f48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This will cause an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrad_xcube2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msources\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \"\"\"\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to\"\n\u001b[0m\u001b[1;32m   1028\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1029\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "# This will cause an error\n",
    "grad_xcube2 = tape.gradient(target=y, sources=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we can create a persistent tape if we want. For example (a bit more complicated example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.         4.8520303  7.690286   9.704061  11.266066 ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_np = np.array([1, 2, 3, 4, 5])\n",
    "x = tf.Variable(x_np, name=\"x\", dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "    # watch_accessed_variables=True allows us to not have to set each variable we want to watch\n",
    "    \n",
    "    z = tf.constant(7, dtype=tf.float32)\n",
    "    #z = tf.Variable([7, 7, 7, 7, 7], dtype=tf.float32, name='z')\n",
    "    \n",
    "    y = z * tf.math.log(x)\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7.        3.5       2.3333335 1.75      1.4      ], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "grad_y_wrt_x = tape.gradient(target=y, sources=x)\n",
    "print(grad_y_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y_wrt_x2 = tape.gradient(target=y, sources=x) # we can grab it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# we can even grab the gradient with respect to something else (e.g., z)\n",
    "grad_y_wrt_z = tape.gradient(target=y, sources=z)\n",
    "print(grad_y_wrt_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So this is nice but what can we do with it? Let's train linear regression model with Tensorflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this, we'll create some simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 1), dtype=float32, numpy=\n",
       "array([[-1.],\n",
       "       [ 5.],\n",
       "       [ 2.],\n",
       "       [-7.],\n",
       "       [ 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First make up a model\n",
    "true_theta = tf.constant([-1, 5, 2, -7, 3], dtype=tf.float32)[:, tf.newaxis]\n",
    "true_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1500\n",
    "ntr = 1000\n",
    "\n",
    "# make some random data\n",
    "x = tf.constant(tf.random.uniform((n, 5), minval=-1, maxval=+1), dtype=tf.float32)\n",
    "\n",
    "# now calculate the y based on the true parameters\n",
    "y = tf.constant(x @ true_theta, dtype=tf.float32)\n",
    "\n",
    "# split the data\n",
    "train_x = x[:ntr,:]\n",
    "train_y = y[:ntr]\n",
    "\n",
    "val_x = x[ntr:,:].numpy()\n",
    "val_y = y[ntr:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is batch gradient descent\n",
    "def train_lr_tf(x, y, eta=0.05, num_iter=250, verbose=False):\n",
    "    \n",
    "    n, m = x.shape\n",
    "    \n",
    "    # weights / parameters (randomly initialized)\n",
    "    theta = tf.Variable(tf.random.uniform((m, 1), minval=-1, maxval=1), dtype=tf.float32)\n",
    "        \n",
    "    for i in range(0, num_iter):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = tf.linalg.matmul(x, theta) # prediction\n",
    "            mse = tf.reduce_mean(tf.square(y - y_pred)) \n",
    "        \n",
    "        # extract the gradients \n",
    "        gradient_vec = tape.gradient(mse, theta)\n",
    "\n",
    "        # do a gradient descent step (we use assign_sub() to update theta in place)\n",
    "        theta.assign_sub(tf.constant([eta], dtype=tf.float32) * gradient_vec) \n",
    "\n",
    "\n",
    "        if verbose and i % int(num_iter/10) == 0:\n",
    "            print('Iteration {}: the (training) loss (MSE) is {:.5f}'.format(i, mse))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: the (training) loss (MSE) is 28.77184\n",
      "Iteration 25: the (training) loss (MSE) is 5.35863\n",
      "Iteration 50: the (training) loss (MSE) is 1.00340\n",
      "Iteration 75: the (training) loss (MSE) is 0.18878\n",
      "Iteration 100: the (training) loss (MSE) is 0.03567\n",
      "Iteration 125: the (training) loss (MSE) is 0.00676\n",
      "Iteration 150: the (training) loss (MSE) is 0.00129\n",
      "Iteration 175: the (training) loss (MSE) is 0.00025\n",
      "Iteration 200: the (training) loss (MSE) is 0.00005\n",
      "Iteration 225: the (training) loss (MSE) is 0.00001\n"
     ]
    }
   ],
   "source": [
    "# Let's do the training\n",
    "theta = train_lr_tf(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(5, 1) dtype=float32, numpy=\n",
      "array([[-0.99959064],\n",
      "       [ 4.998418  ],\n",
      "       [ 1.9997115 ],\n",
      "       [-6.998523  ],\n",
      "       [ 2.9993668 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TF-GD Train] R^2: 1.00, RMSE: 0.00, MedAE: 0.00\n",
      "[TF-GD Val] R^2: 1.00, RMSE: 0.00, MedAE: 0.00\n"
     ]
    }
   ],
   "source": [
    "# given model parameters 'theta' and a feature matrix 'x', this will return predictions\n",
    "def predict_theta(theta, x):\n",
    "    return np.dot(x, theta) # note: there is no bias 'b' in this case\n",
    "    \n",
    "from sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\n",
    "\n",
    "def print_scores(desc, true_y, pred_y):\n",
    "    r2 = r2_score(true_y, pred_y)\n",
    "    rmse = mean_squared_error(true_y, pred_y, squared=False)\n",
    "    medae = median_absolute_error(true_y, pred_y)\n",
    "    \n",
    "    print('[{}] R^2: {:.2f}, RMSE: {:.2f}, MedAE: {:.2f}'.format(desc, r2, rmse, medae))\n",
    "        \n",
    "print_scores('TF-GD Train', train_y, predict_theta(theta.numpy(), train_x))\n",
    "print_scores('TF-GD Val', val_y, predict_theta(theta.numpy(), val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is nice but it seems tedious. Do we have to implement the gradient descent ourselves and do all the low-level stuff?\n",
    "### => No, we can use a higher-level API like Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to define the architecture\n",
    "def create_model(input_shape, num_outputs=1):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # declare input layer (keras needs to know the number of input features to expect)\n",
    "    model.add(keras.Input(shape=(input_shape[1],))) \n",
    "    \n",
    "    # next add our output layer (1 output, linear activation function)\n",
    "    model.add(keras.layers.Dense(num_outputs, activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 6\n",
      "Trainable params: 6\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# first we create the model (i.e., define the architecture)\n",
    "model = create_model(train_x.shape)\n",
    "\n",
    "# Tip: before you go on, use summary() to check that the architecture is what you intended\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we compile it to specify optimizer, loss, and metrics\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 28.9469 - mae: 4.3688 - val_loss: 25.0808 - val_mae: 4.1231\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 21.9992 - mae: 3.8873 - val_loss: 19.4303 - val_mae: 3.6274\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 17.2052 - mae: 3.3912 - val_loss: 15.0541 - val_mae: 3.1917\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 13.3655 - mae: 2.9891 - val_loss: 11.6641 - val_mae: 2.8084\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 10.0640 - mae: 2.5997 - val_loss: 9.0377 - val_mae: 2.4714\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.9006 - mae: 2.3143 - val_loss: 7.0027 - val_mae: 2.1747\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.1289 - mae: 2.0655 - val_loss: 5.4266 - val_mae: 1.9139\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.7362 - mae: 1.7765 - val_loss: 4.2052 - val_mae: 1.6845\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.6036 - mae: 1.5419 - val_loss: 3.2589 - val_mae: 1.4825\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 2.7271 - mae: 1.3501 - val_loss: 2.5257 - val_mae: 1.3048\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 2.1671 - mae: 1.2089 - val_loss: 1.9576 - val_mae: 1.1485\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7080 - mae: 1.0691 - val_loss: 1.5172 - val_mae: 1.0109\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2113 - mae: 0.9071 - val_loss: 1.1760 - val_mae: 0.8899\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.9838 - mae: 0.8155 - val_loss: 0.9116 - val_mae: 0.7834\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.8397 - mae: 0.7502 - val_loss: 0.7066 - val_mae: 0.6897\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.6325 - mae: 0.6510 - val_loss: 0.5477 - val_mae: 0.6072\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.4676 - mae: 0.5647 - val_loss: 0.4246 - val_mae: 0.5346\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3599 - mae: 0.4986 - val_loss: 0.3292 - val_mae: 0.4707\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2948 - mae: 0.4442 - val_loss: 0.2552 - val_mae: 0.4144\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.2271 - mae: 0.3864 - val_loss: 0.1978 - val_mae: 0.3649\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1644 - mae: 0.3297 - val_loss: 0.1534 - val_mae: 0.3212\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1412 - mae: 0.3091 - val_loss: 0.1189 - val_mae: 0.2829\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0999 - mae: 0.2594 - val_loss: 0.0922 - val_mae: 0.2491\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0828 - mae: 0.2368 - val_loss: 0.0715 - val_mae: 0.2193\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0583 - mae: 0.1987 - val_loss: 0.0554 - val_mae: 0.1931\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0491 - mae: 0.1823 - val_loss: 0.0430 - val_mae: 0.1701\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0378 - mae: 0.1588 - val_loss: 0.0333 - val_mae: 0.1498\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0296 - mae: 0.1412 - val_loss: 0.0259 - val_mae: 0.1319\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0216 - mae: 0.1183 - val_loss: 0.0201 - val_mae: 0.1161\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0172 - mae: 0.1072 - val_loss: 0.0156 - val_mae: 0.1023\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0136 - mae: 0.0950 - val_loss: 0.0121 - val_mae: 0.0901\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0106 - mae: 0.0842 - val_loss: 0.0094 - val_mae: 0.0793\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0079 - mae: 0.0721 - val_loss: 0.0073 - val_mae: 0.0699\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0064 - mae: 0.0649 - val_loss: 0.0056 - val_mae: 0.0615\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0049 - mae: 0.0566 - val_loss: 0.0044 - val_mae: 0.0542\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0036 - mae: 0.0490 - val_loss: 0.0034 - val_mae: 0.0477\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0030 - mae: 0.0445 - val_loss: 0.0026 - val_mae: 0.0420\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0391 - val_loss: 0.0020 - val_mae: 0.0370\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0340 - val_loss: 0.0016 - val_mae: 0.0326\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0295 - val_loss: 0.0012 - val_mae: 0.0287\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0264 - val_loss: 9.5095e-04 - val_mae: 0.0253\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.2888e-04 - mae: 0.0236 - val_loss: 7.3773e-04 - val_mae: 0.0223\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.2952e-04 - mae: 0.0205 - val_loss: 5.7226e-04 - val_mae: 0.0196\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.6627e-04 - mae: 0.0177 - val_loss: 4.4399e-04 - val_mae: 0.0173\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.5872e-04 - mae: 0.0154 - val_loss: 3.4448e-04 - val_mae: 0.0152\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.8769e-04 - mae: 0.0138 - val_loss: 2.6725e-04 - val_mae: 0.0134\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.3814e-04 - mae: 0.0124 - val_loss: 2.0733e-04 - val_mae: 0.0118\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7808e-04 - mae: 0.0108 - val_loss: 1.6087e-04 - val_mae: 0.0104\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.3765e-04 - mae: 0.0096 - val_loss: 1.2482e-04 - val_mae: 0.0092\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1211e-04 - mae: 0.0087 - val_loss: 9.6854e-05 - val_mae: 0.0081\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.1970e-05 - mae: 0.0074 - val_loss: 7.5153e-05 - val_mae: 0.0071\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.7768e-05 - mae: 0.0067 - val_loss: 5.8307e-05 - val_mae: 0.0063\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.1294e-05 - mae: 0.0058 - val_loss: 4.5240e-05 - val_mae: 0.0055\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.1494e-05 - mae: 0.0052 - val_loss: 3.5101e-05 - val_mae: 0.0049\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.0952e-05 - mae: 0.0045 - val_loss: 2.7239e-05 - val_mae: 0.0043\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.4548e-05 - mae: 0.0040 - val_loss: 2.1139e-05 - val_mae: 0.0038\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.8532e-05 - mae: 0.0036 - val_loss: 1.6407e-05 - val_mae: 0.0033\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4951e-05 - mae: 0.0031 - val_loss: 1.2732e-05 - val_mae: 0.0029\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0613e-05 - mae: 0.0026 - val_loss: 9.8833e-06 - val_mae: 0.0026\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.8454e-06 - mae: 0.0024 - val_loss: 7.6725e-06 - val_mae: 0.0023\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4282e-06 - mae: 0.0021 - val_loss: 5.9573e-06 - val_mae: 0.0020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.4078e-06 - mae: 0.0019 - val_loss: 4.6222e-06 - val_mae: 0.0018\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.1588e-06 - mae: 0.0017 - val_loss: 3.5877e-06 - val_mae: 0.0016\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.0924e-06 - mae: 0.0014 - val_loss: 2.7845e-06 - val_mae: 0.0014\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.5827e-06 - mae: 0.0013 - val_loss: 2.1601e-06 - val_mae: 0.0012\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.9388e-06 - mae: 0.0011 - val_loss: 1.6769e-06 - val_mae: 0.0011\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.5016e-06 - mae: 9.9312e-04 - val_loss: 1.3004e-06 - val_mae: 9.3603e-04\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1370e-06 - mae: 8.7510e-04 - val_loss: 1.0105e-06 - val_mae: 8.2512e-04\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.7183e-07 - mae: 7.5306e-04 - val_loss: 7.8427e-07 - val_mae: 7.2694e-04\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5013e-07 - mae: 6.4946e-04 - val_loss: 6.0993e-07 - val_mae: 6.4105e-04\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.1613e-07 - mae: 5.8838e-04 - val_loss: 4.7345e-07 - val_mae: 5.6477e-04\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.0555e-07 - mae: 5.1236e-04 - val_loss: 3.6794e-07 - val_mae: 4.9788e-04\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.1536e-07 - mae: 4.5443e-04 - val_loss: 2.8493e-07 - val_mae: 4.3814e-04\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.4536e-07 - mae: 4.0104e-04 - val_loss: 2.2084e-07 - val_mae: 3.8574e-04\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.8627e-07 - mae: 3.5389e-04 - val_loss: 1.7143e-07 - val_mae: 3.3985e-04\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.4977e-07 - mae: 3.1522e-04 - val_loss: 1.3317e-07 - val_mae: 2.9951e-04\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1442e-07 - mae: 2.7711e-04 - val_loss: 1.0338e-07 - val_mae: 2.6389e-04\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 9.0010e-08 - mae: 2.4182e-04 - val_loss: 8.0052e-08 - val_mae: 2.3223e-04\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.1325e-08 - mae: 2.1841e-04 - val_loss: 6.2059e-08 - val_mae: 2.0448e-04\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.5754e-08 - mae: 1.9047e-04 - val_loss: 4.8460e-08 - val_mae: 1.8068e-04\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.1141e-08 - mae: 1.6409e-04 - val_loss: 3.7789e-08 - val_mae: 1.5955e-04\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.1434e-08 - mae: 1.4314e-04 - val_loss: 2.9356e-08 - val_mae: 1.4062e-04\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.5766e-08 - mae: 1.3113e-04 - val_loss: 2.2817e-08 - val_mae: 1.2399e-04\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.9327e-08 - mae: 1.1170e-04 - val_loss: 1.7682e-08 - val_mae: 1.0915e-04\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.6655e-08 - mae: 1.0449e-04 - val_loss: 1.3711e-08 - val_mae: 9.6116e-05\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.1746e-08 - mae: 8.8082e-05 - val_loss: 1.0568e-08 - val_mae: 8.4394e-05\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 9.0897e-09 - mae: 7.8337e-05 - val_loss: 8.2040e-09 - val_mae: 7.4355e-05\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.6597e-09 - mae: 7.1343e-05 - val_loss: 6.4773e-09 - val_mae: 6.6066e-05\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.8869e-09 - mae: 6.2429e-05 - val_loss: 5.0150e-09 - val_mae: 5.8133e-05\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 4.3148e-09 - mae: 5.3237e-05 - val_loss: 3.8480e-09 - val_mae: 5.0928e-05\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 3.2260e-09 - mae: 4.6106e-05 - val_loss: 2.8578e-09 - val_mae: 4.3893e-05\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 2.4178e-09 - mae: 3.9551e-05 - val_loss: 2.0887e-09 - val_mae: 3.7542e-05\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.7368e-09 - mae: 3.3204e-05 - val_loss: 1.5207e-09 - val_mae: 3.2042e-05\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.2335e-09 - mae: 2.8625e-05 - val_loss: 1.1793e-09 - val_mae: 2.8211e-05\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 1.0525e-09 - mae: 2.6448e-05 - val_loss: 9.4848e-10 - val_mae: 2.5299e-05\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 8.7853e-10 - mae: 2.4001e-05 - val_loss: 8.4076e-10 - val_mae: 2.3788e-05\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.7016e-10 - mae: 2.2504e-05 - val_loss: 7.6890e-10 - val_mae: 2.2728e-05\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.6702e-10 - mae: 2.0992e-05 - val_loss: 6.9337e-10 - val_mae: 2.1569e-05\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3940e-10 - mae: 2.0427e-05 - val_loss: 6.5124e-10 - val_mae: 2.0888e-05\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 5.6062e-10 - mae: 1.9154e-05 - val_loss: 6.1400e-10 - val_mae: 2.0269e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f101c0b80d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finally, we train the model\n",
    "model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we extract the parameters?\n",
    "def extract_weights(model):\n",
    "    for layer in model.layers:\n",
    "        return layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the weights? Are they similar as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.9999955],\n",
      "       [ 4.999971 ],\n",
      "       [ 1.9999985],\n",
      "       [-6.999973 ],\n",
      "       [ 2.9999857]], dtype=float32), array([2.4162577e-07], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "weights = extract_weights(model)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a more complex problem with a more complex neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use the Adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (38762, 88), (38762,)\n",
      "Test: (3231, 88), (3231,)\n",
      "Validation: (3229, 88), (3229,)\n"
     ]
    }
   ],
   "source": [
    "### In this case, we'll directly load the Adult dataset pre-processed in a similar way as for assignment 1\n",
    "### and we'll immediately split it into train, test, validation.\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, features, labels = utils.load_preproc_adult(prop_vec=prop_vec, seed=seed)\n",
    "\n",
    "# check that we have what we expect\n",
    "print('Training: {}, {}'.format(train_x.shape, train_y.shape))\n",
    "print('Test: {}, {}'.format(test_x.shape, test_y.shape))\n",
    "print('Validation: {}, {}'.format(val_x.shape, val_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In assignment 2 we had found the best model was a SVM classifier which achieved around 85% accuracy. Can we do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to define the architecture\n",
    "def create_model_adult(input_shape, hidden_widths=[96, 32], num_outputs=1):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # declare input layer (keras needs to know the number of input features to expect)\n",
    "    model.add(keras.Input(shape=(input_shape[1],))) \n",
    "    \n",
    "    # add two hidden layers with ReLU activation\n",
    "    model.add(keras.layers.Dense(hidden_widths[0], activation='relu'))\n",
    "    model.add(keras.layers.Dense(hidden_widths[1], activation='relu'))\n",
    "    \n",
    "    # next add our output layer (binary classification with 1 output, so sigmoid makes the most sense)\n",
    "    model.add(keras.layers.Dense(num_outputs, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 96)                8544      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                3104      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 11,681\n",
      "Trainable params: 11,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model (i.e., define the architecture)\n",
    "model = create_model_adult(train_x.shape)\n",
    "\n",
    "# Tip: before you go on, use summary() to check that the architecture is what you intended\n",
    "model.summary()\n",
    "\n",
    "# then we compile it to specify optimizer, loss, and metrics\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "388/388 [==============================] - 1s 972us/step - loss: 0.5524 - accuracy: 0.7366 - val_loss: 0.3913 - val_accuracy: 0.8210\n",
      "Epoch 2/100\n",
      "388/388 [==============================] - 0s 764us/step - loss: 0.3764 - accuracy: 0.8260 - val_loss: 0.3540 - val_accuracy: 0.8346\n",
      "Epoch 3/100\n",
      "388/388 [==============================] - 0s 803us/step - loss: 0.3489 - accuracy: 0.8360 - val_loss: 0.3435 - val_accuracy: 0.8374\n",
      "Epoch 4/100\n",
      "388/388 [==============================] - 0s 887us/step - loss: 0.3410 - accuracy: 0.8394 - val_loss: 0.3377 - val_accuracy: 0.8421\n",
      "Epoch 5/100\n",
      "388/388 [==============================] - 0s 795us/step - loss: 0.3359 - accuracy: 0.8411 - val_loss: 0.3332 - val_accuracy: 0.8421\n",
      "Epoch 6/100\n",
      "388/388 [==============================] - 0s 900us/step - loss: 0.3297 - accuracy: 0.8457 - val_loss: 0.3300 - val_accuracy: 0.8421\n",
      "Epoch 7/100\n",
      "388/388 [==============================] - 0s 910us/step - loss: 0.3219 - accuracy: 0.8527 - val_loss: 0.3274 - val_accuracy: 0.8442\n",
      "Epoch 8/100\n",
      "388/388 [==============================] - 0s 862us/step - loss: 0.3198 - accuracy: 0.8525 - val_loss: 0.3261 - val_accuracy: 0.8439\n",
      "Epoch 9/100\n",
      "388/388 [==============================] - 0s 773us/step - loss: 0.3215 - accuracy: 0.8509 - val_loss: 0.3248 - val_accuracy: 0.8445\n",
      "Epoch 10/100\n",
      "388/388 [==============================] - 0s 781us/step - loss: 0.3161 - accuracy: 0.8522 - val_loss: 0.3237 - val_accuracy: 0.8430\n",
      "Epoch 11/100\n",
      "388/388 [==============================] - 0s 813us/step - loss: 0.3168 - accuracy: 0.8530 - val_loss: 0.3229 - val_accuracy: 0.8430\n",
      "Epoch 12/100\n",
      "388/388 [==============================] - 0s 804us/step - loss: 0.3174 - accuracy: 0.8507 - val_loss: 0.3224 - val_accuracy: 0.8430\n",
      "Epoch 13/100\n",
      "388/388 [==============================] - 0s 800us/step - loss: 0.3152 - accuracy: 0.8540 - val_loss: 0.3214 - val_accuracy: 0.8436\n",
      "Epoch 14/100\n",
      "388/388 [==============================] - 0s 803us/step - loss: 0.3152 - accuracy: 0.8527 - val_loss: 0.3202 - val_accuracy: 0.8445\n",
      "Epoch 15/100\n",
      "388/388 [==============================] - 0s 813us/step - loss: 0.3207 - accuracy: 0.8500 - val_loss: 0.3211 - val_accuracy: 0.8458\n",
      "Epoch 16/100\n",
      "388/388 [==============================] - 0s 841us/step - loss: 0.3150 - accuracy: 0.8537 - val_loss: 0.3193 - val_accuracy: 0.8452\n",
      "Epoch 17/100\n",
      "388/388 [==============================] - 0s 861us/step - loss: 0.3159 - accuracy: 0.8531 - val_loss: 0.3193 - val_accuracy: 0.8455\n",
      "Epoch 18/100\n",
      "388/388 [==============================] - 0s 865us/step - loss: 0.3112 - accuracy: 0.8527 - val_loss: 0.3189 - val_accuracy: 0.8452\n",
      "Epoch 19/100\n",
      "388/388 [==============================] - 0s 780us/step - loss: 0.3163 - accuracy: 0.8518 - val_loss: 0.3188 - val_accuracy: 0.8439\n",
      "Epoch 20/100\n",
      "388/388 [==============================] - 0s 789us/step - loss: 0.3147 - accuracy: 0.8524 - val_loss: 0.3184 - val_accuracy: 0.8461\n",
      "Epoch 21/100\n",
      "388/388 [==============================] - 0s 901us/step - loss: 0.3133 - accuracy: 0.8533 - val_loss: 0.3177 - val_accuracy: 0.8470\n",
      "Epoch 22/100\n",
      "388/388 [==============================] - 0s 845us/step - loss: 0.3162 - accuracy: 0.8521 - val_loss: 0.3178 - val_accuracy: 0.8464\n",
      "Epoch 23/100\n",
      "388/388 [==============================] - 0s 866us/step - loss: 0.3134 - accuracy: 0.8553 - val_loss: 0.3180 - val_accuracy: 0.8473\n",
      "Epoch 24/100\n",
      "388/388 [==============================] - 0s 824us/step - loss: 0.3106 - accuracy: 0.8556 - val_loss: 0.3183 - val_accuracy: 0.8467\n",
      "Epoch 25/100\n",
      "388/388 [==============================] - 0s 811us/step - loss: 0.3112 - accuracy: 0.8535 - val_loss: 0.3171 - val_accuracy: 0.8476\n",
      "Epoch 26/100\n",
      "388/388 [==============================] - 0s 877us/step - loss: 0.3138 - accuracy: 0.8549 - val_loss: 0.3168 - val_accuracy: 0.8473\n",
      "Epoch 27/100\n",
      "388/388 [==============================] - 0s 804us/step - loss: 0.3144 - accuracy: 0.8549 - val_loss: 0.3170 - val_accuracy: 0.8467\n",
      "Epoch 28/100\n",
      "388/388 [==============================] - 0s 821us/step - loss: 0.3160 - accuracy: 0.8512 - val_loss: 0.3166 - val_accuracy: 0.8483\n",
      "Epoch 29/100\n",
      "388/388 [==============================] - 0s 881us/step - loss: 0.3122 - accuracy: 0.8562 - val_loss: 0.3167 - val_accuracy: 0.8483\n",
      "Epoch 30/100\n",
      "388/388 [==============================] - 0s 787us/step - loss: 0.3097 - accuracy: 0.8586 - val_loss: 0.3161 - val_accuracy: 0.8479\n",
      "Epoch 31/100\n",
      "388/388 [==============================] - 0s 810us/step - loss: 0.3082 - accuracy: 0.8568 - val_loss: 0.3162 - val_accuracy: 0.8461\n",
      "Epoch 32/100\n",
      "388/388 [==============================] - 0s 903us/step - loss: 0.3160 - accuracy: 0.8533 - val_loss: 0.3166 - val_accuracy: 0.8473\n",
      "Epoch 33/100\n",
      "388/388 [==============================] - 0s 837us/step - loss: 0.3100 - accuracy: 0.8552 - val_loss: 0.3161 - val_accuracy: 0.8467\n",
      "Epoch 34/100\n",
      "388/388 [==============================] - 0s 828us/step - loss: 0.3121 - accuracy: 0.8547 - val_loss: 0.3163 - val_accuracy: 0.8489\n",
      "Epoch 35/100\n",
      "388/388 [==============================] - 0s 831us/step - loss: 0.3070 - accuracy: 0.8563 - val_loss: 0.3164 - val_accuracy: 0.8467\n",
      "Epoch 36/100\n",
      "388/388 [==============================] - 0s 877us/step - loss: 0.3107 - accuracy: 0.8546 - val_loss: 0.3166 - val_accuracy: 0.8458\n",
      "Epoch 37/100\n",
      "388/388 [==============================] - 0s 845us/step - loss: 0.3110 - accuracy: 0.8543 - val_loss: 0.3166 - val_accuracy: 0.8455\n",
      "Epoch 38/100\n",
      "388/388 [==============================] - 0s 823us/step - loss: 0.3120 - accuracy: 0.8549 - val_loss: 0.3158 - val_accuracy: 0.8489\n",
      "Epoch 39/100\n",
      "388/388 [==============================] - 0s 861us/step - loss: 0.3098 - accuracy: 0.8565 - val_loss: 0.3161 - val_accuracy: 0.8483\n",
      "Epoch 40/100\n",
      "388/388 [==============================] - 0s 792us/step - loss: 0.3132 - accuracy: 0.8522 - val_loss: 0.3158 - val_accuracy: 0.8489\n",
      "Epoch 41/100\n",
      "388/388 [==============================] - 0s 855us/step - loss: 0.3119 - accuracy: 0.8546 - val_loss: 0.3163 - val_accuracy: 0.8467\n",
      "Epoch 42/100\n",
      "388/388 [==============================] - 0s 992us/step - loss: 0.3133 - accuracy: 0.8549 - val_loss: 0.3155 - val_accuracy: 0.8486\n",
      "Epoch 43/100\n",
      "388/388 [==============================] - 0s 936us/step - loss: 0.3115 - accuracy: 0.8538 - val_loss: 0.3160 - val_accuracy: 0.8476\n",
      "Epoch 44/100\n",
      "388/388 [==============================] - 0s 906us/step - loss: 0.3073 - accuracy: 0.8560 - val_loss: 0.3156 - val_accuracy: 0.8501\n",
      "Epoch 45/100\n",
      "388/388 [==============================] - 0s 925us/step - loss: 0.3076 - accuracy: 0.8547 - val_loss: 0.3161 - val_accuracy: 0.8473\n",
      "Epoch 46/100\n",
      "388/388 [==============================] - 0s 926us/step - loss: 0.3072 - accuracy: 0.8554 - val_loss: 0.3156 - val_accuracy: 0.8473\n",
      "Epoch 47/100\n",
      "388/388 [==============================] - 0s 841us/step - loss: 0.3077 - accuracy: 0.8584 - val_loss: 0.3156 - val_accuracy: 0.8486\n",
      "Epoch 48/100\n",
      "388/388 [==============================] - 0s 936us/step - loss: 0.3073 - accuracy: 0.8562 - val_loss: 0.3155 - val_accuracy: 0.8495\n",
      "Epoch 49/100\n",
      "388/388 [==============================] - 0s 801us/step - loss: 0.3068 - accuracy: 0.8577 - val_loss: 0.3160 - val_accuracy: 0.8495\n",
      "Epoch 50/100\n",
      "388/388 [==============================] - 0s 847us/step - loss: 0.3097 - accuracy: 0.8568 - val_loss: 0.3159 - val_accuracy: 0.8498\n",
      "Epoch 51/100\n",
      "388/388 [==============================] - 0s 981us/step - loss: 0.3117 - accuracy: 0.8539 - val_loss: 0.3155 - val_accuracy: 0.8495\n",
      "Epoch 52/100\n",
      "388/388 [==============================] - 0s 891us/step - loss: 0.3132 - accuracy: 0.8551 - val_loss: 0.3161 - val_accuracy: 0.8464\n",
      "Epoch 53/100\n",
      "388/388 [==============================] - 0s 944us/step - loss: 0.3066 - accuracy: 0.8582 - val_loss: 0.3155 - val_accuracy: 0.8492\n",
      "Epoch 54/100\n",
      "388/388 [==============================] - 0s 945us/step - loss: 0.3081 - accuracy: 0.8556 - val_loss: 0.3153 - val_accuracy: 0.8489\n",
      "Epoch 55/100\n",
      "388/388 [==============================] - 0s 926us/step - loss: 0.3124 - accuracy: 0.8560 - val_loss: 0.3154 - val_accuracy: 0.8476\n",
      "Epoch 56/100\n",
      "388/388 [==============================] - 0s 935us/step - loss: 0.3077 - accuracy: 0.8557 - val_loss: 0.3162 - val_accuracy: 0.8476\n",
      "Epoch 57/100\n",
      "388/388 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8577 - val_loss: 0.3156 - val_accuracy: 0.8489\n",
      "Epoch 58/100\n",
      "388/388 [==============================] - 0s 871us/step - loss: 0.3118 - accuracy: 0.8540 - val_loss: 0.3161 - val_accuracy: 0.8467\n",
      "Epoch 59/100\n",
      "388/388 [==============================] - 0s 823us/step - loss: 0.3095 - accuracy: 0.8559 - val_loss: 0.3153 - val_accuracy: 0.8492\n",
      "Epoch 60/100\n",
      "388/388 [==============================] - 0s 838us/step - loss: 0.3071 - accuracy: 0.8580 - val_loss: 0.3150 - val_accuracy: 0.8501\n",
      "Epoch 61/100\n",
      "388/388 [==============================] - 0s 819us/step - loss: 0.3081 - accuracy: 0.8571 - val_loss: 0.3155 - val_accuracy: 0.8483\n",
      "Epoch 62/100\n",
      "388/388 [==============================] - 0s 996us/step - loss: 0.3085 - accuracy: 0.8555 - val_loss: 0.3153 - val_accuracy: 0.8476\n",
      "Epoch 63/100\n",
      "388/388 [==============================] - 0s 821us/step - loss: 0.3071 - accuracy: 0.8579 - val_loss: 0.3167 - val_accuracy: 0.8464\n",
      "Epoch 64/100\n",
      "388/388 [==============================] - 0s 928us/step - loss: 0.3088 - accuracy: 0.8555 - val_loss: 0.3151 - val_accuracy: 0.8495\n",
      "Epoch 65/100\n",
      "388/388 [==============================] - 0s 835us/step - loss: 0.3090 - accuracy: 0.8541 - val_loss: 0.3152 - val_accuracy: 0.8483\n",
      "Epoch 66/100\n",
      "388/388 [==============================] - 0s 900us/step - loss: 0.3057 - accuracy: 0.8586 - val_loss: 0.3158 - val_accuracy: 0.8470\n",
      "Epoch 67/100\n",
      "388/388 [==============================] - 0s 824us/step - loss: 0.3075 - accuracy: 0.8575 - val_loss: 0.3166 - val_accuracy: 0.8470\n",
      "Epoch 68/100\n",
      "388/388 [==============================] - 0s 931us/step - loss: 0.3053 - accuracy: 0.8562 - val_loss: 0.3153 - val_accuracy: 0.8479\n",
      "Epoch 69/100\n",
      "388/388 [==============================] - 0s 869us/step - loss: 0.3051 - accuracy: 0.8572 - val_loss: 0.3158 - val_accuracy: 0.8473\n",
      "Epoch 70/100\n",
      "388/388 [==============================] - 0s 955us/step - loss: 0.3077 - accuracy: 0.8588 - val_loss: 0.3156 - val_accuracy: 0.8486\n",
      "Epoch 71/100\n",
      "388/388 [==============================] - 0s 866us/step - loss: 0.3101 - accuracy: 0.8567 - val_loss: 0.3151 - val_accuracy: 0.8461\n",
      "Epoch 72/100\n",
      "388/388 [==============================] - 0s 852us/step - loss: 0.3083 - accuracy: 0.8583 - val_loss: 0.3151 - val_accuracy: 0.8470\n",
      "Epoch 73/100\n",
      "388/388 [==============================] - 0s 899us/step - loss: 0.3073 - accuracy: 0.8561 - val_loss: 0.3163 - val_accuracy: 0.8467\n",
      "Epoch 74/100\n",
      "388/388 [==============================] - 0s 911us/step - loss: 0.3026 - accuracy: 0.8603 - val_loss: 0.3153 - val_accuracy: 0.8513\n",
      "Epoch 75/100\n",
      "388/388 [==============================] - 0s 823us/step - loss: 0.3050 - accuracy: 0.8589 - val_loss: 0.3147 - val_accuracy: 0.8498\n",
      "Epoch 76/100\n",
      "388/388 [==============================] - 0s 938us/step - loss: 0.3047 - accuracy: 0.8560 - val_loss: 0.3153 - val_accuracy: 0.8489\n",
      "Epoch 77/100\n",
      "388/388 [==============================] - 0s 920us/step - loss: 0.3056 - accuracy: 0.8590 - val_loss: 0.3149 - val_accuracy: 0.8501\n",
      "Epoch 78/100\n",
      "388/388 [==============================] - 0s 890us/step - loss: 0.3070 - accuracy: 0.8567 - val_loss: 0.3153 - val_accuracy: 0.8486\n",
      "Epoch 79/100\n",
      "388/388 [==============================] - 0s 893us/step - loss: 0.3046 - accuracy: 0.8590 - val_loss: 0.3164 - val_accuracy: 0.8479\n",
      "Epoch 80/100\n",
      "388/388 [==============================] - 0s 921us/step - loss: 0.3050 - accuracy: 0.8576 - val_loss: 0.3158 - val_accuracy: 0.8479\n",
      "Epoch 81/100\n",
      "388/388 [==============================] - 0s 897us/step - loss: 0.3111 - accuracy: 0.8553 - val_loss: 0.3150 - val_accuracy: 0.8507\n",
      "Epoch 82/100\n",
      "388/388 [==============================] - 0s 909us/step - loss: 0.3009 - accuracy: 0.8613 - val_loss: 0.3159 - val_accuracy: 0.8461\n",
      "Epoch 83/100\n",
      "388/388 [==============================] - 0s 921us/step - loss: 0.3052 - accuracy: 0.8566 - val_loss: 0.3153 - val_accuracy: 0.8464\n",
      "Epoch 84/100\n",
      "388/388 [==============================] - 0s 959us/step - loss: 0.3043 - accuracy: 0.8586 - val_loss: 0.3151 - val_accuracy: 0.8473\n",
      "Epoch 85/100\n",
      "388/388 [==============================] - 0s 842us/step - loss: 0.3068 - accuracy: 0.8587 - val_loss: 0.3157 - val_accuracy: 0.8483\n",
      "Epoch 86/100\n",
      "388/388 [==============================] - 0s 867us/step - loss: 0.3035 - accuracy: 0.8580 - val_loss: 0.3157 - val_accuracy: 0.8470\n",
      "Epoch 87/100\n",
      "388/388 [==============================] - 0s 952us/step - loss: 0.3059 - accuracy: 0.8564 - val_loss: 0.3156 - val_accuracy: 0.8483\n",
      "Epoch 88/100\n",
      "388/388 [==============================] - 0s 919us/step - loss: 0.3096 - accuracy: 0.8562 - val_loss: 0.3155 - val_accuracy: 0.8479\n",
      "Epoch 89/100\n",
      "388/388 [==============================] - 0s 972us/step - loss: 0.3037 - accuracy: 0.8565 - val_loss: 0.3155 - val_accuracy: 0.8458\n",
      "Epoch 90/100\n",
      "388/388 [==============================] - 0s 871us/step - loss: 0.3048 - accuracy: 0.8577 - val_loss: 0.3153 - val_accuracy: 0.8483\n",
      "Epoch 91/100\n",
      "388/388 [==============================] - 0s 888us/step - loss: 0.3067 - accuracy: 0.8552 - val_loss: 0.3155 - val_accuracy: 0.8476\n",
      "Epoch 92/100\n",
      "388/388 [==============================] - 0s 877us/step - loss: 0.3044 - accuracy: 0.8598 - val_loss: 0.3153 - val_accuracy: 0.8464\n",
      "Epoch 93/100\n",
      "388/388 [==============================] - 0s 845us/step - loss: 0.3007 - accuracy: 0.8606 - val_loss: 0.3154 - val_accuracy: 0.8486\n",
      "Epoch 94/100\n",
      "329/388 [========================>.....] - ETA: 0s - loss: 0.3061 - accuracy: 0.8562"
     ]
    }
   ],
   "source": [
    "# we train the model\n",
    "model.fit(x=train_x, y=train_y, epochs=100, batch_size=100, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x=test_x, y=test_y, verbose=0)\n",
    "print('Test accuracy: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_adult(train_x.shape)\n",
    "#model.summary()\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# set up tensorboard log directory and callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=100, batch_size=100, validation_data=(val_x, val_y), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard (notebook experience)\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
